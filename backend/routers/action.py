import copy
import json
import math
import re
import traceback
from pathlib import Path
from collections import defaultdict
from typing import List

import numpy as np
from fastapi import APIRouter, HTTPException, status, Response, BackgroundTasks, Query
from fastapi.responses import JSONResponse, FileResponse
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer, util

import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../')))

from db_utils.commons import create_connect_session, execute_query
from db_utils import action_queries, images_queries
from db_utils.validators import validate_data
from db_utils.models import CustomResponseModel, LoginUser, JoinUser
from config import (
    INIT_CLUSTERING_STATUS,
    CONTINUOUS_CLUSTERING_STATUS,
    DEFAULT_IMAGE_PATH,
    DEFAULT_OUTPUT_PATH,
    CAPTION_STOPWORDS,
    MAJOR_COLORS,
    MAJOR_SHAPES,
    TFIDF_SCORE_THRESHOLDS
)
from clustering.clustering_manager import ChromaDBManager, InitClusteringManager
from clustering.mongo_db_manager import MongoDBManager
from clustering.mongo_result_manager import ResultManager
from clustering.chroma_db_manager import ChromaDBManager
from clustering.embeddings_manager.image_embeddings_manager import ImageEmbeddingsManager
from clustering.utils import Utils
from clustering.word_analysis import WordAnalyzer
from clustering.continuous_clustering_reporter import ContinuousClusteringReporter

#åˆ†å‰²ã—ãŸã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ä½œæˆ
#ãƒ­ã‚°ã‚¤ãƒ³æ“ä½œ
action_endpoint = APIRouter()

def add_parent_ids_hierarchical(clustering_dict: dict, parent_id: str = None) -> dict:
    """
    å…¨ã¦ã®è¦ç´ ã«parent_idã‚’è¿½åŠ ã™ã‚‹å†å¸°é–¢æ•°ï¼ˆéšå±¤åˆ†é¡ç”¨ï¼‰
    InitClusteringManager._add_parent_ids()ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯
    
    Args:
        clustering_dict: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã®è¾æ›¸
        parent_id: è¦ªè¦ç´ ã®IDï¼ˆæœ€ä¸Šä½éšå±¤ã®å ´åˆã¯Noneï¼‰
        
    Returns:
        parent_idãŒè¿½åŠ ã•ã‚ŒãŸè¾æ›¸
    """
    result = {}
    
    for key, value in clustering_dict.items():
        # å€¤ãŒè¾æ›¸ã®å ´åˆã®ã¿å‡¦ç†
        if isinstance(value, dict):
            # ç¾åœ¨ã®è¦ç´ ã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ
            new_value = value.copy()
            
            # parent_idã‚’è¿½åŠ 
            new_value['parent_id'] = parent_id
            
            # dataãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚‹å ´åˆã€å†å¸°çš„ã«å‡¦ç†
            if 'data' in new_value and isinstance(new_value['data'], dict):
                new_value['data'] = add_parent_ids_hierarchical(new_value['data'], key)
            
            result[key] = new_value
        else:
            # æ–‡å­—åˆ—ã‚„ãã®ä»–ã®å€¤ã®å ´åˆã¯ãã®ã¾ã¾
            result[key] = value
    
    return result

@action_endpoint.get("/action/clustering/result/{mongo_result_id}",tags=["action"],description="åˆæœŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã‚’å–å¾—ã™ã‚‹")
def get_clustering_result(mongo_result_id:str):
    print(f"ğŸ” get_clustering_result called with mongo_result_id: {mongo_result_id}")
    
    result_manager = ResultManager(mongo_result_id)
    
    # ResultManagerã®get_result()ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
    result_data = result_manager.get_result()
    
    if result_data:
        print(f"âœ… Found result data for mongo_result_id: {mongo_result_id}")
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={"message": "success", "result": result_data}
        )
    else:
        print(f"âŒ Result not found for mongo_result_id: {mongo_result_id}")
        return JSONResponse(
            status_code=status.HTTP_404_NOT_FOUND,
            content={"message": "Clustering result not found"}
        )


@action_endpoint.get("/action/clustering/all_nodes/{mongo_result_id}",tags=["action"],description="æŒ‡å®šã•ã‚ŒãŸmongo_result_idã®all_nodesã‚’å–å¾—ã™ã‚‹")
def get_all_nodes(mongo_result_id: str):
    """
    æŒ‡å®šã•ã‚ŒãŸmongo_result_idã«ç´ã¥ãall_nodesã‚’å–å¾—ã™ã‚‹
    
    Args:
        mongo_result_id (str): MongoDBã®çµæœID
        
    Returns:
        JSONResponse: all_nodesã®æƒ…å ±
    """
    try:
        # å…¥åŠ›ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if not mongo_result_id or not mongo_result_id.strip():
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "mongo_result_id is required", "data": None}
            )
        
        print(f"ğŸ“‹ get_all_nodeså‘¼ã³å‡ºã—: mongo_result_id={mongo_result_id}")
        
        # ResultManagerã‚’åˆæœŸåŒ–
        result_manager = ResultManager(mongo_result_id)
        
        # all_nodesã‚’å–å¾—
        all_nodes_data = result_manager.get_all_nodes()
        
        if all_nodes_data is None:
            return JSONResponse(
                status_code=status.HTTP_404_NOT_FOUND,
                content={
                    "message": "all_nodes not found for the given mongo_result_id",
                    "data": None
                }
            )
        
        print(f"âœ… all_nodeså–å¾—æˆåŠŸ: {len(all_nodes_data)}å€‹ã®ãƒãƒ¼ãƒ‰")
        
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "message": "success",
                "data": all_nodes_data
            }
        )
        
    except Exception as e:
        print(f"âŒ get_all_nodeså‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={
                "message": f"Internal server error occurred: {str(e)}",
                "data": None
            }
        )


@action_endpoint.post("/action/clustering/copy",tags=["action"],description="ä»–ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã‚’ã‚³ãƒ”ãƒ¼ã™ã‚‹")
def copy_clustering_data(
    source_user_id: int = Query(..., description="ã‚³ãƒ”ãƒ¼å…ƒã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ID"),
    target_user_id: int = Query(..., description="ã‚³ãƒ”ãƒ¼å…ˆã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ID"),
    project_id: int = Query(..., description="ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID")
):
    """
    ä»–ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœï¼ˆall_nodesã¨resultï¼‰ã‚’ã‚³ãƒ”ãƒ¼ã™ã‚‹
    
    Args:
        source_user_id: ã‚³ãƒ”ãƒ¼å…ƒã®ãƒ¦ãƒ¼ã‚¶ãƒ¼IDï¼ˆinit_clustering_stateãŒ2ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼‰
        target_user_id: ã‚³ãƒ”ãƒ¼å…ˆã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        project_id: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
        
    Returns:
        JSONResponse: ã‚³ãƒ”ãƒ¼çµæœ
    """
    connect_session = create_connect_session()
    
    if connect_session is None:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={"message": "failed to connect to database", "data": None}
        )
    
    try:
        # 1. ã‚³ãƒ”ãƒ¼å…ƒãƒ¦ãƒ¼ã‚¶ãƒ¼ã®init_clustering_stateãŒ2ï¼ˆå®Œäº†ï¼‰ã‹ãƒã‚§ãƒƒã‚¯
        source_result, _ = action_queries.get_membership_init_and_mongo(connect_session, source_user_id, project_id)
        
        if not source_result:
            return JSONResponse(
                status_code=status.HTTP_404_NOT_FOUND,
                content={"message": "source user membership not found", "data": None}
            )
        
        source_data = source_result.mappings().first()
        if source_data["init_clustering_state"] != INIT_CLUSTERING_STATUS.FINISHED:
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "source user has not completed init clustering", "data": None}
            )
        
        source_mongo_result_id = source_data["mongo_result_id"]
        
        # 2. ã‚³ãƒ”ãƒ¼å…ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®mongo_result_idã‚’å–å¾—
        target_result, _ = action_queries.get_membership_init_and_mongo(connect_session, target_user_id, project_id)
        
        if not target_result:
            return JSONResponse(
                status_code=status.HTTP_404_NOT_FOUND,
                content={"message": "target user membership not found", "data": None}
            )
        
        target_data = target_result.mappings().first()
        target_mongo_result_id = target_data["mongo_result_id"]
        
        # 3. ã‚³ãƒ”ãƒ¼å…ƒã®all_nodesã¨resultã‚’å–å¾—
        source_result_manager = ResultManager(source_mongo_result_id)
        source_all_nodes = source_result_manager.get_all_nodes()
        source_result_data = source_result_manager.get_result()
        
        if source_all_nodes is None or source_result_data is None:
            return JSONResponse(
                status_code=status.HTTP_404_NOT_FOUND,
                content={"message": "source clustering data not found", "data": None}
            )
        
        # 4. ã‚³ãƒ”ãƒ¼å…ˆã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆãƒ‡ã‚£ãƒ¼ãƒ—ã‚³ãƒ”ãƒ¼ã§å®Œå…¨ã«ç‹¬ç«‹ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼‰
        target_result_manager = ResultManager(target_mongo_result_id)
        copied_all_nodes = copy.deepcopy(source_all_nodes)
        copied_result = copy.deepcopy(source_result_data)
        
        target_result_manager.update_result(copied_result, copied_all_nodes)
        
        # 5. ã‚³ãƒ”ãƒ¼å…ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®init_clustering_stateã‚’2ï¼ˆå®Œäº†ï¼‰ã«æ›´æ–°
        _, _ = action_queries.update_init_state(connect_session, target_user_id, project_id, INIT_CLUSTERING_STATUS.FINISHED)
        
        # 6. ã‚³ãƒ”ãƒ¼å…ƒã®executed_clustering_countã‚’ã‚³ãƒ”ãƒ¼å…ˆã«åæ˜ 
        # clustering_idã§ç”»åƒã‚’ç´ä»˜ã‘ã¦ã€ã‚³ãƒ”ãƒ¼å…ƒã¨åŒã˜executed_clustering_countã‚’è¨­å®š
        _, _ = action_queries.copy_clustering_states_by_clustering_id(connect_session, source_user_id, target_user_id, project_id)
        
        print(f"âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼{source_user_id}ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼{target_user_id}ã«ã‚³ãƒ”ãƒ¼å®Œäº†ï¼ˆexecuted_clustering_countã‚‚å«ã‚€ï¼‰")
        
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "message": "succeeded to copy clustering data",
                "data": {
                    "source_user_id": source_user_id,
                    "target_user_id": target_user_id,
                    "project_id": project_id,
                    "source_mongo_result_id": source_mongo_result_id,
                    "target_mongo_result_id": target_mongo_result_id
                }
            }
        )
        
    except Exception as e:
        print(f"âŒ copy_clustering_dataå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={
                "message": f"failed to copy clustering data: {str(e)}",
                "data": None
            }
        )


@action_endpoint.get("/action/clustering/init/{project_id}", tags=["action"], description="åˆæœŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè£…ã™ã‚‹")
def execute_init_clustering(
    project_id: int = None,
    user_id: int = None,
    use_hierarchical: bool = False,
    background_tasks: BackgroundTasks = None
):
    # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå‘¼ã³å‡ºã—æ™‚ã®ãƒˆã‚°ãƒ«å€¤ã‚’å‡ºåŠ›
    print(f"\n{'='*80}")
    print(f"ğŸ”” execute_init_clustering ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå‘¼ã³å‡ºã—")
    print(f"  - project_id: {project_id}")
    print(f"  - user_id: {user_id}")
    print(f"  - use_hierarchical: {use_hierarchical}")
    print(f"{'='*80}\n")
    
    if project_id is None or user_id is None:
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"message": "project_id and user_id is required", "data": None}
        )

    try:
        project_id = int(project_id)
        user_id = int(user_id)
    except:
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"message": "project_id or user_id is invalid", "data": None}
        )

    connect_session = create_connect_session()
    result, _ = action_queries.get_membership_and_project_info(connect_session, project_id, user_id)
    result_mappings = result.mappings().first()

    if result_mappings is None:
        return JSONResponse(
            status_code=status.HTTP_404_NOT_FOUND,
            content={"message": "project or membership not found", "data": None}
        )

    init_clustering_state = result_mappings["init_clustering_state"]
    original_images_folder_path = result_mappings["original_images_folder_path"]
    mongo_result_id = result_mappings["mongo_result_id"]

    if init_clustering_state == INIT_CLUSTERING_STATUS.EXECUTING or init_clustering_state ==INIT_CLUSTERING_STATUS.FINISHED:
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"message": "init clustering already started", "data": None}
        )

    # å¯¾è±¡ç”»åƒã®å–å¾—
    result, _ = action_queries.select_images_for_init(connect_session, project_id)
    if result is None:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={"message": "failed to get images", "data": None}
        )

    rows = result.mappings().all()
    # æ¤œç´¢ç”¨è¾æ›¸ã‚’ä½œæˆ
    by_clustering_id = {}
    by_chromadb_sentence_id = {}
    by_chromadb_image_id = {}

    for row in rows:
        cid = row["clustering_id"]
        sid = row["chromadb_sentence_id"]
        iid = row["chromadb_image_id"]
        
        by_clustering_id[cid] = {"sentence_id": sid, "image_id": iid}
        by_chromadb_sentence_id[sid] = {"clustering_id": cid, "image_id": iid}
        by_chromadb_image_id[iid] = {"clustering_id": cid,"sentence_id":sid}
    
    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ã«æ¸¡ã™é–¢æ•°
    def run_clustering(cid_dict: dict, sid_dict: dict, iid_dict: dict, project_id: int, original_images_folder_path: str, use_hierarchical: bool = False):
        try:
            # ãƒˆã‚°ãƒ«ã®å€¤ã‚’å‡ºåŠ›
            print(f"ğŸ”„ use_hierarchical = {use_hierarchical}")
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ä½¿ç”¨ã™ã‚‹ç”»åƒæƒ…å ±ã‚’å‡ºåŠ›
            print(f"\nğŸ“¸ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å¯¾è±¡ç”»åƒæƒ…å ±:")
            print(f"  - ç”»åƒæ•°: {len(cid_dict)}")
            print(f"  - Sentence IDæ•°: {len(sid_dict)}")
            print(f"  - Image IDæ•°: {len(iid_dict)}")
            print(f"\nğŸ“‹ Clustering ID ãƒªã‚¹ãƒˆ (æœ€åˆã®10ä»¶):")
            for i, (clustering_id, info) in enumerate(list(cid_dict.items())[:10]):
                print(f"  [{i+1}] {clustering_id}")
                print(f"      -> sentence_id: {info.get('sentence_id')}")
                print(f"      -> image_id: {info.get('image_id')}")
            if len(cid_dict) > 10:
                print(f"  ... ä»– {len(cid_dict) - 10} ä»¶")
            print()
            
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’å–å¾—
            project_result, _ = action_queries.get_project_name(connect_session, project_id)
            project_mapping = project_result.mappings().first() if project_result else None
            project_name = project_mapping['name'] if project_mapping else f"Project_{project_id}"
            
            print(f"ğŸ·ï¸ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’å–å¾—: {project_name} (project_id: {project_id})")
            
            # é€šå¸¸ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ã‚’å®Ÿè¡Œ
            cl_module = InitClusteringManager(
                sentence_name_db=ChromaDBManager('sentence_name_embeddings'),
                sentence_usage_db=ChromaDBManager('sentence_usage_embeddings'),
                sentence_category_db=ChromaDBManager('sentence_category_embeddings'),
                image_db=ChromaDBManager("image_embeddings"),
                images_folder_path=f"./{DEFAULT_IMAGE_PATH}/{original_images_folder_path}",
                output_base_path=f"./{DEFAULT_OUTPUT_PATH}/{project_id}",
            )
            
            target_sentence_ids = list(sid_dict.keys())
            target_image_ids = list(iid_dict.keys())
            
            # sentence_name_dbã‹ã‚‰ç›´æ¥sentence_idã‚’ä½¿ç”¨ã—ã¦embeddingsã‚’å–å¾—
            sentence_data = cl_module.sentence_name_db.get_data_by_sentence_ids(target_sentence_ids)
            embeddings = sentence_data['embeddings']
            cluster_num, _ = cl_module.get_optimal_cluster_num(embeddings=embeddings)
            
            # ãƒˆã‚°ãƒ«ã®å€¤ã«å¿œã˜ã¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ¡ã‚½ãƒƒãƒ‰ã‚’é¸æŠ
            if use_hierarchical:
                print(f"\nğŸ”„ use_hierarchical = True: clustering_dummy()ã‚’å®Ÿè¡Œã—ã¾ã™\n")
                result_dict, all_nodes = cl_module.clustering_dummy(
                    sentence_name_db_data=sentence_data,
                    image_db_data=cl_module.image_db.get_data_by_ids(target_image_ids),
                    clustering_id_dict=cid_dict,
                    sentence_id_dict=sid_dict,
                    image_id_dict=iid_dict,
                    cluster_num=cluster_num,
                    overall_folder_name=project_name,
                    output_folder=True,
                    output_json=True
                )
            else:
                print(f"\nğŸ”„ use_hierarchical = False: clustering()ã‚’å®Ÿè¡Œã—ã¾ã™\n")
                result_dict, all_nodes = cl_module.clustering(
                    sentence_name_db_data=sentence_data,
                    image_db_data=cl_module.image_db.get_data_by_ids(target_image_ids),
                    clustering_id_dict=cid_dict,
                    sentence_id_dict=sid_dict,  # å…ƒã®å½¢å¼ã«æˆ»ã™
                    image_id_dict=iid_dict,
                    cluster_num=cluster_num,
                    overall_folder_name=project_name,
                    output_folder=True,
                    output_json=True
                )
            
            # all_nodesã‚’é…åˆ—ã‹ã‚‰è¾æ›¸å½¢å¼ã«å¤‰æ›ï¼ˆidã‚’ã‚­ãƒ¼ã¨ã—ã¦ï¼‰
            all_nodes_dict = {}
            for node in all_nodes:
                if 'id' in node:
                    all_nodes_dict[node['id']] = node
            
            # MongoDBã‚’æ›´æ–°ï¼ˆãƒ€ãƒŸãƒ¼ãƒ»é€šå¸¸ä¸¡æ–¹ã§å®Ÿè¡Œï¼‰
            print(f"\nğŸ’¾ MongoDBã‚’æ›´æ–°:")
            print(f"  - mongo_result_id: {mongo_result_id}")
            print(f"  - result_dict keys: {list(result_dict.keys())[:5]}...")
            print(f"  - all_nodes_dict size: {len(all_nodes_dict)}")
            
            result_manager = ResultManager(mongo_result_id)
            result_manager.update_result(result_dict, all_nodes_dict)
            
            print(f"âœ… MongoDBæ›´æ–°å®Œäº†")
        except Exception as e:
            print(f"Error during clustering:{e}")
            
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯åˆæœŸåŒ–çŠ¶æ…‹ã‚’æ›´æ–°
            clustering_state = INIT_CLUSTERING_STATUS.FAILED
        else:
            clustering_state = INIT_CLUSTERING_STATUS.FINISHED
            
            # åˆæœŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æˆåŠŸæ™‚ã€è©²å½“ãƒ¦ãƒ¼ã‚¶ã®å…¨ç”»åƒã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯
            try:
                _, _ = action_queries.mark_user_images_clustered_with_executed_count(connect_session, user_id, project_id, 0)
                print(f"âœ… ãƒ¦ãƒ¼ã‚¶{user_id}ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ{project_id}å†…ã®å…¨ç”»åƒã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿(executed_clustering_count=0)ã¨ã—ã¦ãƒãƒ¼ã‚¯ã—ã¾ã—ãŸ")
            except Exception as mark_error:
                print(f"âš ï¸ user_image_clustering_statesæ›´æ–°ã‚¨ãƒ©ãƒ¼: {mark_error}")
        finally:
            _, _ = action_queries.update_init_state(connect_session, user_id, project_id, clustering_state)
                
    # éåŒæœŸå®Ÿè¡Œ
    background_tasks.add_task(run_clustering, by_clustering_id, by_chromadb_sentence_id, by_chromadb_image_id, project_id, original_images_folder_path, use_hierarchical)
    #å®Ÿè¡Œä¸­ã«å¤‰æ›´
    _, _ = action_queries.update_init_state(connect_session, user_id, project_id, INIT_CLUSTERING_STATUS.EXECUTING)
    
    return JSONResponse(
        status_code=status.HTTP_200_OK,
        content={"message": "init clustering started in background", "data": project_id}
    )


@action_endpoint.get("/action/clustering/continuous/{project_id}", tags=["action"], description="ç¶™ç¶šçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè£…ã™ã‚‹")
def execute_continuous_clustering(
    project_id: int = None,
    user_id: int = None,
    background_tasks: BackgroundTasks = None
):
    """
    ç¶™ç¶šçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°: æ–°è¦è¿½åŠ ã•ã‚ŒãŸæœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒã‚’æ—¢å­˜ã®éšå±¤ã«è¿½åŠ ã™ã‚‹
    
    Args:
        project_id: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
        user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        background_tasks: ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯
        
    Returns:
        JSONResponse: ç¶™ç¶šçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°é–‹å§‹çµæœ
    """
    if project_id is None or user_id is None:
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"message": "project_id and user_id is required", "data": None}
        )

    try:
        project_id = int(project_id)
        user_id = int(user_id)
    except:
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"message": "project_id or user_id is invalid", "data": None}
        )

    connect_session = create_connect_session()

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ¡ãƒ³ãƒãƒ¼ã‚·ãƒƒãƒ—æƒ…å ±ã‚’å–å¾—
    result, _ = action_queries.get_membership_and_project_info(connect_session, project_id, user_id)
    result_mappings = result.mappings().first()

    if result_mappings is None:
        return JSONResponse(
            status_code=status.HTTP_404_NOT_FOUND,
            content={"message": "project or membership not found", "data": None}
        )

    init_clustering_state = result_mappings["init_clustering_state"]
    continuous_clustering_state = result_mappings["continuous_clustering_state"]
    mongo_result_id = result_mappings["mongo_result_id"]
    original_images_folder_path = result_mappings["original_images_folder_path"]

    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºåŠ›
    print(f"\nğŸ” ç¶™ç¶šçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯:")
    print(f"   init_clustering_state: {init_clustering_state} (æœŸå¾…å€¤: {INIT_CLUSTERING_STATUS.FINISHED})")
    print(f"   continuous_clustering_state: {continuous_clustering_state} (æœŸå¾…å€¤: {CONTINUOUS_CLUSTERING_STATUS.EXECUTABLE})")
    print(f"   mongo_result_id: {mongo_result_id}")
    
    # åˆæœŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãŒå®Œäº†ã—ã¦ã„ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
    if init_clustering_state != INIT_CLUSTERING_STATUS.FINISHED:
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={
                "message": "init clustering not completed yet", 
                "data": {
                    "current_init_state": init_clustering_state,
                    "required_init_state": INIT_CLUSTERING_STATUS.FINISHED
                }
            }
        )

    # ç¶™ç¶šçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãŒå®Ÿè¡Œå¯èƒ½ã§ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
    if continuous_clustering_state != CONTINUOUS_CLUSTERING_STATUS.EXECUTABLE:  # 2 = å®Ÿè¡Œå¯èƒ½
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={
                "message": "continuous clustering is not executable", 
                "data": {
                    "current_continuous_state": continuous_clustering_state,
                    "required_continuous_state": CONTINUOUS_CLUSTERING_STATUS.EXECUTABLE
                }
            }
        )

    # æœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒã®å–å¾—ï¼ˆç”»åƒã®è©³ç´°æƒ…å ±ã‚‚å«ã‚ã‚‹ï¼‰
    unclustered_images_query = f"""
        SELECT 
            i.id as image_id,
            i.name as image_name,
            i.clustering_id, 
            i.chromadb_sentence_id, 
            i.chromadb_image_id,
            i.caption,
            i.created_at
        FROM images i
        LEFT JOIN user_image_clustering_states uics 
            ON i.id = uics.image_id AND uics.user_id = {user_id}
        WHERE i.project_id = {project_id} 
            AND i.is_created_caption = TRUE
            AND (uics.is_clustered = 0 OR uics.is_clustered IS NULL);
    """

    result, _ = action_queries.get_unclustered_images(connect_session, project_id, user_id)
    
    if result is None:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={"message": "failed to get unclustered images", "data": None}
        )

    rows = result.mappings().all()
    
    print(f"\nğŸ“Š æœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒã®å–å¾—çµæœ:")
    print(f"   å–å¾—ã—ãŸç”»åƒæ•°: {len(rows)}")
    for row in rows:
        print(f"   - ç”»åƒID: {row['image_id']}, åå‰: {row['image_name']}")
    
    if len(rows) == 0:
        print(f"âš ï¸ æœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={
                "message": "no unclustered images found", 
                "data": {
                    "project_id": project_id,
                    "user_id": user_id,
                    "unclustered_count": 0
                }
            }
        )

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—
    user_result, _ = action_queries.get_user_info(connect_session, user_id)
    user_info = user_result.mappings().first() if user_result else None

    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«è©³ç´°æƒ…å ±ã‚’å‡ºåŠ›
    print("=" * 80)
    print("=" * 80)
    print(f"\nğŸ“‹ å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±:")
    print(f"  - ãƒ¦ãƒ¼ã‚¶ãƒ¼ID: {user_id}")
    if user_info:
        print(f"  - ãƒ¦ãƒ¼ã‚¶ãƒ¼å: {user_info['name']}")
        print(f"  - ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹: {user_info['email']}")
    for idx, row in enumerate(rows, 1):
        print(f"\n  [{idx}] ç”»åƒæƒ…å ±:")
        print(f"      - ç”»åƒID: {row['image_id']}")
        print(f"      - ç”»åƒå: {row['image_name']}")
    print("\n" + "=" * 80)

    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ã«æ¸¡ã™é–¢æ•°
    def run_continuous_clustering(unclustered_rows: list, project_id: int, user_id: int, mongo_result_id: str):
        try:
            print(f"\nğŸ”„ ç¶™ç¶šçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†é–‹å§‹")
            print(f"   ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID: {project_id}")
            print(f"   ãƒ¦ãƒ¼ã‚¶ãƒ¼ID: {user_id}")
            print(f"   æœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒæ•°: {len(unclustered_rows)}")
            
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã‚’å–å¾—ï¼ˆãƒ¬ãƒãƒ¼ãƒˆç”¨ï¼‰
            project_result, _ = action_queries.get_project_name(connect_session, project_id)
            project_info = project_result.mappings().first() if project_result else None
            project_name = project_info['name'] if project_info else f"project_{project_id}"
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—ï¼ˆãƒ¬ãƒãƒ¼ãƒˆç”¨ï¼‰
            user_result, _ = action_queries.get_user_info(connect_session, user_id)
            user_info = user_result.mappings().first() if user_result else None
            user_name = user_info['name'] if user_info else f"user_{user_id}"
            
            # ãƒ¬ãƒãƒ¼ã‚¿ãƒ¼åˆæœŸåŒ–
            reporter = ContinuousClusteringReporter(
                project_name=project_name,
                user_name=user_name,
                output_base_dir=DEFAULT_OUTPUT_PATH
            )
            
            # å®Ÿè¡Œæ™‚åˆ»ã‚’è¨˜éŒ²
            from datetime import datetime
            execution_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            # å…¨ç”»åƒã®ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
            all_reports_data = []
            
            # ResultManagerã¨ChromaDBManagerã‚’åˆæœŸåŒ–
            result_manager = ResultManager(mongo_result_id)
            # æ–‡ç« åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã¨ç”»åƒåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ä¸¡æ–¹ã‚’ä½¿ç”¨
            sentence_name_db = ChromaDBManager("sentence_name_embeddings")
            image_db = ChromaDBManager("image_embeddings")
            
            # ç¾åœ¨ã®executed_clustering_countã‚’å–å¾—ã—ã¦+1
            count_result, _ = action_queries.get_executed_clustering_count(connect_session, user_id, project_id)
            current_count = count_result.mappings().first()['executed_clustering_count']
            new_count = current_count + 1
            
            print(f"ğŸ“Š ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å›æ•°: {current_count} â†’ æ–°ã—ã„å›æ•°: {new_count}")
            
            # ã™ã¹ã¦ã®ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€ã‚’å–å¾—
            leaf_folders = result_manager.get_all_leaf_folders()
            print(f"ğŸ“‚ ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€æ•°: {len(leaf_folders)}")
            
            if len(leaf_folders) == 0:
                print("âŒ ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return
            
            # å„ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€ã®æ–‡ç« åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã¨ç”»åƒåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®å¹³å‡ã‚’è¨ˆç®—
            folder_sentence_embeddings = {}
            folder_image_embeddings = {}
            for folder in leaf_folders:
                folder_id = folder['id']
                
                # resultå†…ã§ãƒ•ã‚©ãƒ«ãƒ€IDã‚’æ¢ç´¢ã—ã¦dataã‚’å–å¾—
                folder_data_result = result_manager.get_folder_data_from_result(folder_id)
                
                if not folder_data_result['success']:
                    print(f"  âš ï¸ ãƒ•ã‚©ãƒ«ãƒ€ {folder_id} ({folder['name']}) ã®ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: {folder_data_result.get('error', 'Unknown error')}")
                    continue
                
                # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ç”»åƒã®clustering_idã‚’å–å¾—
                folder_data = folder_data_result['data']
                if not isinstance(folder_data, dict) or len(folder_data) == 0:
                    print(f"  âš ï¸ ãƒ•ã‚©ãƒ«ãƒ€ {folder_id} ({folder['name']}) ã¯ç©ºã§ã™")
                    continue
                
                clustering_ids = list(folder_data.keys())
                print(f"  ğŸ“ ãƒ•ã‚©ãƒ«ãƒ€ {folder['name']} ({folder_id}): {len(clustering_ids)}å€‹ã®ç”»åƒã‚’å«ã‚€")
                
                # clustering_idã‹ã‚‰chromadb_sentence_idã¨chromadb_image_idã‚’å–å¾—
                sentence_ids = []
                image_ids = []
                for cid in clustering_ids:
                    sent_result, _ = action_queries.get_chromadb_sentence_id_by_clustering_id(connect_session, cid, project_id)
                    if sent_result:
                        sent_mapping = sent_result.mappings().first()
                        if sent_mapping:
                            sentence_ids.append(sent_mapping['chromadb_sentence_id'])
                    
                    img_result, _ = action_queries.get_chromadb_image_id_by_clustering_id(connect_session, cid, project_id)
                    if img_result:
                        img_mapping = img_result.mappings().first()
                        if img_mapping:
                            image_ids.append(img_mapping['chromadb_image_id'])

                # ChromaDBã‹ã‚‰æ–‡ç« ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
                if len(sentence_ids) > 0:
                    try:
                        sentence_data = sentence_name_db.get_data_by_ids(sentence_ids)
                        sentence_embeddings = sentence_data['embeddings']
                        avg_sentence_embedding = np.mean(sentence_embeddings, axis=0)
                        folder_sentence_embeddings[folder_id] = avg_sentence_embedding
                        print(f"  âœ… ãƒ•ã‚©ãƒ«ãƒ€ {folder['name']} ({folder_id}): {len(sentence_embeddings)}å€‹ã®æ–‡ç« ã®å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—å®Œäº†")
                    except Exception as e:
                        print(f"  âš ï¸ ãƒ•ã‚©ãƒ«ãƒ€ {folder_id} ã®æ–‡ç« åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                
                # ChromaDBã‹ã‚‰ç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
                if len(image_ids) > 0:
                    try:
                        image_data = image_db.get_data_by_ids(image_ids)
                        image_embeddings = image_data['embeddings']
                        avg_image_embedding = np.mean(image_embeddings, axis=0)
                        folder_image_embeddings[folder_id] = avg_image_embedding
                        print(f"  âœ… ãƒ•ã‚©ãƒ«ãƒ€ {folder['name']} ({folder_id}): {len(image_embeddings)}å€‹ã®ç”»åƒã®å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—å®Œäº†")
                    except Exception as e:
                        print(f"  âš ï¸ ãƒ•ã‚©ãƒ«ãƒ€ {folder_id} ã®ç”»åƒåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            
            print(f"\nğŸ“Š æ–‡ç« åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŒã¤ãƒ•ã‚©ãƒ«ãƒ€æ•°: {len(folder_sentence_embeddings)}")
            print(f"ğŸ“Š ç”»åƒåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŒã¤ãƒ•ã‚©ãƒ«ãƒ€æ•°: {len(folder_image_embeddings)}")
            
            # é¡ä¼¼åº¦é–¾å€¤ã‚’å®šç¾©ï¼ˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã§ã‚‚ä½¿ç”¨ï¼‰
            SIMILARITY_THRESHOLD = 0.4  # é¡ä¼¼åº¦é–¾å€¤ï¼ˆèª¿æ•´å¯èƒ½ï¼‰
            
            # å„æœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒã‚’å‡¦ç†
            for idx, row in enumerate(unclustered_rows, 1):
                try:
                    image_id = row['image_id']
                    image_name = row['image_name']
                    clustering_id = row['clustering_id']
                    chromadb_sentence_id = row['chromadb_sentence_id']
                    chromadb_image_id = row['chromadb_image_id']
                    caption = row.get('caption', '')
                    
                    print(f"\n  [{idx}/{len(unclustered_rows)}] å‡¦ç†ä¸­: {image_name} (ID: {image_id})")
                    
                    # ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–
                    report_data = {
                        'execution_time': execution_time,
                        'project_name': project_name,
                        'user_name': user_name,
                        'clustering_count': new_count,
                        'image_id': image_id,
                        'image_name': image_name,
                        'clustering_id': clustering_id,
                        'chromadb_sentence_id': chromadb_sentence_id,
                        'chromadb_image_id': chromadb_image_id,
                        'caption': caption,
                        'sentence_embedding_available': False,
                        'image_embedding_available': False,
                        'total_folders_checked': len(leaf_folders),
                        'similarity_scores': [],
                        'errors': [],
                        'new_folder_created': False,
                        'classification_criteria_used': False,
                        'additional_info': {},
                        'feature_analysis': {},
                        'sibling_folders_info': {},
                        'processing_steps': [],  # å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã®è¨˜éŒ²
                        'decision_step': None,   # æœ€çµ‚æ±ºå®šã‚¹ãƒ†ãƒƒãƒ—
                        'decision_reason': None  # æ±ºå®šç†ç”±
                    }
                    
                    # ChromaDBã‹ã‚‰æ–‡ç« ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
                    new_sentence_embedding = None
                    try:
                        new_sentence_data = sentence_name_db.get_data_by_ids([chromadb_sentence_id])
                        new_sentence_embedding = new_sentence_data['embeddings'][0]
                        report_data['sentence_embedding_available'] = True
                    except Exception as e:
                        print(f"    âš ï¸ æ–‡ç« åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                        report_data['errors'].append(f"æ–‡ç« åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    
                    # ChromaDBã‹ã‚‰ç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
                    new_image_embedding = None
                    try:
                        new_image_data = image_db.get_data_by_ids([chromadb_image_id])
                        new_image_embedding = new_image_data['embeddings'][0]
                        report_data['image_embedding_available'] = True
                    except Exception as e:
                        print(f"    âš ï¸ ç”»åƒåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                        report_data['errors'].append(f"ç”»åƒåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    
                    # ä¸¡æ–¹ã®ãƒ™ã‚¯ãƒˆãƒ«ãŒå–å¾—ã§ããªã‹ã£ãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    if new_sentence_embedding is None and new_image_embedding is None:
                        print(f"    âš ï¸ åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                        continue
                    
                    # å„ãƒ•ã‚©ãƒ«ãƒ€ã¨ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆæ–‡ç« ã¨ç”»åƒã®ä¸¡æ–¹ï¼‰
                    max_similarity = -1
                    best_folder_id = None
                    best_similarity_type = None  # 'sentence' or 'image'
                    all_similarity_scores = []  # å…¨ãƒ•ã‚©ãƒ«ãƒ€ã¨ã®é¡ä¼¼åº¦ã‚’è¨˜éŒ²
                    
                    # æ–‡ç« ãƒ™ã‚¯ãƒˆãƒ«ã§é¡ä¼¼åº¦è¨ˆç®—
                    if new_sentence_embedding is not None:
                        for folder_id, folder_embedding in folder_sentence_embeddings.items():
                            similarity = cosine_similarity(
                                [new_sentence_embedding],
                                [folder_embedding]
                            )[0][0]
                            
                            # ãƒ•ã‚©ãƒ«ãƒ€åã‚’å–å¾—
                            folder_obj = next((f for f in leaf_folders if f['id'] == folder_id), None)
                            folder_name = folder_obj['name'] if folder_obj else folder_id
                            
                            all_similarity_scores.append({
                                'folder_id': folder_id,
                                'folder_name': folder_name,
                                'similarity': float(similarity),
                                'type': 'sentence'
                            })
                            
                            if similarity > max_similarity:
                                max_similarity = similarity
                                best_folder_id = folder_id
                                best_similarity_type = 'sentence'
                    
                    # ç”»åƒãƒ™ã‚¯ãƒˆãƒ«ã§é¡ä¼¼åº¦è¨ˆç®—
                    if new_image_embedding is not None:
                        for folder_id, folder_embedding in folder_image_embeddings.items():
                            similarity = cosine_similarity(
                                [new_image_embedding],
                                [folder_embedding]
                            )[0][0]
                            
                            # ãƒ•ã‚©ãƒ«ãƒ€åã‚’å–å¾—
                            folder_obj = next((f for f in leaf_folders if f['id'] == folder_id), None)
                            folder_name = folder_obj['name'] if folder_obj else folder_id
                            
                            all_similarity_scores.append({
                                'folder_id': folder_id,
                                'folder_name': folder_name,
                                'similarity': float(similarity),
                                'type': 'image'
                            })
                            
                            if similarity > max_similarity:
                                max_similarity = similarity
                                best_folder_id = folder_id
                                best_similarity_type = 'image'
                    
                    # é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã‚’ã‚½ãƒ¼ãƒˆã—ã¦ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã«ä¿å­˜
                    all_similarity_scores.sort(key=lambda x: x['similarity'], reverse=True)
                    report_data['similarity_scores'] = all_similarity_scores
                    
                    if best_folder_id is None:
                        print(f"    âš ï¸ é©åˆ‡ãªãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                        report_data['errors'].append("é©åˆ‡ãªãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                        report_data['decision_step'] = 'NO_FOLDER_FOUND'
                        report_data['decision_reason'] = 'é¡ä¼¼åº¦è¨ˆç®—ã§é©åˆ‡ãªãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ'
                        
                        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
                        try:
                            reporter.generate_image_report(report_data)
                        except Exception as report_e:
                            print(f"    âš ï¸ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {report_e}")
                            traceback.print_exc()
                        
                        # ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
                        all_reports_data.append(report_data)
                        continue
                    
                    print(f"    ğŸ“Š æœ€é«˜é¡ä¼¼åº¦: {max_similarity:.4f} (ã‚¿ã‚¤ãƒ—: {best_similarity_type})")
                    
                    # é¡ä¼¼åº¦é–¾å€¤ãƒã‚§ãƒƒã‚¯ï¼šé–¾å€¤ã‚’ä¸‹å›ã‚‹å ´åˆã¯æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
                    report_data['similarity_threshold'] = SIMILARITY_THRESHOLD
                    
                    if max_similarity < SIMILARITY_THRESHOLD:
                        print(f"    âš ï¸ æœ€é«˜é¡ä¼¼åº¦ {max_similarity:.4f} ãŒé–¾å€¤ {SIMILARITY_THRESHOLD} ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™")
                        print(f"    ğŸ†• æ–°ã—ã„ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¾ã™...")
                        report_data['processing_steps'].append(f'é¡ä¼¼åº¦é–¾å€¤ãƒã‚§ãƒƒã‚¯: {max_similarity:.4f} < {SIMILARITY_THRESHOLD}')
                        report_data['decision_step'] = 'NEW_FOLDER_CREATION'
                        report_data['decision_reason'] = f'æœ€é«˜é¡ä¼¼åº¦({max_similarity:.4f})ãŒé–¾å€¤({SIMILARITY_THRESHOLD})ã‚’ä¸‹å›ã£ãŸãŸã‚æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ'
                        
                        # ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‹ã‚‰æ–°ãƒ•ã‚©ãƒ«ãƒ€åã‚’ç”Ÿæˆ
                        try:
                            caption_res, _ = images_queries.select_caption_by_clustering_id(connect_session, clustering_id)
                            if caption_res:
                                caption_row = caption_res.mappings().first()
                                if caption_row and 'caption' in caption_row and caption_row['caption']:
                                    caption = caption_row['caption']
                                    # ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‹ã‚‰ç‰¹å¾´çš„ãªå˜èªã‚’æŠ½å‡ºã—ã¦ãƒ•ã‚©ãƒ«ãƒ€åã‚’ç”Ÿæˆ
                                    # æœ€åˆã®æ–‡ç¯€ï¼ˆ.ã®å‰ï¼‰ã‹ã‚‰å˜èªã‚’æŠ½å‡º
                                    first_sentence = caption.split('.')[0] if '.' in caption else caption
                                    # 2-3å€‹ã®ç‰¹å¾´çš„ãªå˜èªã‚’æŠ½å‡ºï¼ˆã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å¤–ï¼‰

                                    words = WordAnalyzer.extract_words(first_sentence)
                                    # æœ€å¤§3å˜èªã§ãƒ•ã‚©ãƒ«ãƒ€åã‚’ä½œæˆ
                                    new_folder_name = ','.join(words[:3]) if len(words) > 0 else f"new_category_{idx}"
                                else:
                                    new_folder_name = f"new_category_{idx}"
                            else:
                                new_folder_name = f"new_category_{idx}"
                        except Exception as name_e:
                            print(f"    âš ï¸ ãƒ•ã‚©ãƒ«ãƒ€åç”Ÿæˆã‚¨ãƒ©ãƒ¼: {name_e}")
                            new_folder_name = f"new_category_{idx}"
                        
                        # imagesãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰image_pathã‚’å–å¾—
                        path_result, _ = action_queries.get_image_name_by_id(connect_session, image_id)
                        image_path = path_result.mappings().first()['name']
                        
                        # ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ï¼ˆparent_id=Noneï¼‰ã«æ–°ã—ã„ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
                        create_result = result_manager.create_new_leaf_folder(
                            folder_name=new_folder_name,
                            parent_id=None,  # ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã«ä½œæˆ
                            initial_clustering_id=clustering_id,
                            initial_image_path=image_path
                        )
                        
                        if create_result['success']:
                            new_folder_id = create_result['folder_id']
                            print(f"    âœ… æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¾ã—ãŸ: {new_folder_name} (ID: {new_folder_id})")
                            
                            # ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã«æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆæƒ…å ±ã‚’è¨˜éŒ²
                            report_data['new_folder_created'] = True
                            report_data['new_folder_name'] = new_folder_name
                            report_data['new_folder_id'] = new_folder_id
                            report_data['final_folder_name'] = new_folder_name
                            report_data['final_folder_id'] = new_folder_id
                            report_data['final_similarity'] = max_similarity
                            report_data['final_similarity_type'] = best_similarity_type
                            
                            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
                            try:
                                reporter.generate_image_report(report_data)
                            except Exception as report_e:
                                print(f"    âš ï¸ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {report_e}")
                                traceback.print_exc()
                            
                            # ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
                            all_reports_data.append(report_data)
                            
                            # user_image_clustering_statesã‚’æ›´æ–°
                            _, _ = action_queries.update_user_image_state_for_image(connect_session, user_id, image_id, new_count)
                            
                            # æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿½åŠ ï¼ˆä¸¡æ–¹ï¼‰
                            if new_sentence_embedding is not None:
                                folder_sentence_embeddings[new_folder_id] = new_sentence_embedding
                            if new_image_embedding is not None:
                                folder_image_embeddings[new_folder_id] = new_image_embedding
                            
                            # leaf_foldersãƒªã‚¹ãƒˆã«ã‚‚è¿½åŠ 
                            leaf_folders.append({
                                'id': new_folder_id,
                                'name': new_folder_name,
                                'parent_id': None,
                                'is_leaf': True
                            })
                            
                            print(f"    â„¹ï¸ é¡ä¼¼åº¦ãŒä½ã„ãŸã‚ã€å¾Œç¶šã®ãƒ•ã‚©ãƒ«ãƒ€ç‰¹å¾´åˆ†æã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                            continue  # å¾Œç¶šã®å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
                        else:
                            print(f"    âŒ ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆã‚¨ãƒ©ãƒ¼: {create_result.get('error', 'Unknown error')}")
                            print(f"    â†’ æ—¢å­˜ã®ãƒ•ã‚©ãƒ«ãƒ€ã«é…ç½®ã‚’è©¦ã¿ã¾ã™")
                            report_data['errors'].append(f"æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆå¤±æ•—: {create_result.get('error', 'Unknown error')}")
                            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã¸ã®é…ç½®å‡¦ç†ã«é€²ã‚€ï¼ˆãƒ¬ãƒãƒ¼ãƒˆã¯å¾Œã§ç”Ÿæˆï¼‰
                    
                    best_folder = next((f for f in leaf_folders if f['id'] == best_folder_id), None)
                    folder_name = best_folder['name'] if best_folder else best_folder_id
                    
                    print(f"    ğŸ¯ æœ€ã‚‚é¡ä¼¼ã—ãŸãƒ•ã‚©ãƒ«ãƒ€: {folder_name} (é¡ä¼¼åº¦: {max_similarity:.4f})")
                    
                    # --- åˆ†é¡åŸºæº–ã‚’ä½¿ã£ãŸæŒ¯ã‚Šåˆ†ã‘ãƒ­ã‚¸ãƒƒã‚¯ ---
                    # å¾Œã§ä½¿ç”¨ã™ã‚‹ãŸã‚ã®å¤‰æ•°ã‚’åˆæœŸåŒ–
                    classification_criteria = {}
                    classification_words_found = []
                    target_folder_id_by_criteria = None
                    sibling_leaf_folders = []  # åˆæœŸåŒ–ã—ã¦æœªå®šç¾©ã‚¨ãƒ©ãƒ¼ã‚’é˜²æ­¢
                    
                    # --- æŒ‡å®šã—ãŸãƒ•ã‚©ãƒ«ãƒ€ã¨åŒã˜éšå±¤ã«ã‚ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã‚’å–å¾— ---
                    try:
                        # all_nodesã‹ã‚‰æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€ï¼ˆbest_folderï¼‰ã®æƒ…å ±ã‚’å–å¾—
                        all_nodes = result_manager.get_all_nodes()
                        best_node = all_nodes.get(best_folder_id) if all_nodes else None
                        
                        if not best_node:
                            print(f"    âš ï¸ æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€ {best_folder_id} ãŒall_nodesã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                            # best_folder_idã®ã¿ã‚’å«ã‚€ãƒªã‚¹ãƒˆã¨ã—ã¦æ‰±ã†
                            sibling_leaf_folders = [best_folder] if best_folder else []
                        else:
                            parent_id_of_best = best_node.get('parent_id')
                            print(f"    ğŸ“ æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€ã®parent_id: {parent_id_of_best}")
                            
                            # åŒã˜parent_idã‚’æŒã¤ãƒ•ã‚©ãƒ«ãƒ€ã‚’å–å¾—
                            sibling_folders = []
                            for node_id, node_data in all_nodes.items():
                                if node_data.get('parent_id') == parent_id_of_best:
                                    sibling_folders.append({
                                        'id': node_id,
                                        'name': node_data.get('name'),
                                        'parent_id': node_data.get('parent_id'),
                                        'is_leaf': node_data.get('is_leaf', False)
                                    })
                            
                            print(f"    ğŸ“‚ åŒéšå±¤ãƒ•ã‚©ãƒ«ãƒ€: {len(sibling_folders)}å€‹")
                            
                            # --- is_leafãƒ•ã‚©ãƒ«ãƒ€ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ä¸€è¦§ã‚’å–å¾— ---
                            sibling_leaf_folders = [f for f in sibling_folders if f['is_leaf']]
                            all_captions = []  # å…¨ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’æ ¼ç´
                            folder_captions_map = {}  # ãƒ•ã‚©ãƒ«ãƒ€ã”ã¨ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³
                            
                            for sib_folder in sibling_leaf_folders:
                                sib_folder_id = sib_folder['id']
                                sib_folder_name = sib_folder['name']
                                
                                # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®clustering_idã‚’å–å¾—
                                folder_data_result = result_manager.get_folder_data_from_result(sib_folder_id)
                                
                                if folder_data_result['success']:
                                    folder_data = folder_data_result['data']
                                    clustering_ids = list(folder_data.keys())
                                    
                                    folder_captions = []
                                    for cid in clustering_ids:
                                        try:
                                            caption_res, _ = images_queries.select_caption_by_clustering_id(connect_session, cid)
                                            if caption_res:
                                                caption_row = caption_res.mappings().first()
                                                if caption_row and 'caption' in caption_row and caption_row['caption']:
                                                    caption = caption_row['caption']
                                                    folder_captions.append(caption)
                                                    all_captions.append(caption)
                                        except Exception as cap_e:
                                            print(f"       âš ï¸ ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³å–å¾—ã‚¨ãƒ©ãƒ¼ (clustering_id: {cid}): {cap_e}")
                                    
                                    folder_captions_map[sib_folder_id] = {
                                        'folder_name': sib_folder_name,
                                        'caption_count': len(folder_captions),
                                        'captions': folder_captions
                                    }
                            
                            # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’æº–å‚™
                            stopwords_set = set(CAPTION_STOPWORDS)
                            
                            # folder_captions_mapãŒç©ºã®å ´åˆã¯å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
                            if len(folder_captions_map) == 0:
                                print(f"    âš ï¸ ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚©ãƒ«ãƒ€ç‰¹å¾´åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                            else:
                                # å„ãƒ•ã‚©ãƒ«ãƒ€ã®å˜èªã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’ä½œæˆï¼ˆæ–‡ã®ä½ç½®ã«ã‚ˆã‚‹ãƒã‚¤ã‚¢ã‚¹ä»˜ãï¼‰
                                folder_word_counters = {}
                                for sib_folder_id, folder_info in folder_captions_map.items():
                                    folder_words = []
                                    for caption in folder_info['captions']:
                                        # ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’æ–‡ã«åˆ†å‰²ï¼ˆ.ã§åŒºåˆ‡ã‚‹ï¼‰
                                        sentences = caption.split('.')
                                        
                                        for sentence_idx, sentence in enumerate(sentences):
                                            if not sentence.strip():  # ç©ºã®æ–‡ã¯ã‚¹ã‚­ãƒƒãƒ—
                                                continue
                                            
                                            # æ–‡ã®ä½ç½®ã«ã‚ˆã‚‹é‡ã¿ï¼ˆ1æ–‡ç›®: 1.0, 2æ–‡ç›®: 0.85, 3æ–‡ç›®: 0.7ã€ãã‚Œä»¥é™: 0.6ï¼‰
                                            # æ¥µç«¯ã«ãªã‚‰ãªã„ã‚ˆã†ã«èª¿æ•´
                                            if sentence_idx == 0:
                                                position_weight = 1.0
                                            elif sentence_idx == 1:
                                                position_weight = 0.85
                                            elif sentence_idx == 2:
                                                position_weight = 0.7
                                            else:
                                                position_weight = 0.6
                                            
                                            words = re.findall(r'\b[a-z]+\b', sentence.lower())
                                            filtered_sentence_words = [w for w in words if w not in stopwords_set]
                                            
                                            # é‡ã¿ä»˜ãã§å˜èªã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆé‡ã¿ã«å¿œã˜ã¦è¤‡æ•°å›è¿½åŠ ï¼‰
                                            for word in filtered_sentence_words:
                                                # é‡ã¿ã‚’è€ƒæ…®ã™ã‚‹ãŸã‚ã€fractional countã¨ã—ã¦æ‰±ã†
                                                # Counterã¯æ•´æ•°ã—ã‹æ‰±ãˆãªã„ã®ã§ã€å¾Œã§ã‚¹ã‚³ã‚¢è¨ˆç®—æ™‚ã«é©ç”¨
                                                folder_words.append((word, position_weight))
                                    
                                    # é‡ã¿ä»˜ãã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’ä½œæˆ
                                    weighted_counter = {}
                                    for word, weight in folder_words:
                                        weighted_counter[word] = weighted_counter.get(word, 0.0) + weight
                                    
                                    folder_word_counters[sib_folder_id] = weighted_counter
                                
                                # folder_word_countersãŒç©ºã®å ´åˆã®ãƒã‚§ãƒƒã‚¯
                                if len(folder_word_counters) == 0:
                                    print(f"    âš ï¸ å˜èªã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ãŒç©ºã§ã™ã€‚åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                                else:
                                    # å„ãƒ•ã‚©ãƒ«ãƒ€ã®ç‰¹å¾´çš„ãªå˜èªã‚’æŠ½å‡ºï¼ˆæ”¹å–„ç‰ˆ: ãƒ•ã‚©ãƒ«ãƒ€ä»£è¡¨æ€§ã‚¹ã‚³ã‚¢ï¼‰
                                    folder_unique_words = {}
                                    TOP_N_UNIQUE_WORDS = 10  # å„ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰ä¸Šä½Nå€‹ã®ç‰¹å¾´çš„ãªå˜èªã‚’æŠ½å‡º
                                    
                                    # === ã‚°ãƒ­ãƒ¼ãƒãƒ«çµ±è¨ˆã®è¨ˆç®— ===
                                    num_folders = len(folder_word_counters)
                                    
                                    # å„å˜èªãŒä½•å€‹ã®ãƒ•ã‚©ãƒ«ãƒ€ã«å‡ºç¾ã™ã‚‹ã‹
                                    word_folder_count = {}
                                    # å„å˜èªã®å…¨ãƒ•ã‚©ãƒ«ãƒ€ã§ã®ç·å‡ºç¾å›æ•°
                                    word_total_count = {}
                                    
                                    for counter in folder_word_counters.values():
                                        for word, count in counter.items():
                                            if word not in word_folder_count:
                                                word_folder_count[word] = 0
                                                word_total_count[word] = 0.0
                                            word_folder_count[word] += 1
                                            word_total_count[word] += count
                                    
                                    # === å„ãƒ•ã‚©ãƒ«ãƒ€ã®å˜èªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— ===
                                    for target_folder_id, target_counter in folder_word_counters.items():
                                        # ã“ã®ãƒ•ã‚©ãƒ«ãƒ€ã®ç·ç”»åƒæ•°ã‚’å–å¾—
                                        folder_data_result = result_manager.get_folder_data_from_result(target_folder_id)
                                        total_images_in_folder = len(folder_data_result['data']) if folder_data_result['success'] else 1
                                        
                                        # folder_captions_mapã«å­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                                        if target_folder_id not in folder_captions_map:
                                            print(f"  âš ï¸ ãƒ•ã‚©ãƒ«ãƒ€ {target_folder_id} ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                                            continue
                                        
                                        # ã“ã®ãƒ•ã‚©ãƒ«ãƒ€å†…ã§å˜èªã‚’å«ã‚€ç”»åƒæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆä¸€è²«æ€§è¨ˆç®—ç”¨ï¼‰
                                        # é‡ã¿ä»˜ãã‚«ã‚¦ãƒ³ãƒˆã§ã¯ãªãã€ç´”ç²‹ãªç”»åƒæ•°
                                        word_image_count = {}
                                        for caption in folder_captions_map[target_folder_id]['captions']:
                                            words_in_caption = set(re.findall(r'\b[a-z]+\b', caption.lower())) - stopwords_set
                                            for word in words_in_caption:
                                                word_image_count[word] = word_image_count.get(word, 0) + 1
                                        
                                        word_scores = {}
                                        
                                        for word, count_in_target in target_counter.items():
                                            # === æŒ‡æ¨™1: TFï¼ˆTerm Frequencyï¼‰- æ–‡ä½ç½®é‡ã¿ä»˜ã ===
                                            tf = count_in_target / max(total_images_in_folder, 1)
                                            
                                            # === æŒ‡æ¨™2: ãƒ•ã‚©ãƒ«ãƒ€é›†ä¸­åº¦ï¼ˆConcentrationï¼‰ ===
                                            # ã“ã®å˜èªã®å…¨ä½“å‡ºç¾ã®ã†ã¡ã€ã“ã®ãƒ•ã‚©ãƒ«ãƒ€ã«ä½•%é›†ä¸­ã—ã¦ã„ã‚‹ã‹
                                            concentration = count_in_target / max(word_total_count.get(word, 1), 0.001)
                                            
                                            # === æŒ‡æ¨™3: ãƒ•ã‚©ãƒ«ãƒ€å†…ä¸€è²«æ€§ï¼ˆConsistencyï¼‰ ===
                                            # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ä½•%ã®ç”»åƒã«ã“ã®å˜èªãŒå‡ºç¾ã™ã‚‹ã‹
                                            num_images_with_word = word_image_count.get(word, 0)
                                            consistency = num_images_with_word / max(total_images_in_folder, 1)
                                            
                                            # === æŒ‡æ¨™4: ã‚°ãƒ­ãƒ¼ãƒãƒ«å¸Œå°‘æ€§ï¼ˆIDFï¼‰ ===
                                            num_folders_with_word = word_folder_count.get(word, 1)
                                            base_idf = math.log((num_folders + 1) / (num_folders_with_word + 1))
                                            
                                            # === ä»£è¡¨æ€§ã‚¹ã‚³ã‚¢ï¼ˆRepresentativeness Scoreï¼‰ ===
                                            # ãƒ•ã‚©ãƒ«ãƒ€ã®å†…å®¹ã‚’è¡¨ã™å˜èª
                                            score_repr = tf * concentration * (consistency ** 0.5) * 1000
                                            
                                            # === è­˜åˆ¥æ€§ã‚¹ã‚³ã‚¢ï¼ˆDistinctiveness Scoreï¼‰ ===
                                            # ä»–ãƒ•ã‚©ãƒ«ãƒ€ã¨åŒºåˆ¥ã™ã‚‹å˜èª
                                            score_dist = tf * base_idf * concentration * 100
                                            
                                            # === æœ€çµ‚ã‚¹ã‚³ã‚¢: ä»£è¡¨æ€§70% + è­˜åˆ¥æ€§30% ===
                                            final_score = 0.7 * score_repr + 0.3 * score_dist
                                            
                                            word_scores[word] = {
                                                'score': final_score,
                                                'score_repr': score_repr,
                                                'score_dist': score_dist,
                                                'tf': tf,
                                                'concentration': concentration,
                                                'consistency': consistency,
                                                'base_idf': base_idf,
                                                'count_in_folder': count_in_target,
                                                'num_images_with_word': num_images_with_word,
                                                'num_folders_with_word': num_folders_with_word,
                                                'total_count_all_folders': word_total_count.get(word, 0),
                                                'total_images': total_images_in_folder
                                            }
                                        
                                        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
                                        sorted_words = sorted(
                                            word_scores.items(), 
                                            key=lambda x: x[1]['score'], 
                                            reverse=True
                                        )
                                        
                                        # ä¸Šä½Nå€‹ã‚’å–å¾—
                                        top_unique = sorted_words[:TOP_N_UNIQUE_WORDS]
                                        
                                        # folder_captions_mapã«å­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
                                        folder_display_name = folder_captions_map.get(target_folder_id, {}).get('folder_name', str(target_folder_id))
                                        
                                        # å„å˜èªã®ä¸Šä½èªã‚’å–å¾—
                                        from nltk.corpus import wordnet as wn
                                        
                                        unique_words_with_hypernyms = []
                                        for word, info in top_unique:
                                            # WordNetã‹ã‚‰ä¸Šä½èªã‚’å–å¾—
                                            hypernym = 'N/A'
                                            try:
                                                synsets = wn.synsets(word)
                                                if synsets:
                                                    # æœ€åˆã®synsetã®æœ€ã‚‚ä¸€èˆ¬çš„ãªä¸Šä½èªã‚’å–å¾—
                                                    hypernyms = synsets[0].hypernyms()
                                                    if hypernyms:
                                                        # æœ€åˆã®ä¸Šä½èªã®åå‰ã‚’å–å¾—ï¼ˆ.name()ã‹ã‚‰å˜èªéƒ¨åˆ†ã®ã¿æŠ½å‡ºï¼‰
                                                        hypernym = hypernyms[0].name().split('.')[0]
                                            except Exception as e:
                                                print(f"      âš ï¸ '{word}'ã®ä¸Šä½èªå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                                            
                                            unique_words_with_hypernyms.append({
                                                'word': word,
                                                'hypernym': hypernym,
                                                'score': round(info['score'], 2),
                                                'score_repr': round(info['score_repr'], 2),
                                                'score_dist': round(info['score_dist'], 2),
                                                'tf': round(info['tf'], 4),
                                                'concentration': round(info['concentration'], 4),
                                                'consistency': round(info['consistency'], 4),
                                                'base_idf': round(info['base_idf'], 4),
                                                'count_in_folder': info['count_in_folder'],
                                                'num_images_with_word': info['num_images_with_word'],
                                                'num_folders_with_word': info['num_folders_with_word'],
                                                'total_count_all_folders': info['total_count_all_folders'],
                                                'total_images': info['total_images']
                                            })
                                        
                                        folder_unique_words[target_folder_id] = {
                                            'folder_name': folder_display_name,
                                            'unique_words': unique_words_with_hypernyms
                                        }
                                    
                                    # å„ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰ä¸Šä½10å€‹ã®ç‰¹å¾´çš„å˜èªã‚’å–å¾—
                                    folder_top_words_list = {}
                                    for folder_id, unique_info in folder_unique_words.items():
                                        top_10_words = [w['word'] for w in unique_info['unique_words'][:10]]
                                        folder_top_words_list[folder_id] = top_10_words
                                    
                                    # å…¨ãƒ•ã‚©ãƒ«ãƒ€ã«å…±é€šã™ã‚‹å˜èªã‚’ç‰¹å®š
                                    if len(folder_top_words_list) > 0:
                                        # å„ãƒ•ã‚©ãƒ«ãƒ€ã®å˜èªã‚»ãƒƒãƒˆã‚’ä½œæˆ
                                        folder_word_sets = [set(words) for words in folder_top_words_list.values()]
                                        # å…¨ãƒ•ã‚©ãƒ«ãƒ€ã«å…±é€šã™ã‚‹å˜èªã‚’å–å¾—
                                        common_to_all_folders = set.intersection(*folder_word_sets) if len(folder_word_sets) > 1 else set()
                                        
                                        if len(common_to_all_folders) > 0:
                                            print(f"\n    ğŸ” å…¨ãƒ•ã‚©ãƒ«ãƒ€å…±é€šå˜èª: {len(common_to_all_folders)}å€‹ã‚’é™¤å¤–")
                                            
                                            # å„ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒˆãƒƒãƒ—10å˜èªã‹ã‚‰å…±é€šå˜èªã‚’é™¤å¤–
                                            for folder_id in folder_top_words_list.keys():
                                                folder_top_words_list[folder_id] = [
                                                    w for w in folder_top_words_list[folder_id] 
                                                    if w not in common_to_all_folders
                                                ]
                                    
                                    # WordAnalyzerã‚’åˆæœŸåŒ–ï¼ˆæ—¢å­˜ã®WordNetãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ï¼‰
                                    from sentence_transformers import SentenceTransformer
                                    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
                                    word_analyzer = WordAnalyzer(embedding_model)
                                    
                                    # --- å…¨ãƒ•ã‚©ãƒ«ãƒ€ã§åŒã˜ã‚«ãƒ†ã‚´ãƒªã‚’æŒã¤å˜èªã®ã¿ã‚’æŠ½å‡º ---
                                    folder_ids_list = list(folder_unique_words.keys())
                                    
                                    # å„å˜èªãŒã©ã®ã‚«ãƒ†ã‚´ãƒªã«å±ã™ã‚‹ã‹ã‚’ãƒ•ã‚©ãƒ«ãƒ€ã”ã¨ã«åˆ†æ
                                    # {folder_id: {word: [(category, score, target_word), ...]}}
                                    folder_word_categories = {}
                                    
                                    for folder_id in folder_ids_list:
                                        folder_word_categories[folder_id] = {}
                                        folder_words = folder_top_words_list[folder_id]
                                        
                                        for word in folder_words:
                                            # ã“ã®å˜èªã¨ä»–ã®å…¨ãƒ•ã‚©ãƒ«ãƒ€ã®å˜èªã‚’æ¯”è¼ƒã—ã¦ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—
                                            word_category_info = []  # [(category, score, other_folder_word), ...]
                                            
                                            for other_folder_id in folder_ids_list:
                                                if other_folder_id == folder_id:
                                                    continue
                                                
                                                other_folder_words = folder_top_words_list[other_folder_id]
                                                
                                                for other_word in other_folder_words:
                                                    common_categories, category_score = word_analyzer.get_common_category(word, other_word)
                                                    
                                                    if len(common_categories) > 0 and category_score >= 3.0:
                                                        # ã‚¹ã‚³ã‚¢3.0ä»¥ä¸Šã®å…±é€šã‚«ãƒ†ã‚´ãƒªã®ã¿
                                                        for cat in common_categories[:1]:  # æœ€ä¸Šä½ã‚«ãƒ†ã‚´ãƒªã®ã¿
                                                            word_category_info.append((cat, category_score, other_word, other_folder_id))
                                            
                                            if len(word_category_info) > 0:
                                                folder_word_categories[folder_id][word] = word_category_info
                                    
                                    # å…¨ãƒ•ã‚©ãƒ«ãƒ€ã§å…±é€šã—ã¦å‡ºç¾ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã‚’ç‰¹å®š
                                    from collections import defaultdict
                                    category_occurrence = defaultdict(lambda: {
                                        'folders': set(),
                                        'words_by_folder': defaultdict(list),
                                        'word_category_scores': defaultdict(list)
                                    })
                                    
                                    # å„ãƒ•ã‚©ãƒ«ãƒ€ã®å„å˜èªãŒå±ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã‚’é›†è¨ˆ
                                    for folder_id, word_cats in folder_word_categories.items():
                                        for word, cat_info_list in word_cats.items():
                                            if len(cat_info_list) == 0:
                                                continue
                                            
                                            # ã“ã®wordãŒæœ€ã‚‚å±ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã‚’æ±ºå®šï¼ˆã‚¹ã‚³ã‚¢ã®å¹³å‡ãŒæœ€ã‚‚é«˜ã„ã‚«ãƒ†ã‚´ãƒªï¼‰
                                            cat_scores = defaultdict(list)
                                            for cat, score, other_word, other_folder_id in cat_info_list:
                                                cat_scores[cat].append(score)
                                            
                                            # å„ã‚«ãƒ†ã‚´ãƒªã®å¹³å‡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                                            best_category = None
                                            best_avg_score = -1
                                            for cat, scores in cat_scores.items():
                                                avg_score = sum(scores) / len(scores)
                                                if avg_score > best_avg_score:
                                                    best_avg_score = avg_score
                                                    best_category = cat
                                            
                                            if best_category:
                                                category_occurrence[best_category]['folders'].add(folder_id)
                                                category_occurrence[best_category]['words_by_folder'][folder_id].append((word, best_avg_score))
                                                category_occurrence[best_category]['word_category_scores'][word].append(best_avg_score)
                                    
                                    # å…¨ãƒ•ã‚©ãƒ«ãƒ€ã«å‡ºç¾ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                                    num_folders = len(folder_ids_list)
                                    common_categories_across_all_folders = {}
                                    
                                    for category, info in category_occurrence.items():
                                        if len(info['folders']) == num_folders:
                                            common_categories_across_all_folders[category] = info
                                    
                                    print(f"\n    ğŸ“Š å…±é€šã‚«ãƒ†ã‚´ãƒª: {len(common_categories_across_all_folders)}å€‹")
                                    
                                    # åˆ†é¡åŸºæº–ã®æ¨å®š
                                    classification_criteria = {}
                                    
                                    if len(common_categories_across_all_folders) > 0:
                                        # å„ã‚«ãƒ†ã‚´ãƒªã®å¹³å‡ã‚¹ã‚³ã‚¢ã¨å˜èªæ•°ã§è©•ä¾¡
                                        sorted_categories = []
                                        for category, info in common_categories_across_all_folders.items():
                                            # å„å˜èªã®å¹³å‡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                                            word_scores = []
                                            words_with_scores = []  # [(word, avg_score)]
                                            
                                            for word, scores_list in info['word_category_scores'].items():
                                                avg_score = sum(scores_list) / len(scores_list)
                                                word_scores.append(avg_score)
                                                words_with_scores.append((word, avg_score))
                                            
                                            # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
                                            words_with_scores.sort(key=lambda x: x[1], reverse=True)
                                            
                                            category_avg_score = sum(word_scores) / len(word_scores) if len(word_scores) > 0 else 0.0
                                            
                                            sorted_categories.append((category, {
                                                'words_with_scores': words_with_scores,
                                                'avg_score': category_avg_score,
                                                'word_count': len(words_with_scores),
                                                'folders': info['folders']
                                            }))
                                        
                                        # å¹³å‡ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
                                        sorted_categories.sort(key=lambda x: x[1]['avg_score'], reverse=True)
                                        
                                        for rank, (category, info) in enumerate(sorted_categories[:5], 1):
                                            classification_criteria[category] = {
                                                'rank': rank,
                                                'category': category,
                                                'words': [w for w, s in info['words_with_scores']],
                                                'words_with_scores': info['words_with_scores'],
                                                'word_count': info['word_count'],
                                                'avg_score': round(info['avg_score'], 2),
                                                'folders': sorted([folder_unique_words[fid]['folder_name'] for fid in info['folders']])
                                            }
                                        
                                        # æœ€ã‚‚æ”¯é…çš„ãªã‚«ãƒ†ã‚´ãƒªã‚’åˆ†é¡åŸºæº–ã¨ã—ã¦ç‰¹å®š
                                        if len(sorted_categories) > 0:
                                            top_category = sorted_categories[0][0]
                                    else:
                                        print(f"       âš ï¸ å…¨ãƒ•ã‚©ãƒ«ãƒ€ã«å…±é€šã™ã‚‹ã‚«ãƒ†ã‚´ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                            
                            # ãƒ‡ãƒãƒƒã‚°ç”¨JSONå‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
                            debug_output = {
                                'summary': {
                                    'total_captions': len(all_captions),
                                    'sibling_leaf_folder_count': len(sibling_leaf_folders),
                                    'common_categories_count': len(common_categories_across_all_folders) if 'common_categories_across_all_folders' in locals() else 0,
                                    'classification_criteria_count': len(classification_criteria)
                                },
                                'classification_criteria': classification_criteria,
                                'folder_unique_words': folder_unique_words
                            }
                            
                            # JSONå½¢å¼ã§å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ - ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰
                            # import json
                            # from datetime import datetime
                            # 
                            # # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                            # timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            # json_filename = f"sibling_captions_analysis_{timestamp}.json"
                            # json_filepath = os.path.join("./", json_filename)
                            # 
                            # try:
                            #     with open(json_filepath, 'w', encoding='utf-8') as f:
                            #         json.dump(debug_output, f, indent=2, ensure_ascii=False)
                            # except Exception as json_e:
                            #     print(f"    âš ï¸ JSONä¿å­˜ã‚¨ãƒ©ãƒ¼: {json_e}")
                            
                            # æ–°è¦ç”»åƒã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—
                            try:
                                new_image_caption_res, _ = images_queries.select_caption_by_clustering_id(connect_session, clustering_id)
                                new_image_caption = None
                                if new_image_caption_res:
                                    caption_row = new_image_caption_res.mappings().first()
                                    if caption_row and 'caption' in caption_row and caption_row['caption']:
                                        new_image_caption = caption_row['caption'].lower()
                                
                                if new_image_caption:
                                    # åˆ†é¡åŸºæº–ã‹ã‚‰æœ€ä¸Šä½ã‚«ãƒ†ã‚´ãƒªã®ã¿ã‚’ä½¿ç”¨
                                    if len(classification_criteria) > 0:
                                        # rankã§ã‚½ãƒ¼ãƒˆï¼ˆã™ã§ã«rankãŒã¤ã„ã¦ã„ã‚‹ï¼‰
                                        sorted_criteria = sorted(
                                            classification_criteria.items(),
                                            key=lambda x: x[1].get('rank', 999)
                                        )
                                        
                                        # æœ€ä¸Šä½ã‚«ãƒ†ã‚´ãƒªã®ã¿ã‚’ä½¿ç”¨
                                        top_category, top_info = sorted_criteria[0]
                                        top_category_words_with_scores = top_info.get('words_with_scores', [])
                                        
                                        # æ–°è¦ç”»åƒã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‹ã‚‰å˜èªã‚’æŠ½å‡º
                                        new_image_words = set(re.findall(r'\b[a-z]+\b', new_image_caption))
                                        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å¤–
                                        new_image_words = new_image_words - stopwords_set
                                        
                                        # é™¤å¤–ã™ã‚‹å˜èªã‚»ãƒƒãƒˆ: ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ + å…¨ãƒ•ã‚©ãƒ«ãƒ€å…±é€šå˜èª
                                        exclude_words_for_matching = stopwords_set.copy()
                                        common_words_count = 0
                                        if 'common_to_all_folders' in locals() and len(common_to_all_folders) > 0:
                                            exclude_words_for_matching.update(common_to_all_folders)
                                            common_words_count = len(common_to_all_folders)
                                        
                                        # Step 0: é«˜ã‚¹ã‚³ã‚¢å˜èªã«ã‚ˆã‚‹å³åº§ã®ãƒ•ã‚©ãƒ«ãƒ€æ±ºå®š
                                        high_score_matches = []
                                        
                                        # ç”»åƒé¡ä¼¼åº¦ã¨æ–‡ç« é¡ä¼¼åº¦ã®é–¾å€¤è¨­å®š
                                        IMAGE_SIMILARITY_THRESHOLD = 0.85
                                        SENTENCE_SIMILARITY_THRESHOLD = 0.75
                                        
                                        # å„ãƒ•ã‚©ãƒ«ãƒ€ã®é«˜ã‚¹ã‚³ã‚¢å˜èªï¼ˆé–¾å€¤ä»¥ä¸Šï¼‰ã‚’ãƒã‚§ãƒƒã‚¯
                                        for sib_folder in sibling_leaf_folders:
                                            sib_folder_id = sib_folder['id']
                                            sib_folder_name = sib_folder['name']
                                            
                                            if sib_folder_id not in folder_unique_words:
                                                continue
                                            
                                            folder_high_score_words = []
                                            # ã“ã®ãƒ•ã‚©ãƒ«ãƒ€ã®é«˜ã‚¹ã‚³ã‚¢å˜èªã‚’æŠ½å‡º
                                            for w_info in folder_unique_words[sib_folder_id]['unique_words']:
                                                if w_info['score'] >= TFIDF_SCORE_THRESHOLDS['high']:
                                                    folder_high_score_words.append(w_info)
                                            
                                            if len(folder_high_score_words) == 0:
                                                continue
                                            
                                            # ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³å†…ã®å˜èªã¨ç…§åˆ
                                            matched_high_score_words = []
                                            for w_info in folder_high_score_words:
                                                if w_info['word'] in new_image_words and w_info['word'] not in exclude_words_for_matching:
                                                    matched_high_score_words.append(w_info)
                                            
                                            if len(matched_high_score_words) > 0:
                                                # æœ€é«˜ã‚¹ã‚³ã‚¢ã®å˜èªã‚’é¸æŠ
                                                best_match = max(matched_high_score_words, key=lambda x: x['score'])
                                                
                                                # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®clustering_idã‚’å–å¾—
                                                folder_data_result = result_manager.get_folder_data_from_result(sib_folder_id)
                                                if not folder_data_result['success']:
                                                    print(f"             âš ï¸ ãƒ•ã‚©ãƒ«ãƒ€ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—")
                                                    continue
                                                
                                                folder_data = folder_data_result['data']
                                                clustering_ids_in_folder = list(folder_data.keys())
                                                
                                                # å„ç”»åƒã¨ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
                                                image_similarities = []
                                                for cid in clustering_ids_in_folder:
                                                    try:
                                                        # clustering_idã‹ã‚‰ç”»åƒIDã¨æ–‡ç« IDã‚’å–å¾—
                                                        img_result, _ = action_queries.get_chromadb_image_id_by_clustering_id(connect_session, cid, project_id)
                                                        sent_result, _ = action_queries.get_chromadb_sentence_id_by_clustering_id(connect_session, cid, project_id)
                                                        
                                                        if not img_result or not sent_result:
                                                            continue
                                                        
                                                        img_mapping = img_result.mappings().first()
                                                        sent_mapping = sent_result.mappings().first()
                                                        
                                                        if not img_mapping or not sent_mapping:
                                                            continue
                                                        
                                                        folder_image_id = img_mapping['chromadb_image_id']
                                                        folder_sentence_id = sent_mapping['chromadb_sentence_id']
                                                        
                                                        # ç”»åƒåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
                                                        folder_img_data = image_db.get_data_by_ids([folder_image_id])
                                                        folder_img_embedding = folder_img_data['embeddings'][0] if folder_img_data['embeddings'] else None
                                                        
                                                        # æ–‡ç« åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
                                                        folder_sent_data = sentence_name_db.get_data_by_ids([folder_sentence_id])
                                                        folder_sent_embedding = folder_sent_data['embeddings'][0] if folder_sent_data['embeddings'] else None
                                                        
                                                        if folder_img_embedding is None or folder_sent_embedding is None:
                                                            continue
                                                        
                                                        if new_image_embedding is None or new_sentence_embedding is None:
                                                            continue
                                                        
                                                        # ç”»åƒé¡ä¼¼åº¦ã‚’è¨ˆç®—
                                                        img_sim = float(np.dot(new_image_embedding, folder_img_embedding) / (
                                                            np.linalg.norm(new_image_embedding) * np.linalg.norm(folder_img_embedding)
                                                        ))
                                                        
                                                        # æ–‡ç« é¡ä¼¼åº¦ã‚’è¨ˆç®—
                                                        sent_sim = float(np.dot(new_sentence_embedding, folder_sent_embedding) / (
                                                            np.linalg.norm(new_sentence_embedding) * np.linalg.norm(folder_sent_embedding)
                                                        ))
                                                        
                                                        image_similarities.append({
                                                            'clustering_id': cid,
                                                            'image_similarity': img_sim,
                                                            'sentence_similarity': sent_sim
                                                        })
                                                        
                                                    except Exception as sim_e:
                                                        continue
                                                
                                                # ç”»åƒé¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
                                                image_similarities.sort(key=lambda x: x['image_similarity'], reverse=True)
                                                
                                                # ç”»åƒé¡ä¼¼åº¦ãŒé«˜ã„é †ã«ã€ä¸¡æ–¹ã®é–¾å€¤ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹ã‚‚ã®ã‚’æ¢ã™
                                                best_matching_image = None
                                                for sim_info in image_similarities:
                                                    if (sim_info['image_similarity'] >= IMAGE_SIMILARITY_THRESHOLD and 
                                                        sim_info['sentence_similarity'] >= SENTENCE_SIMILARITY_THRESHOLD):
                                                        best_matching_image = sim_info
                                                        break
                                                
                                                # ä¸¡æ–¹ã®é–¾å€¤ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹ç”»åƒãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã®ã¿ã€ã“ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ãƒãƒƒãƒå€™è£œã«è¿½åŠ 
                                                if best_matching_image is not None:
                                                    high_score_matches.append({
                                                        'folder': sib_folder,
                                                        'matched_words': matched_high_score_words,
                                                        'best_word': best_match['word'],
                                                        'best_score': best_match['score'],
                                                        'image_similarity': best_matching_image['image_similarity'],
                                                        'sentence_similarity': best_matching_image['sentence_similarity'],
                                                        'matching_clustering_id': best_matching_image['clustering_id']
                                                    })
                                        
                                        # é«˜ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã€å³åº§ã«ãƒ•ã‚©ãƒ«ãƒ€ã‚’æ±ºå®š
                                        if len(high_score_matches) > 0:
                                            # è¤‡æ•°ãƒãƒƒãƒãŒã‚ã‚‹å ´åˆã®é¸æŠãƒ­ã‚¸ãƒƒã‚¯
                                            if len(high_score_matches) == 1:
                                                best_match = high_score_matches[0]
                                            else:
                                                # è¤‡æ•°ã‚ã‚‹å ´åˆã€ç”»åƒé¡ä¼¼åº¦ãŒæœ€ã‚‚é«˜ã„ã‚‚ã®ã‚’é¸æŠ
                                                best_match = max(high_score_matches, key=lambda x: x['image_similarity'])
                                            
                                            print(f"\n       â­ Step 0: é«˜ã‚¹ã‚³ã‚¢ãƒãƒƒãƒ '{best_match['best_word']}' â†’ '{best_match['folder']['name']}'")
                                            
                                            # å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¨˜éŒ²
                                            report_data['processing_steps'].append('Step 0: é«˜ã‚¹ã‚³ã‚¢å˜èªã«ã‚ˆã‚‹å³åº§ã®ãƒ•ã‚©ãƒ«ãƒ€æ±ºå®š')
                                            report_data['decision_step'] = 'STEP_0_HIGH_SCORE_MATCH'
                                            report_data['decision_reason'] = f"å˜èª'{best_match['best_word']}'ãŒé«˜ã‚¹ã‚³ã‚¢({best_match['best_score']:.2f})ã§ãƒ•ã‚©ãƒ«ãƒ€'{best_match['folder']['name']}'ã«ãƒãƒƒãƒ"
                                            report_data['matched_word'] = best_match['best_word']
                                            report_data['matched_score'] = best_match['best_score']
                                            
                                            # æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã«æŒ¿å…¥
                                            target_folder_id_by_criteria = best_match['folder']['id']
                                        
                                        # é«˜ã‚¹ã‚³ã‚¢ãƒãƒƒãƒãŒãªã„å ´åˆã®ã¿ã€å¾“æ¥ã®ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒãƒ³ã‚°ã‚’å®Ÿè¡Œ
                                        if len(high_score_matches) == 0:
                                        
                                            # Step 1: æ–°è¦ç”»åƒã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³å†…ã®å˜èªã‹ã‚‰ã€åˆ†é¡åŸºæº–ã‚«ãƒ†ã‚´ãƒª(top_category)ã«å±ã™ã‚‹å˜èªã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                                            category_words_in_caption = []
                                            excluded_by_filter = []
                                            checked_but_not_matched = []
                                            word_matching_details = []
                                            for new_word in sorted(list(new_image_words)):
                                                if new_word in exclude_words_for_matching:
                                                    excluded_by_filter.append(new_word)
                                                    continue
                                                
                                                max_score_for_word = -1
                                                belongs_to_category = False
                                                best_match_word = None
                                                match_details = []
                                                
                                                for category_word, cat_word_score in top_category_words_with_scores:
                                                    common_categories, category_score = word_analyzer.get_common_category(new_word, category_word)
                                                    
                                                    if len(common_categories) > 0:
                                                        match_details.append({
                                                            'category_word': category_word,
                                                            'common_categories': common_categories,
                                                            'category_score': category_score,
                                                            'matched': common_categories[0] == top_category and category_score >= 3.0
                                                        })
                                                    
                                                    if len(common_categories) > 0 and common_categories[0] == top_category and category_score >= 3.0:
                                                        belongs_to_category = True
                                                        if category_score > max_score_for_word:
                                                            max_score_for_word = category_score
                                                            best_match_word = category_word
                                                
                                                word_matching_details.append({
                                                    'word': new_word,
                                                    'matched': belongs_to_category,
                                                    'max_score': max_score_for_word,
                                                    'best_match_word': best_match_word,
                                                    'match_details': match_details
                                                })
                                                
                                                if belongs_to_category:
                                                    category_words_in_caption.append((new_word, max_score_for_word))
                                                else:
                                                    checked_but_not_matched.append(new_word)
                                            
                                            # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
                                            category_words_in_caption.sort(key=lambda x: x[1], reverse=True)
                                            
                                            print(f"\n       â­ Step 1: {len(category_words_in_caption)}å€‹ã®ã‚«ãƒ†ã‚´ãƒªå˜èªã‚’æŠ½å‡º")
                                            report_data['processing_steps'].append(f'Step 1: ã‚«ãƒ†ã‚´ãƒª\'{top_category}\'ã«å±ã™ã‚‹å˜èªã‚’{len(category_words_in_caption)}å€‹æŠ½å‡º')
                                            report_data['processing_steps'].append(f'Step 1: ã‚«ãƒ†ã‚´ãƒª\'{top_category}\'ã«å±ã™ã‚‹å˜èªã‚’{len(category_words_in_caption)}å€‹æŠ½å‡º')
                                            
                                            if len(category_words_in_caption) == 0:
                                                print(f"       âš ï¸ ã‚«ãƒ†ã‚´ãƒª '{top_category}' ã«å±ã™ã‚‹å˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                                            
                                            # Step 2: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ãŸå˜èªã¨æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã®ç‰¹å¾´çš„ãªå˜èªã‚’ç…§åˆ
                                            if len(category_words_in_caption) > 0:
                                                print(f"\n       â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
                                                print(f"       ğŸ” Step 2: æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã®ç‰¹å¾´çš„ãªå˜èªã¨ç…§åˆ...")
                                                print(f"          ğŸ“‹ ç…§åˆå¯¾è±¡ã®å˜èª: {[w for w, s in category_words_in_caption]}")
                                                print(f"          ğŸ“‚ ç…§åˆå¯¾è±¡ã®ãƒ•ã‚©ãƒ«ãƒ€æ•°: {len(sibling_leaf_folders)}å€‹")
                                                print(f"\n       ğŸ” ãƒ•ã‚©ãƒ«ãƒ€ç…§åˆã®è©³ç´°:")
                                                
                                                folder_candidates = []
                                                words_checked_count = {}
                                                folders_checked_for_word = {}
                                                
                                                # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ãŸå„å˜èªã«ã¤ã„ã¦ã€æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã®ç‰¹å¾´çš„ãªå˜èªã«å«ã¾ã‚Œã‚‹ã‹ç¢ºèª
                                                for new_word, word_category_score in category_words_in_caption:
                                                    words_checked_count[new_word] = 0
                                                    folders_checked_for_word[new_word] = []
                                                    print(f"\n          ğŸ” '{new_word}' ã‚’å„ãƒ•ã‚©ãƒ«ãƒ€ã®ç‰¹å¾´çš„å˜èªã¨ç…§åˆä¸­...")
                                                    # å„å…„å¼Ÿãƒ•ã‚©ãƒ«ãƒ€ã®ç‰¹å¾´çš„ãªå˜èªãƒªã‚¹ãƒˆã‚’ãƒã‚§ãƒƒã‚¯
                                                    for sib_folder in sibling_leaf_folders:
                                                        sib_folder_id = sib_folder['id']
                                                        sib_folder_name = sib_folder['name']
                                                        words_checked_count[new_word] += 1
                                                        
                                                        # ã“ã®ãƒ•ã‚©ãƒ«ãƒ€ã®ç‰¹å¾´çš„ãªå˜èªã‚’å–å¾—
                                                        if sib_folder_id in folder_unique_words:
                                                            folder_unique_list = folder_unique_words[sib_folder_id]['unique_words']
                                                            folder_words_list = [w['word'] for w in folder_unique_list]
                                                            folders_checked_for_word[new_word].append({
                                                                'folder_name': sib_folder_name,
                                                                'folder_id': sib_folder_id,
                                                                'unique_words': folder_words_list[:5]  # ä¸Šä½5å€‹
                                                            })
                                                            
                                                            # ç‰¹å¾´çš„ãªå˜èªãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
                                                            for w_info in folder_unique_list:
                                                                if w_info['word'] == new_word:
                                                                    # ä¸€è‡´ã—ãŸï¼
                                                                    folder_tf_idf_score = w_info['score']
                                                                    
                                                                    # ç·åˆã‚¹ã‚³ã‚¢ = ãƒ•ã‚©ãƒ«ãƒ€ã®TF-IDFã‚¹ã‚³ã‚¢ + ã‚«ãƒ†ã‚´ãƒªã‚¹ã‚³ã‚¢
                                                                    combined_score = folder_tf_idf_score + (word_category_score * 0.1)
                                                                    
                                                                    folder_candidates.append({
                                                                        'folder': sib_folder,
                                                                        'word': new_word,
                                                                        'folder_score': folder_tf_idf_score,
                                                                        'category_score': word_category_score,
                                                                        'combined_score': combined_score
                                                                    })
                                                                    
                                                                    print(f"             âœ… ãƒãƒƒãƒï¼ãƒ•ã‚©ãƒ«ãƒ€ '{sib_folder_name}'")
                                                                    print(f"                â””â”€ TF-IDF: {folder_tf_idf_score:.2f}, ã‚«ãƒ†ã‚´ãƒª: {word_category_score:.2f}, ç·åˆ: {combined_score:.2f}")
                                                                    break
                                                    
                                                    # ã“ã®å˜èªã§ãƒãƒƒãƒã—ãªã‹ã£ãŸãƒ•ã‚©ãƒ«ãƒ€ã®æƒ…å ±ã‚’å‡ºåŠ›
                                                    if words_checked_count[new_word] == len(sibling_leaf_folders) and len([c for c in folder_candidates if c['word'] == new_word]) == 0:
                                                        print(f"             âŒ ã©ã®ãƒ•ã‚©ãƒ«ãƒ€ã®ç‰¹å¾´çš„å˜èªã«ã‚‚å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
                                                        print(f"             ğŸ“Š ç¢ºèªã—ãŸãƒ•ã‚©ãƒ«ãƒ€: {words_checked_count[new_word]}å€‹")
                                                
                                                print(f"\n       â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
                                                print(f"       ğŸ“Š Step 2 çµæœã‚µãƒãƒªãƒ¼:")
                                                print(f"          ğŸ¯ ãƒãƒƒãƒã—ãŸå€™è£œæ•°: {len(folder_candidates)}å€‹")
                                                report_data['processing_steps'].append(f'Step 2: æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã¨ã®ç…§åˆã§{len(folder_candidates)}å€‹ã®å€™è£œã‚’ç™ºè¦‹')
                                                if len(folder_candidates) > 0:
                                                    print(f"          ğŸ“‹ å€™è£œãƒªã‚¹ãƒˆ:")
                                                    for idx, candidate in enumerate(sorted(folder_candidates, key=lambda x: x['combined_score'], reverse=True)[:5], 1):
                                                        print(f"             {idx}. '{candidate['word']}' â†’ '{candidate['folder']['name']}' (ç·åˆ: {candidate['combined_score']:.2f})")
                                                print(f"       â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
                                                
                                                # Step 3: å€™è£œã‹ã‚‰æœ€é©ãªãƒ•ã‚©ãƒ«ãƒ€ã‚’é¸æŠ
                                                print(f"\n       ğŸ¯ Step 3: æœ€é©ãªãƒ•ã‚©ãƒ«ãƒ€ã‚’é¸æŠ...")
                                                matched_folder = None
                                                matched_word = None
                                                
                                                if len(folder_candidates) > 0:
                                                    # ç·åˆã‚¹ã‚³ã‚¢ãŒé«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
                                                    folder_candidates.sort(key=lambda x: x['combined_score'], reverse=True)
                                                    best_candidate = folder_candidates[0]
                                                    matched_folder = best_candidate['folder']
                                                    matched_word = best_candidate['word']
                                                    
                                                    print(f"\n       â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
                                                    print(f"       â­ æœ€çµ‚æ±ºå®š:")
                                                    print(f"          ğŸ¯ é¸æŠã•ã‚ŒãŸå˜èª: '{matched_word}'")
                                                    print(f"          ğŸ“‚ æŒ¿å…¥å…ˆãƒ•ã‚©ãƒ«ãƒ€: '{matched_folder['name']}'")
                                                    print(f"          ğŸ“Š ã‚¹ã‚³ã‚¢è©³ç´°:")
                                                    print(f"             - TF-IDFã‚¹ã‚³ã‚¢: {best_candidate['folder_score']:.2f}")
                                                    print(f"             - ã‚«ãƒ†ã‚´ãƒªã‚¹ã‚³ã‚¢: {best_candidate['category_score']:.2f}")
                                                    print(f"             - ç·åˆã‚¹ã‚³ã‚¢: {best_candidate['combined_score']:.2f}")
                                                    print(f"          ğŸ”¢ ãƒ•ã‚©ãƒ«ãƒ€ID: {matched_folder['id']}")
                                                    print(f"       â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
                                                    
                                                    # æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã«æŒ¿å…¥
                                                    target_folder_id_by_criteria = matched_folder['id']
                                                
                                                else:
                                                    # æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã«ãƒãƒƒãƒã™ã‚‹å˜èªãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸ â†’ æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
                                                    # ã‚«ãƒ†ã‚´ãƒªã«æœ€ã‚‚å±ã™ã‚‹å˜èªï¼ˆæœ€é«˜ã‚¹ã‚³ã‚¢ï¼‰ã‚’ä½¿ç”¨
                                                    new_folder_word = category_words_in_caption[0][0]
                                                    
                                                    print(f"\n       â„¹ï¸ æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã«ä¸€è‡´ã™ã‚‹å˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                                                    print(f"       ğŸ†• æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆäºˆå®š: ãƒ•ã‚©ãƒ«ãƒ€å='{new_folder_word}'")
                                                    
                                                    report_data['processing_steps'].append('Step 3: æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã«ãƒãƒƒãƒã›ãšã€æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆã‚’æ±ºå®š')
                                                    report_data['decision_step'] = 'STEP_3_NEW_FOLDER_REQUIRED'
                                                    report_data['decision_reason'] = f'ã‚«ãƒ†ã‚´ãƒªå˜èªã¯è¦‹ã¤ã‹ã£ãŸãŒã€æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã«ãƒãƒƒãƒã›ãšã€\'{new_folder_word}\'ã§æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ'
                                                    
                                                    report_data['processing_steps'].append('Step 3: æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã«ãƒãƒƒãƒã›ãšã€æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆã‚’æ±ºå®š')
                                                    report_data['decision_step'] = 'STEP_3_NEW_FOLDER_REQUIRED'
                                                    report_data['decision_reason'] = f'ã‚«ãƒ†ã‚´ãƒªå˜èªã¯è¦‹ã¤ã‹ã£ãŸãŒã€æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã«ãƒãƒƒãƒã›ãšã€\'{new_folder_word}\'ã§æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ'
                                                    
                                                    # è¦ªãƒ•ã‚©ãƒ«ãƒ€IDã‚’å–å¾—ï¼ˆbest_folderã¨åŒã˜éšå±¤ï¼‰
                                                    parent_id_for_new = parent_id_of_best if 'parent_id_of_best' in locals() else None
                                                    
                                                    # ç”»åƒãƒ‘ã‚¹ã‚’å–å¾—
                                                    path_result_temp, _ = action_queries.get_image_name_by_id(connect_session, image_id)
                                                    image_path_temp = path_result_temp.mappings().first()['name']
                                                    
                                                    # æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
                                                    create_result = result_manager.create_new_leaf_folder(
                                                        folder_name=new_folder_word,
                                                        parent_id=parent_id_for_new,
                                                        initial_clustering_id=clustering_id,
                                                        initial_image_path=image_path_temp
                                                    )
                                                    
                                                    if create_result['success']:
                                                        new_folder_id = create_result['folder_id']
                                                        target_folder_id_by_criteria = new_folder_id
                                                        print(f"       âœ… æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆæˆåŠŸ: '{new_folder_word}' (ID: {new_folder_id})")
                                                        
                                                        # user_image_clustering_statesã‚’æ›´æ–°
                                                        _, _ = action_queries.update_user_image_state_for_image(connect_session, user_id, image_id, new_count)                                                    
                                                        # æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿½åŠ 
                                                        if new_sentence_embedding is not None:
                                                            folder_sentence_embeddings[new_folder_id] = new_sentence_embedding
                                                        if new_image_embedding is not None:
                                                            folder_image_embeddings[new_folder_id] = new_image_embedding                                                    
                                                        # leaf_foldersãƒªã‚¹ãƒˆã«ã‚‚è¿½åŠ 
                                                        leaf_folders.append({
                                                            'id': new_folder_id,
                                                            'name': new_folder_word,
                                                            'parent_id': parent_id_for_new,
                                                            'is_leaf': True
                                                        })
                                                        
                                                        # sibling_leaf_foldersã«ã‚‚è¿½åŠ 
                                                        sibling_leaf_folders.append({
                                                            'id': new_folder_id,
                                                            'name': new_folder_word,
                                                            'parent_id': parent_id_for_new,
                                                            'is_leaf': True
                                                            })
                                                        
                                                        print(f"       â„¹ï¸ æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆã«ã‚ˆã‚Šã€å¾Œç¶šã®æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€æŒ¿å…¥å‡¦ç†ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                                                        # æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆãŒæˆåŠŸã—ãŸã®ã§ã€å¾Œç¶šã®æŒ¿å…¥å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
                                                        continue
                                                    else:
                                                        print(f"       âŒ æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆå¤±æ•—: {create_result.get('error', 'Unknown error')}")
                                                        print(f"       â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦æœ€ã‚‚é¡ä¼¼ã—ãŸãƒ•ã‚©ãƒ«ãƒ€ã«æŒ¿å…¥ã—ã¾ã™")
                                                        target_folder_id_by_criteria = None
                                            else:
                                                print(f"       â„¹ï¸ ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³å†…ã«åˆ†é¡åŸºæº–ã‚«ãƒ†ã‚´ãƒªã«å±ã™ã‚‹å˜èªãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
                                                print(f"       â†’ æœ€ã‚‚é¡ä¼¼ã—ãŸãƒ•ã‚©ãƒ«ãƒ€ï¼ˆ{folder_name}ï¼‰ã«æŒ¿å…¥ã—ã¾ã™")
                                        else:
                                            print(f"       â„¹ï¸ åˆ†é¡åŸºæº–ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“")
                                            print(f"       â†’ æœ€ã‚‚é¡ä¼¼ã—ãŸãƒ•ã‚©ãƒ«ãƒ€ï¼ˆ{folder_name}ï¼‰ã«æŒ¿å…¥ã—ã¾ã™")
                                    else:
                                        print(f"       âš ï¸ æ–°è¦ç”»åƒã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                                        print(f"       â†’ æœ€ã‚‚é¡ä¼¼ã—ãŸãƒ•ã‚©ãƒ«ãƒ€ï¼ˆ{folder_name}ï¼‰ã«æŒ¿å…¥ã—ã¾ã™")
                                    
                                    # ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã«å…„å¼Ÿãƒ•ã‚©ãƒ«ãƒ€ã®TF-IDFã‚¹ã‚³ã‚¢è¡¨ã‚’è¿½åŠ 
                                    if 'folder_unique_words' in locals() and len(folder_unique_words) > 0:
                                        report_data['sibling_folder_tfidf_scores'] = folder_unique_words
                                        report_data['classification_criteria_process_executed'] = True
                                        if 'classification_criteria' in locals():
                                            report_data['classification_criteria_details'] = classification_criteria

                            except Exception as criteria_e:
                                print(f"       âš ï¸ åˆ†é¡åŸºæº–ã«ã‚ˆã‚‹æŒ¯ã‚Šåˆ†ã‘å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {criteria_e}")
                                traceback.print_exc()
                                print(f"       â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦æœ€ã‚‚é¡ä¼¼ã—ãŸãƒ•ã‚©ãƒ«ãƒ€ã«æŒ¿å…¥ã—ã¾ã™")
                    except Exception as sib_e:
                        print(f"    âš ï¸ åŒéšå±¤ãƒ•ã‚©ãƒ«ãƒ€å–å¾—ã‚¨ãƒ©ãƒ¼: {sib_e}")
                        traceback.print_exc()
                    
                    # ãƒ•ã‚©ãƒ«ãƒ€é¸æŠãŒã†ã¾ãã„ã‹ãªã‹ã£ãŸå ´åˆã€å…¨ã¦ã®ç”»åƒã‹ã‚‰æœ€ã‚‚é¡ä¼¼ã—ãŸã‚‚ã®ã‚’æ¢ã—ã¦ãã®ãƒ•ã‚©ãƒ«ãƒ€ã«æŒ¿å…¥
                    if target_folder_id_by_criteria is None:
                        print(f"\n    ğŸ” å…¨ç”»åƒã‹ã‚‰æœ€ã‚‚é¡ä¼¼ã—ãŸç”»åƒã‚’æ¤œç´¢ä¸­...")
                        
                        try:
                            # å…„å¼Ÿãƒ•ã‚©ãƒ«ãƒ€ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°å…¨ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½¿ç”¨
                            folders_to_check = sibling_leaf_folders if 'sibling_leaf_folders' in locals() and len(sibling_leaf_folders) > 0 else leaf_folders
                            print(f"       å¯¾è±¡ãƒ•ã‚©ãƒ«ãƒ€æ•°: {len(folders_to_check)}å€‹ ({'å…„å¼Ÿãƒ•ã‚©ãƒ«ãƒ€ã®ã¿' if 'sibling_leaf_folders' in locals() and len(sibling_leaf_folders) > 0 else 'å…¨ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€'})")
                            
                            max_image_similarity = -1
                            max_sentence_similarity = -1
                            best_matching_folder_id = None
                            best_matching_image_info = {}
                            all_image_similarities = []
                            
                            # å„ãƒ•ã‚©ãƒ«ãƒ€å†…ã®å…¨ç”»åƒã¨æ¯”è¼ƒ
                            for folder in folders_to_check:
                                folder_id = folder['id']
                                folder_name = folder['name']
                                
                                # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®clustering_idã‚’å–å¾—
                                folder_data_result = result_manager.get_folder_data_from_result(folder_id)
                                if not folder_data_result['success']:
                                    print(f"       âš ï¸ ãƒ•ã‚©ãƒ«ãƒ€ {folder_name} (ID: {folder_id}) ã®ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—")
                                    continue
                                
                                folder_data = folder_data_result['data']
                                clustering_ids = list(folder_data.keys())
                                
                                if len(clustering_ids) == 0:
                                    print(f"       âš ï¸ ãƒ•ã‚©ãƒ«ãƒ€ {folder_name} (ID: {folder_id}) ã¯ç©ºã§ã™")
                                    continue
                                
                                # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®å…¨ç”»åƒã¨æ¯”è¼ƒ
                                for cid in clustering_ids:
                                    try:
                                        # chromadb_image_idã¨chromadb_sentence_idã‚’å–å¾—
                                        img_result, _ = action_queries.get_chromadb_image_id_by_clustering_id(connect_session, cid, project_id)
                                        if not img_result:
                                            continue
                                        
                                        img_mapping = img_result.mappings().first()
                                        if not img_mapping:
                                            continue
                                        
                                        chromadb_img_id = img_mapping['chromadb_image_id']
                                        
                                        sent_result, _ = action_queries.get_chromadb_sentence_id_by_clustering_id(connect_session, cid, project_id)
                                        if not sent_result:
                                            continue
                                        
                                        sent_mapping = sent_result.mappings().first()
                                        if not sent_mapping:
                                            continue
                                        
                                        chromadb_sent_id = sent_mapping['chromadb_sentence_id']
                                        
                                        # ChromaDBã‹ã‚‰åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
                                        existing_sentence_data = sentence_db.get_data_by_ids([chromadb_sent_id])
                                        existing_sentence_embedding = existing_sentence_data['embeddings'][0]
                                        
                                        existing_image_data = image_db.get_data_by_ids([chromadb_img_id])
                                        existing_image_embedding = existing_image_data['embeddings'][0]
                                        
                                        # æ–‡ç« ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
                                        sentence_similarity = 0.0
                                        if new_sentence_embedding is not None:
                                            sentence_similarity = cosine_similarity(
                                                [new_sentence_embedding],
                                                [existing_sentence_embedding]
                                            )[0][0]
                                        
                                        # ç”»åƒã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
                                        image_similarity = 0.0
                                        if new_image_embedding is not None:
                                            image_similarity = cosine_similarity(
                                                [new_image_embedding],
                                                [existing_image_embedding]
                                            )[0][0]
                                        
                                        # è¨˜éŒ²ç”¨
                                        all_image_similarities.append({
                                            'folder_id': folder_id,
                                            'folder_name': folder_name,
                                            'clustering_id': cid,
                                            'sentence_similarity': float(sentence_similarity),
                                            'image_similarity': float(image_similarity)
                                        })
                                        
                                        # ç”»åƒé¡ä¼¼åº¦ã§æœ€å¤§å€¤ã‚’æ›´æ–°
                                        if image_similarity > max_image_similarity:
                                            max_image_similarity = image_similarity
                                            max_sentence_similarity = sentence_similarity
                                            best_matching_folder_id = folder_id
                                            best_matching_image_info = {
                                                'folder_name': folder_name,
                                                'clustering_id': cid,
                                                'sentence_similarity': float(sentence_similarity),
                                                'image_similarity': float(image_similarity)
                                            }
                                    
                                    except Exception as embed_e:
                                        continue
                            
                            # æœ€ã‚‚é¡ä¼¼ã—ãŸç”»åƒãŒè¦‹ã¤ã‹ã£ãŸå ´åˆ
                            if best_matching_folder_id is not None:
                                target_folder_id_by_criteria = best_matching_folder_id
                                
                                print(f"\n    âœ… æœ€ã‚‚é¡ä¼¼ã—ãŸç”»åƒã‚’ç™ºè¦‹")
                                print(f"       ãƒ•ã‚©ãƒ«ãƒ€: {best_matching_image_info['folder_name']} (ID: {best_matching_folder_id})")
                                print(f"       ç”»åƒID: {best_matching_image_info['clustering_id']}")
                                print(f"       ç”»åƒé¡ä¼¼åº¦: {max_image_similarity:.4f}")
                                print(f"       æ–‡ç« é¡ä¼¼åº¦: {max_sentence_similarity:.4f}")
                                
                                # ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã«è¨˜éŒ²
                                report_data['most_similar_image_matching_used'] = True
                                report_data['best_matching_image_similarity'] = float(max_image_similarity)
                                report_data['best_matching_sentence_similarity'] = float(max_sentence_similarity)
                                report_data['best_matching_folder_id'] = best_matching_folder_id
                                report_data['best_matching_folder_name'] = best_matching_image_info['folder_name']
                                report_data['best_matching_clustering_id'] = best_matching_image_info['clustering_id']
                                report_data['all_image_similarities'] = all_image_similarities[:100]  # æœ€å¤§100ä»¶ã¾ã§è¨˜éŒ²
                            else:
                                print(f"    âš ï¸ é©åˆ‡ãªç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                        
                        except Exception as image_matching_e:
                            print(f"    âš ï¸ é¡ä¼¼ç”»åƒãƒãƒƒãƒãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {image_matching_e}")
                            traceback.print_exc()
                    
                    # æœ€çµ‚çš„ãªæŒ¿å…¥å…ˆãƒ•ã‚©ãƒ«ãƒ€ã‚’æ±ºå®š
                    final_target_folder_id = target_folder_id_by_criteria if target_folder_id_by_criteria else best_folder_id
                    
                    # æœ€çµ‚ãƒ•ã‚©ãƒ«ãƒ€æƒ…å ±ã‚’ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã«è¨˜éŒ²
                    final_folder_obj = next((f for f in leaf_folders if f['id'] == final_target_folder_id), None)
                    final_folder_name = final_folder_obj['name'] if final_folder_obj else final_target_folder_id
                    
                    report_data['final_folder_id'] = final_target_folder_id
                    report_data['final_folder_name'] = final_folder_name
                    report_data['final_similarity'] = max_similarity
                    report_data['final_similarity_type'] = best_similarity_type
                    
                    # æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã«è¿½åŠ ã™ã‚‹å ´åˆã€ãƒ•ã‚©ãƒ«ãƒ€å¹³å‡ã¨ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
                    try:
                        # æ–‡ç« ç‰¹å¾´é‡ã®é¡ä¼¼åº¦
                        if final_target_folder_id in folder_sentence_embeddings and new_sentence_embedding is not None:
                            folder_sent_vec = folder_sentence_embeddings[final_target_folder_id]
                            new_image_sent_vec = new_sentence_embedding
                            sentence_similarity_with_folder = float(np.dot(folder_sent_vec, new_image_sent_vec) / 
                                                                          (np.linalg.norm(folder_sent_vec) * np.linalg.norm(new_image_sent_vec)))
                            report_data['folder_average_sentence_similarity'] = sentence_similarity_with_folder
                            print(f"    ğŸ“Š ãƒ•ã‚©ãƒ«ãƒ€å¹³å‡ã¨ã®æ–‡ç« é¡ä¼¼åº¦: {sentence_similarity_with_folder:.4f}")
                        
                        # ç”»åƒç‰¹å¾´é‡ã®é¡ä¼¼åº¦
                        if final_target_folder_id in folder_image_embeddings and new_image_embedding is not None:
                            folder_img_vec = folder_image_embeddings[final_target_folder_id]
                            new_image_img_vec = new_image_embedding
                            image_similarity_with_folder = float(np.dot(folder_img_vec, new_image_img_vec) / 
                                                                       (np.linalg.norm(folder_img_vec) * np.linalg.norm(new_image_img_vec)))
                            report_data['folder_average_image_similarity'] = image_similarity_with_folder
                            print(f"    ğŸ“Š ãƒ•ã‚©ãƒ«ãƒ€å¹³å‡ã¨ã®ç”»åƒé¡ä¼¼åº¦: {image_similarity_with_folder:.4f}")
                    except Exception as sim_calc_e:
                        print(f"    âš ï¸ ãƒ•ã‚©ãƒ«ãƒ€å¹³å‡ã¨ã®é¡ä¼¼åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {sim_calc_e}")
                    
                    if target_folder_id_by_criteria:
                        report_data['classification_criteria_used'] = True
                    
                    print(f"\n    ğŸ“Œ æœ€çµ‚æŒ¿å…¥å…ˆãƒ•ã‚©ãƒ«ãƒ€: ID={final_target_folder_id}")
                    
                    # ç”»åƒã‚’ãƒ•ã‚©ãƒ«ãƒ€ã«æŒ¿å…¥
                    # imagesãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰image_pathã‚’å–å¾—
                    path_result, _ = action_queries.get_image_name_by_id(connect_session, image_id)
                    image_path = path_result.mappings().first()['name']
                    
                    insert_result = result_manager.insert_image_to_leaf_folder(
                        clustering_id=clustering_id,
                        image_path=image_path,
                        target_folder_id=final_target_folder_id
                    )

                    if insert_result['success']:
                        print(f"    âœ… ãƒ•ã‚©ãƒ«ãƒ€ã«ç”»åƒã‚’æŒ¿å…¥ã—ã¾ã—ãŸ")
                        
                        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
                        try:
                            reporter.generate_image_report(report_data)
                        except Exception as report_e:
                            print(f"    âš ï¸ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {report_e}")
                            traceback.print_exc()
                        
                        # ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
                        all_reports_data.append(report_data)

                        # user_image_clustering_statesã‚’æ›´æ–°
                        _, _ = action_queries.update_user_image_state_for_image(connect_session, user_id, image_id, new_count)

                        # ãƒ•ã‚©ãƒ«ãƒ€ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å†è¨ˆç®—ï¼ˆæ–°ã—ã„ç”»åƒã‚’è¿½åŠ ã—ãŸãŸã‚ï¼‰
                        print(f"    ğŸ”„ ãƒ•ã‚©ãƒ«ãƒ€åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å†è¨ˆç®—ä¸­...")
                        try:
                            # resultå†…ã§ãƒ•ã‚©ãƒ«ãƒ€IDã‚’æ¢ç´¢ã—ã¦dataã‚’å–å¾—
                            folder_data_result = result_manager.get_folder_data_from_result(final_target_folder_id)
                            if folder_data_result['success']:
                                folder_data = folder_data_result['data']
                                clustering_ids = list(folder_data.keys())

                                # æ–‡ç« åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®å†è¨ˆç®—
                                sentence_ids = []
                                for cid in clustering_ids:
                                    sent_result, _ = action_queries.get_chromadb_sentence_id_by_clustering_id(connect_session, cid, project_id)
                                    if sent_result:
                                        sent_mapping = sent_result.mappings().first()
                                        if sent_mapping:
                                            sentence_ids.append(sent_mapping['chromadb_sentence_id'])

                                if len(sentence_ids) > 0:
                                    updated_sentence_data = sentence_name_db.get_data_by_ids(sentence_ids)
                                    updated_sentence_embeddings = updated_sentence_data['embeddings']
                                    folder_sentence_embeddings[final_target_folder_id] = np.mean(updated_sentence_embeddings, axis=0)
                                    print(f"    âœ… ãƒ•ã‚©ãƒ«ãƒ€æ–‡ç« åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å†è¨ˆç®—å®Œäº† ({len(sentence_ids)}å€‹ã®æ–‡ç« )")
                                
                                # ç”»åƒåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®å†è¨ˆç®—
                                image_ids = []
                                for cid in clustering_ids:
                                    img_result, _ = action_queries.get_chromadb_image_id_by_clustering_id(connect_session, cid, project_id)
                                    if img_result:
                                        img_mapping = img_result.mappings().first()
                                        if img_mapping:
                                            image_ids.append(img_mapping['chromadb_image_id'])

                                if len(image_ids) > 0:
                                    updated_image_data = image_db.get_data_by_ids(image_ids)
                                    updated_image_embeddings = updated_image_data['embeddings']
                                    folder_image_embeddings[final_target_folder_id] = np.mean(updated_image_embeddings, axis=0)
                                    print(f"    âœ… ãƒ•ã‚©ãƒ«ãƒ€ç”»åƒåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å†è¨ˆç®—å®Œäº† ({len(image_ids)}å€‹ã®ç”»åƒ)")
                            else:
                                print(f"    âš ï¸ ãƒ•ã‚©ãƒ«ãƒ€ãƒ‡ãƒ¼ã‚¿ã®å†å–å¾—å¤±æ•—: {folder_data_result.get('error', 'Unknown error')}")
                        except Exception as e:
                            print(f"    âš ï¸ ãƒ•ã‚©ãƒ«ãƒ€åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å†è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                            traceback.print_exc()
                    else:
                        print(f"    âŒ ç”»åƒæŒ¿å…¥ã‚¨ãƒ©ãƒ¼: {insert_result.get('error', 'Unknown error')}")
                        report_data['errors'].append(f"ç”»åƒæŒ¿å…¥ã‚¨ãƒ©ãƒ¼: {insert_result.get('error', 'Unknown error')}")
                        
                        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
                        try:
                            reporter.generate_image_report(report_data)
                        except Exception as report_e:
                            print(f"    âš ï¸ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {report_e}")
                            traceback.print_exc()
                        
                        # ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
                        all_reports_data.append(report_data)
                        
                except Exception as img_error:
                    print(f"    âŒ ç”»åƒå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {img_error}")
                    traceback.print_exc()
                    
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã§ã‚‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
                    if 'report_data' in locals():
                        report_data['errors'].append(f"ç”»åƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(img_error)}")
                        
                        try:
                            reporter.generate_image_report(report_data)
                        except Exception as report_e:
                            print(f"    âš ï¸ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {report_e}")
                            traceback.print_exc()
                        
                        all_reports_data.append(report_data)
                    
                    continue
            
            # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            if len(all_reports_data) > 0:
                try:
                    print(f"\nğŸ“Š ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
                    reporter.generate_summary_report(all_reports_data)
                    print(f"âœ… ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
                    
                    print(f"\nğŸ“Š è©•ä¾¡æŒ‡æ¨™ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
                    # ãƒ•ã‚©ãƒ«ãƒ€ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                    all_nodes = result_manager.get_all_nodes()
                    reporter.generate_metrics_report(
                        all_reports_data,
                        folder_data=all_nodes,
                        similarity_threshold=SIMILARITY_THRESHOLD
                    )
                    print(f"âœ… è©•ä¾¡æŒ‡æ¨™ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
                except Exception as summary_e:
                    print(f"âš ï¸ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {summary_e}")
                    traceback.print_exc()
            
            # project_membershipsã®executed_clustering_countã‚’æ›´æ–°
            _, _ = action_queries.update_project_executed_clustering_count(connect_session, user_id, project_id, new_count)
            
            # æœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒãŒæ®‹ã£ã¦ã„ã‚‹ã‹ç¢ºèª
            check_result, _ = action_queries.get_unclustered_count_for_project(connect_session, user_id, project_id)
            remaining_unclustered = check_result.mappings().first()['unclustered_count']
            
            print(f"\nğŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Œäº†å¾Œã®çŠ¶æ…‹ç¢ºèª:")
            print(f"   æ®‹ã‚Šã®æœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒæ•°: {remaining_unclustered}")
            
            # æœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒãŒæ®‹ã£ã¦ã„ã‚Œã°2ï¼ˆå®Ÿè¡Œå¯èƒ½ï¼‰ã€ãªã‘ã‚Œã°0ï¼ˆå®Ÿè¡Œä¸å¯èƒ½ï¼‰
            new_state = 2 if remaining_unclustered > 0 else 0
            state_description = "å®Ÿè¡Œå¯èƒ½" if new_state == 2 else "å®Ÿè¡Œä¸å¯èƒ½"
            
            _, _ = action_queries.update_continuous_state(connect_session, user_id, project_id, new_state)
            
            print(f"   continuous_clustering_state: {new_state} ({state_description})")
            print(f"\nâœ… ç¶™ç¶šçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†å®Œäº†")
            print(f"   å‡¦ç†ã—ãŸç”»åƒæ•°: {len(unclustered_rows)}")
            print(f"   æ–°ã—ã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å›æ•°: {new_count}")
            
        except Exception as e:
            print(f"âŒ ç¶™ç¶šçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
            traceback.print_exc()
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚æœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒã®æœ‰ç„¡ã‚’ç¢ºèªã—ã¦çŠ¶æ…‹ã‚’è¨­å®š
            try:
                check_result, _ = action_queries.get_unclustered_count_for_project(connect_session, user_id, project_id)
                remaining_unclustered = check_result.mappings().first()['unclustered_count']
                
                # æœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒãŒæ®‹ã£ã¦ã„ã‚Œã°2ï¼ˆå®Ÿè¡Œå¯èƒ½ï¼‰ã€ãªã‘ã‚Œã°0ï¼ˆå®Ÿè¡Œä¸å¯èƒ½ï¼‰
                new_state = 2 if remaining_unclustered > 0 else 0
                
                _, _ = action_queries.update_continuous_state(connect_session, user_id, project_id, new_state)
                print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼å¾Œã®çŠ¶æ…‹æ›´æ–°: continuous_clustering_state = {new_state} (æœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒ: {remaining_unclustered})")
            except Exception as state_error:
                print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼å¾Œã®çŠ¶æ…‹æ›´æ–°ã«å¤±æ•—: {state_error}")
                
    # continuous_clustering_stateã‚’1ï¼ˆå®Ÿè¡Œä¸­ï¼‰ã«æ›´æ–°
    _, _ = action_queries.update_continuous_state(connect_session, user_id, project_id, 1)
    
    # éåŒæœŸå®Ÿè¡Œ
    background_tasks.add_task(run_continuous_clustering, rows, project_id, user_id, mongo_result_id)
    
    return JSONResponse(
        status_code=status.HTTP_200_OK,
        content={
            "message": "continuous clustering started in background",
            "data": {
                "project_id": project_id,
                "user_id": user_id,
                "unclustered_image_count": len(rows)
            }
        }
    )


@action_endpoint.put("/action/clustering/move/{mongo_result_id}", tags=["action"], description="ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã®ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã‚’å¤‰æ›´ã™ã‚‹")
def move_clustering_items(
    mongo_result_id: str,
    source_type: str = Query(..., description="ç§»å‹•ã™ã‚‹ã‚½ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒ—: 'folders' ã¾ãŸã¯ 'images'"),
    sources: List[str] = Query(..., description="ç§»å‹•ã™ã‚‹è¦ç´ ã®åå‰ã®é…åˆ—"),
    destination_folder: str = Query(..., description="ç§»å‹•å…ˆã®ãƒ•ã‚©ãƒ«ãƒ€å")
):
    """
    æŒ‡å®šã—ãŸmongo_result_idã«ç´ã¤ã„ãŸjsonã‚’ç·¨é›†ã—ã¦ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã‚’ä½œã‚Šæ›¿ãˆã¦nosqlãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚³ãƒŸãƒƒãƒˆã™ã‚‹
    
    Args:
        mongo_result_id: MongoDBã®çµæœID
        source_type: ç§»å‹•ã™ã‚‹ã‚½ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒ— ("folders" ã¾ãŸã¯ "images")
        sources: ç§»å‹•ã™ã‚‹è¦ç´ ã®åå‰ã®é…åˆ—
        destination_folder: ç§»å‹•å…ˆã®ãƒ•ã‚©ãƒ«ãƒ€å
    """
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¤œè¨¼
    if source_type not in ["folders", "images"]:
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"message": "source_type must be 'folders' or 'images'", "data": None}
        )
    
    if not sources or len(sources) == 0:
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"message": "sources must not be empty", "data": None}
        )
    
    if not destination_folder:
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"message": "destination_folder is required", "data": None}
        )

    result_manager = ResultManager(mongo_result_id)
    
    if source_type == "folders":
        try:
            result_manager.move_folder_node(sources, destination_folder)
        except Exception as e:
            return JSONResponse(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                content={"message": f"ãƒ•ã‚©ãƒ«ãƒ€ç§»å‹•ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}", "data": None}
            )
            
    elif source_type == "images":
        try:
            for source in sources:
                result_manager.move_file_node(source, destination_folder)
        except Exception as e:
            return JSONResponse(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                content={"message": f"ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}", "data": None}
            )
            
    else:
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"message": "source_type must be 'folders' or 'images'", "data": None}
        )
    


    return JSONResponse(
        status_code=status.HTTP_200_OK,
        content={"message": "success", "data": None}
    )
    

@action_endpoint.post("/action/folders/{mongo_result_id}", tags=["action"], description="æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ")
def create_folder(
    mongo_result_id: str,
    parent_folder_id: str = Query(..., description="è¦ªãƒ•ã‚©ãƒ«ãƒ€ã®ID"),
    is_leaf: bool = Query(..., description="ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€ã‹ã©ã†ã‹ï¼ˆTrue: ãƒ•ã‚¡ã‚¤ãƒ«ç”¨, False: ã‚«ãƒ†ã‚´ãƒªç”¨ï¼‰")
):
    """
    æŒ‡å®šã•ã‚ŒãŸè¦ªãƒ•ã‚©ãƒ«ãƒ€ã®é…ä¸‹ã«æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã™ã‚‹
    
    Args:
        mongo_result_id: MongoDBã®çµæœID
        parent_folder_id: è¦ªãƒ•ã‚©ãƒ«ãƒ€ã®ID
        is_leaf: ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€ã‹ã©ã†ã‹ï¼ˆTrue: ãƒ•ã‚¡ã‚¤ãƒ«ç”¨, False: ã‚«ãƒ†ã‚´ãƒªç”¨ï¼‰
        
    Returns:
        JSONResponse: ä½œæˆã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ã®æƒ…å ±
    """
    try:
        # å…¥åŠ›ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if not mongo_result_id or not mongo_result_id.strip():
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "mongo_result_id is required", "data": None}
            )
        
        if not parent_folder_id or not parent_folder_id.strip():
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "parent_folder_id is required", "data": None}
            )
        
        print(f"ğŸ“ ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ:")
        print(f"   mongo_result_id: {mongo_result_id}")
        print(f"   parent_folder_id: {parent_folder_id}")
        print(f"   is_leaf: {is_leaf}")
        
        # ResultManagerã‚’åˆæœŸåŒ–
        result_manager = ResultManager(mongo_result_id)
        
        # æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€IDã‚’ç”Ÿæˆ
        new_folder_id = Utils.generate_uuid()
        
        # ãƒ•ã‚©ãƒ«ãƒ€åã‚’is_leafã«å¿œã˜ã¦è¨­å®š
        folder_prefix = "leaf" if is_leaf else "category"
        folder_name = f"{folder_prefix}-{new_folder_id[:8]}"
        
        print(f"   ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ID: {new_folder_id}")
        print(f"   ãƒ•ã‚©ãƒ«ãƒ€å: {folder_name}")
        
        # all_nodesã¨resultã‚’å–å¾—
        all_nodes = result_manager.get_all_nodes()
        result_data = result_manager.get_result()
        
        if all_nodes is None or result_data is None:
            return JSONResponse(
                status_code=status.HTTP_404_NOT_FOUND,
                content={"message": "result or all_nodes not found", "data": None}
            )
        
        # è¦ªãƒ•ã‚©ãƒ«ãƒ€ã®å­˜åœ¨ã‚’ç¢ºèª
        if parent_folder_id not in all_nodes:
            return JSONResponse(
                status_code=status.HTTP_404_NOT_FOUND,
                content={"message": f"parent folder not found: {parent_folder_id}", "data": None}
            )
        
        # æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€ãƒãƒ¼ãƒ‰ã‚’ä½œæˆ
        new_folder_node = {
            "type": "folder",
            "id": new_folder_id,
            "name": folder_name,
            "parent_id": parent_folder_id,
            "is_leaf": is_leaf
        }
        
        # all_nodesã«è¿½åŠ 
        all_nodes[new_folder_id] = new_folder_node
        print(f"   ğŸ“ all_nodesã«è¿½åŠ : {new_folder_id}")
        
        # resultã«ç©ºã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        new_folder_data = {
            "type": "folder",
            "name": folder_name,
            "is_leaf": is_leaf,
            "data": {}  # ç©ºã®ãƒ•ã‚©ãƒ«ãƒ€ã¨ã—ã¦ä½œæˆ
        }
        
        # resultå†…ã§è¦ªãƒ•ã‚©ãƒ«ãƒ€ã‚’å†å¸°çš„ã«æ¢ç´¢ã—ã¦è¿½åŠ 
        def add_folder_to_parent_recursive(node: dict, target_parent_id: str) -> bool:
            """
            resultã‚’å†å¸°çš„ã«æ¢ç´¢ã—ã¦è¦ªãƒ•ã‚©ãƒ«ãƒ€ã‚’è¦‹ã¤ã‘ã€æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€ã‚’è¿½åŠ ã™ã‚‹
            
            Args:
                node: ç¾åœ¨ã®ãƒãƒ¼ãƒ‰
                target_parent_id: è¦ªãƒ•ã‚©ãƒ«ãƒ€ã®ID
                
            Returns:
                bool: è¿½åŠ ã«æˆåŠŸã—ãŸã‹ã©ã†ã‹
            """
            for folder_id, folder_data in node.items():
                if folder_id == target_parent_id:
                    # è¦ªãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã£ãŸ
                    if isinstance(folder_data, dict):
                        if "data" not in folder_data:
                            folder_data["data"] = {}
                        folder_data["data"][new_folder_id] = new_folder_data
                        print(f"   âœ… è¦ªãƒ•ã‚©ãƒ«ãƒ€ {target_parent_id} ã«è¿½åŠ ã—ã¾ã—ãŸ")
                        return True
                elif isinstance(folder_data, dict) and "data" in folder_data and isinstance(folder_data["data"], dict):
                    # å†å¸°çš„ã«æ¢ç´¢
                    if add_folder_to_parent_recursive(folder_data["data"], target_parent_id):
                        return True
            return False
        
        # ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã‹ã‚‰æ¢ç´¢
        if not add_folder_to_parent_recursive(result_data, parent_folder_id):
            # ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã«è¦ªãŒã‚ã‚‹å ´åˆï¼ˆresultã®ç›´ä¸‹ï¼‰
            if parent_folder_id in result_data:
                parent_data = result_data[parent_folder_id]
                if "data" not in parent_data:
                    parent_data["data"] = {}
                parent_data["data"][new_folder_id] = new_folder_data
                print(f"   âœ… ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®è¦ªãƒ•ã‚©ãƒ«ãƒ€ {parent_folder_id} ã«è¿½åŠ ã—ã¾ã—ãŸ")
            else:
                return JSONResponse(
                    status_code=status.HTTP_404_NOT_FOUND,
                    content={"message": f"parent folder not found in result: {parent_folder_id}", "data": None}
                )
        
        # è¦ªãƒ•ã‚©ãƒ«ãƒ€ã‚’è¾¿ã£ã¦resultå†…ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
        print(f"   ğŸ”„ è¦ªãƒ•ã‚©ãƒ«ãƒ€ã‚’è¾¿ã£ã¦resultæ›´æ–°ä¸­...")
        parent_path = result_manager.get_parents(new_folder_id)
        print(f"   ğŸ“‚ è¦ªãƒ‘ã‚¹: {parent_path}")
        
        # æ›´æ–°ã‚’MongoDBã«ä¿å­˜
        result_manager.update_result(result_data, all_nodes)
        print(f"   ğŸ’¾ MongoDBã«ä¿å­˜å®Œäº†")
        
        print(f"âœ… ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆæˆåŠŸ: {folder_name} (ID: {new_folder_id})")
        
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "message": "success",
                "data": {
                    "folder_id": new_folder_id,
                    "folder_name": folder_name,
                    "parent_id": parent_folder_id,
                    "is_leaf": is_leaf
                }
            }
        )
        
    except Exception as e:
        print(f"âŒ ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        traceback.print_exc()
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={
                "message": f"ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}",
                "data": None
            }
        )


@action_endpoint.delete("/action/folders/{mongo_result_id}", tags=["action"], description="æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤")
async def delete_folders(mongo_result_id: str, sources: List[str] = Query(...)):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€IDãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€çµæœã‹ã‚‰å‰Šé™¤ã™ã‚‹
    
    Args:
        mongo_result_id (str): MongoDBã®çµæœID
        sources (List[str]): å‰Šé™¤å¯¾è±¡ã®ãƒ•ã‚©ãƒ«ãƒ€IDãƒªã‚¹ãƒˆ
    
    Returns:
        JSONResponse: å‰Šé™¤å‡¦ç†ã®çµæœ
    """
    try:
        # å…¥åŠ›ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if not mongo_result_id or not mongo_result_id.strip():
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "mongo_result_id is required"}
            )
        
        if not sources or len(sources) == 0:
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "sources parameter is required and must contain at least one folder ID"}
            )
        
        print(f"ğŸ—‚ï¸ delete_folderså‘¼ã³å‡ºã—: mongo_result_id={mongo_result_id}")
        print(f"ğŸ“‹ å—ã‘å–ã£ãŸãƒ•ã‚©ãƒ«ãƒ€IDãƒªã‚¹ãƒˆ (sources): {sources}")
        print(f"ğŸ“Š å‰Šé™¤å¯¾è±¡ãƒ•ã‚©ãƒ«ãƒ€æ•°: {len(sources)}")
        
        # ResultManagerã‚’åˆæœŸåŒ–
        result_manager = ResultManager(mongo_result_id)
        
        # ãƒ•ã‚©ãƒ«ãƒ€ã‚’çµæœã‹ã‚‰å‰Šé™¤
        is_success = result_manager.remove_folders_from_result(sources)
        
        if is_success:
            print(f"âœ… ãƒ•ã‚©ãƒ«ãƒ€å‰Šé™¤æˆåŠŸ: {len(sources)}å€‹ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            return JSONResponse(
                status_code=status.HTTP_200_OK,
                content={
                    "message": "success", 
                    "data": {
                        "deleted_folder_count": len(sources),
                        "deleted_folders": sources
                    }
                }
            )
        else:
            print(f"âŒ ãƒ•ã‚©ãƒ«ãƒ€å‰Šé™¤å¤±æ•—: remove_folders_from_result returned False")
            return JSONResponse(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                content={
                    "message": "Failed to delete folders from result",
                    "data": {
                        "mongo_result_id": mongo_result_id,
                        "attempted_folder_ids": sources,
                        "error": "remove_folders_from_result operation failed"
                    }
                }
            )
            
    except Exception as e:
        print(f"âŒ delete_folderså‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
        return JSONResponse(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                content={
                    "message": "Internal server error occurred during folder deletion",
                    "data": {
                        "mongo_result_id": mongo_result_id,
                        "attempted_folder_ids": sources,
                        "error": str(e)
                    }
                }
            )


@action_endpoint.get("/action/clustering/node/{mongo_result_id}/{node_id}", tags=["action"], description="æŒ‡å®šã•ã‚ŒãŸãƒãƒ¼ãƒ‰ã®æƒ…å ±ã‚’å–å¾—ã™ã‚‹")
async def get_node_info(mongo_result_id: str, node_id: str):
    """
    æŒ‡å®šã•ã‚ŒãŸnode_idã®ãƒãƒ¼ãƒ‰æƒ…å ±ã‚’all_nodesã‹ã‚‰å–å¾—ã™ã‚‹
    
    Args:
        mongo_result_id (str): MongoDBã®çµæœIDï¼ˆãƒ‘ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
        node_id (str): ãƒãƒ¼ãƒ‰IDï¼ˆãƒ‘ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
        
    Returns:
        JSONResponse: ãƒãƒ¼ãƒ‰æƒ…å ±
    """
    try:
        # å…¥åŠ›ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if not mongo_result_id or not mongo_result_id.strip():
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "mongo_result_id is required"}
            )
        
        if not node_id or not node_id.strip():
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "node_id is required"}
            )
        
        print(f"ğŸ” get_node_infoå‘¼ã³å‡ºã—: mongo_result_id={mongo_result_id}, node_id={node_id}")
        
        # ResultManagerã‚’åˆæœŸåŒ–ã—ã¦ãƒãƒ¼ãƒ‰æƒ…å ±ã‚’å–å¾—
        result_manager = ResultManager(mongo_result_id)
        node_data = result_manager.get_node_info(node_id=node_id)
        
        # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯HTTPExceptionã‚’ç™ºç”Ÿ
        if not node_data["success"]:
            if "not found" in node_data.get("error", "").lower():
                return JSONResponse(
                    status_code=status.HTTP_404_NOT_FOUND,
                    content={
                        "message": node_data["error"],
                        "data": {
                            "mongo_result_id": mongo_result_id,
                            "node_id": node_id
                        }
                    }
                )
            else:
                return JSONResponse(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    content={
                        "message": node_data["error"],
                        "data": {
                            "mongo_result_id": mongo_result_id,
                            "node_id": node_id
                        }
                    }
                )
        
        
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "message": "success",
                "data": node_data["data"]  # all_nodesã®å€¤ã®ã¿ã‚’è¿”ã™
            }
        )
        
    except Exception as e:
        print(f"âŒ get_node_infoå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={
                "message": "Internal server error occurred during node retrieval",
                "data": {
                    "mongo_result_id": mongo_result_id,
                    "node_id": node_id,
                    "error": str(e)
                }
            }
        )


@action_endpoint.get("/action/clustering/captions/{mongo_result_id}", tags=["action"], description="æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€å†…ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°IDã«å¯¾å¿œã™ã‚‹ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—ã™ã‚‹")
async def get_captions_for_folder(mongo_result_id: str, folder_id: str = Query(..., description="ãƒ•ã‚©ãƒ«ãƒ€ã® node_id")):
    """
    mongo_result_id (ãƒ‘ã‚¹) ã¨ folder_id (ã‚¯ã‚¨ãƒª) ã‚’å—ã‘å–ã‚Šã€ãã®ãƒ•ã‚©ãƒ«ãƒ€ã«å«ã¾ã‚Œã‚‹ç”»åƒã® clustering_id ã‚’å–å¾—ã—ã€
    images ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ caption ã‚’å–å¾—ã—ã¦ {clustering_id: caption} ã®ãƒãƒƒãƒ—ã‚’è¿”ã™ã€‚
    """
    try:
        if not mongo_result_id or not mongo_result_id.strip():
            return JSONResponse(status_code=status.HTTP_400_BAD_REQUEST, content={"message": "mongo_result_id is required"})

        if not folder_id or not folder_id.strip():
            return JSONResponse(status_code=status.HTTP_400_BAD_REQUEST, content={"message": "folder_id is required"})

        # ResultManagerã‚’åˆæœŸåŒ–ã—ã¦ãƒ•ã‚©ãƒ«ãƒ€å†…ã®clustering_idä¸€è¦§ã‚’å–å¾—
        result_manager = ResultManager(mongo_result_id)
        clustering_ids_result = result_manager.get_leaf_folder_image_clustering_ids(folder_id)

        # debug logs for tracing
        print(f"ğŸ” get_captions_for_folder: clustering_ids_result={clustering_ids_result}")

        if not clustering_ids_result.get('success', False):
            return JSONResponse(status_code=status.HTTP_404_NOT_FOUND, content={"message": clustering_ids_result.get('error', 'folder not found'), "data": None})

        clustering_ids = clustering_ids_result.get('data', [])
        print(f"ğŸ” get_captions_for_folder: clustering_ids (count={len(clustering_ids)}): {clustering_ids[:50]}")

        captions_map = {}

        # DBã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¦1ã¤ãšã¤captionã‚’å–å¾—ï¼ˆå°†æ¥çš„ã«INã‚¯ã‚¨ãƒªã¸æœ€é©åŒ–å¯ï¼‰
        connect_session = create_connect_session()
        if connect_session is None:
            return JSONResponse(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, content={"message": "failed to connect to database"})

        for cid in clustering_ids:
            try:
                res, _ = images_queries.select_caption_by_clustering_id(connect_session, cid)
                if res is None:
                    captions_map[cid] = None
                    continue
                mapping = res.mappings().first()
                captions_map[cid] = mapping['caption'] if mapping and 'caption' in mapping else None
            except Exception as q_e:
                # å€‹åˆ¥å–å¾—ã«å¤±æ•—ã—ã¦ã‚‚ä»–ã®çµæœã¯è¿”ã™
                captions_map[cid] = None

        return JSONResponse(status_code=status.HTTP_200_OK, content={"message": "success", "data": {"folder_id": folder_id, "captions": captions_map}})

    except Exception as e:
        print(f"âŒ get_captions_for_folderã‚¨ãƒ©ãƒ¼: {e}")
        return JSONResponse(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, content={"message": f"Internal server error: {str(e)}"})


@action_endpoint.put("/action/folders/{mongo_result_id}/{node_id}", tags=["action"], description="ãƒ•ã‚©ãƒ«ãƒ€ã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«ã®åå‰ã‚’å¤‰æ›´")
async def rename_folder_or_file(
    mongo_result_id: str,
    node_id: str,
    name: str = Query(None, description="æ–°ã—ã„åå‰"),
    is_leaf: bool = Query(None, description="ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã‹ã©ã†ã‹")
):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒãƒ¼ãƒ‰ã®åå‰ã‚’å¤‰æ›´ã™ã‚‹
    
    Args:
        mongo_result_id (str): MongoDBã®çµæœID
        node_id (str): å¤‰æ›´å¯¾è±¡ã®ãƒãƒ¼ãƒ‰ID
        name (str, optional): æ–°ã—ã„åå‰
        is_leaf (bool, optional): ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã‹ã©ã†ã‹
    
    Returns:
        JSONResponse: åå‰å¤‰æ›´å‡¦ç†ã®çµæœ
    """
    try:
        # å…¥åŠ›ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if not mongo_result_id or not mongo_result_id.strip():
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "mongo_result_id is required"}
            )
        
        if not node_id or not node_id.strip():
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "node_id is required"}
            )
        
        # nameã¨is_leafã®ä¸¡æ–¹ãŒNoneã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼
        if name is None and is_leaf is None:
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "At least one of 'name' or 'is_leaf' parameters is required"}
            )
        
        # nameãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ç©ºæ–‡å­—ãƒã‚§ãƒƒã‚¯
        if name is not None and not name.strip():
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "name parameter must not be empty when provided"}
            )
        
        print(f"ğŸ·ï¸ rename_folder_or_fileå‘¼ã³å‡ºã—: mongo_result_id={mongo_result_id}, node_id={node_id}")
        print(f"ğŸ“ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: name={name}, is_leaf={is_leaf}")
        
        # ResultManagerã‚’åˆæœŸåŒ–
        result_manager = ResultManager(mongo_result_id)
        
        # åå‰ãƒ»is_leafå¤‰æ›´å‡¦ç†
        update_result = result_manager.rename_node(
            node_id=node_id, 
            new_name=name.strip() if name is not None else None, 
            is_leaf=is_leaf
        )
        
        print(f"âœ… æ›´æ–°çµæœ: {update_result}")
        
        if update_result.get("success", False):
            return JSONResponse(
                status_code=status.HTTP_200_OK,
                content={
                    "message": "success",
                    "data": {
                        "node_id": node_id,
                        "updated_fields": update_result.get("updated_fields", {}),
                        "is_leaf": is_leaf
                    }
                }
            )
        else:
            return JSONResponse(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                content={
                    "message": "Failed to update node",
                    "data": {
                        "mongo_result_id": mongo_result_id,
                        "node_id": node_id,
                        "attempted_name": name,
                        "attempted_is_leaf": is_leaf,
                        "error": update_result.get("error", "Unknown error")
                    }
                }
            )
            
    except Exception as e:
        print(f"âŒ rename_folder_or_fileå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={
                "message": "Internal server error occurred during rename operation",
                "data": {
                    "mongo_result_id": mongo_result_id,
                    "node_id": node_id,
                    "attempted_name": name,
                    "error": str(e)
                }
            }
        )


@action_endpoint.get("/action/clustering/download/{project_id}", tags=["action"], description="åˆ†é¡çµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹")
async def download_classification_result(
    project_id: int,
    user_id: int = Query(..., description="ãƒ¦ãƒ¼ã‚¶ãƒ¼ID")
):
    """
    åˆ†é¡çµæœã‚’ZIPãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    
    ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹:
    - result.json: åˆ†é¡çµæœã®éšå±¤æ§‹é€ 
    - all_nodes.json: å…¨ãƒãƒ¼ãƒ‰ã®æƒ…å ±
    - images/: åˆ†é¡çµæœã«åŸºã¥ã„ãŸãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«
    
    Args:
        project_id: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
        user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        
    Returns:
        FileResponse: ZIPãƒ•ã‚¡ã‚¤ãƒ«
    """
    connect_session = create_connect_session()
    
    if connect_session is None:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={"message": "failed to connect to database", "data": None}
        )
    
    try:
        
        result, _ = action_queries.get_project_info_and_mongo(connect_session, project_id, user_id)
        result_mapping = result.mappings().first()
        
        if result_mapping is None:
            return JSONResponse(
                status_code=status.HTTP_404_NOT_FOUND,
                content={"message": "project or membership not found", "data": None}
            )
        
        project_name = result_mapping['project_name']
        original_images_folder_path = result_mapping['original_images_folder_path']
        mongo_result_id = result_mapping['mongo_result_id']
        init_clustering_state = result_mapping['init_clustering_state']
        
        # åˆæœŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãŒå®Œäº†ã—ã¦ã„ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
        if init_clustering_state != INIT_CLUSTERING_STATUS.FINISHED:
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "clustering not completed yet", "data": None}
            )
        
        print(f"ğŸ“¦ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†é–‹å§‹:")
        print(f"   ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {project_name}")
        print(f"   ãƒ¦ãƒ¼ã‚¶ãƒ¼ID: {user_id}")
        print(f"   mongo_result_id: {mongo_result_id}")
        
        # ResultManagerã‹ã‚‰åˆ†é¡çµæœãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        result_manager = ResultManager(mongo_result_id)
        export_data = result_manager.export_classification_data()
        
        if not export_data['success']:
            return JSONResponse(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                content={
                    "message": f"failed to export classification data: {export_data.get('error', 'Unknown error')}",
                    "data": None
                }
            )
        
        result_dict = export_data['result']
        all_nodes_dict = export_data['all_nodes']
        
        # ç”»åƒãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹
        source_images_path = Path(f"./{DEFAULT_IMAGE_PATH}/{original_images_folder_path}")
        
        if not source_images_path.exists():
            return JSONResponse(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                content={
                    "message": f"source images folder not found: {source_images_path}",
                    "data": None
                }
            )
        
        print(f"   ç”»åƒãƒ•ã‚©ãƒ«ãƒ€: {source_images_path}")
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ä½œæˆ
        try:
            zip_path = Utils.create_classification_download_package(
                result_dict=result_dict,
                all_nodes_dict=all_nodes_dict,
                source_images_path=source_images_path,
                project_name=project_name
            )
            
            print(f"   ZIPãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†: {zip_path}")
            
            # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            return FileResponse(
                path=str(zip_path),
                media_type='application/zip',
                filename=f"{project_name}.zip",
                headers={
                    "Content-Disposition": f'attachment; filename="{project_name}.zip"'
                }
            )
            
        except Exception as create_error:
            print(f"âŒ ZIPãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {create_error}")
            traceback.print_exc()
            return JSONResponse(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                content={
                    "message": f"failed to create download package: {str(create_error)}",
                    "data": None
                }
            )
        
    except Exception as e:
        print(f"âŒ download_classification_resultå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
        traceback.print_exc()
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={
                "message": f"Internal server error: {str(e)}",
                "data": None
            }
        )


@action_endpoint.get("/action/clustering/counts/{project_id}", tags=["action"], description="ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®ç”»åƒã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å›æ•°æƒ…å ±ã‚’å–å¾—ã™ã‚‹")
async def get_clustering_counts(
    project_id: int,
    user_id: int = Query(..., description="ãƒ¦ãƒ¼ã‚¶ãƒ¼ID")
):
    """
    ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®å…¨ç”»åƒã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å›æ•°æƒ…å ±ã‚’å–å¾—ã™ã‚‹
    
    Returns:
        {
            "available_counts": [0, 1, 2, ...],  # å®Ÿè¡Œã•ã‚ŒãŸå›æ•°ã®ãƒªã‚¹ãƒˆ
            "image_counts": {
                "clustering_id_1": 0,  # å„ç”»åƒã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å›æ•°
                "clustering_id_2": 1,
                ...
            }
        }
    """
    connect_session = create_connect_session()
    
    if connect_session is None:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={"message": "failed to connect to database", "data": None}
        )
    
    try:
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ¡ãƒ³ãƒãƒ¼ã‚·ãƒƒãƒ—ã‚’ç¢ºèªï¼ˆCOUNTã§å­˜åœ¨ç¢ºèªã™ã‚‹ã€‚
        # ä¸€éƒ¨ç’°å¢ƒã§ project_memberships ã« id ã‚«ãƒ©ãƒ ãŒç„¡ã„å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€å˜ç´”ãªå­˜åœ¨ç¢ºèªã‚’ä½¿ã†ï¼‰
        membership_result, _ = action_queries.membership_exists(connect_session, project_id, user_id)

        # execute_query ãŒå¤±æ•—ã—ã¦ None ã‚’è¿”ã™å ´åˆã‚’å®‰å…¨ã«æ‰±ã†
        if membership_result is None:
            return JSONResponse(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                content={"message": "failed to query project_memberships", "data": None}
            )

        membership_row = membership_result.mappings().first()
        if membership_row is None or membership_row.get('cnt', 0) == 0:
            return JSONResponse(
                status_code=status.HTTP_404_NOT_FOUND,
                content={"message": "project membership not found", "data": None}
            )
        
        result, _ = action_queries.get_image_counts_for_clustering_counts(connect_session, user_id, project_id)
        rows = result.mappings().all()

        # executed_clustering_count ã”ã¨ã« clustering_id ã®é…åˆ—ã‚’ä½œæˆ
        grouped_by_count: dict[str, list] = {}
        # clustering_id -> executed_clustering_count ã®è¾æ›¸
        image_counts: dict = {}
        available_counts_set = set()

        for row in rows:
            clustering_id = row.get('clustering_id')
            count = row.get('exec_count')

            # clustering_id ã¾ãŸã¯ count ãŒç„¡ã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if clustering_id is None or count is None:
                continue

            # image_counts ãƒãƒƒãƒ—
            image_counts[clustering_id] = int(count)

            # grouped map: key ã‚’æ–‡å­—åˆ—ã«ã—ã¦è¿”ã™ï¼ˆä¾‹: '0', '1', ...ï¼‰
            key = str(int(count))
            if key not in grouped_by_count:
                grouped_by_count[key] = []
            # é‡è¤‡ã‚’é¿ã‘ã¦è¿½åŠ 
            if clustering_id not in grouped_by_count[key]:
                grouped_by_count[key].append(clustering_id)

            available_counts_set.add(int(count))

        # åˆ©ç”¨å¯èƒ½ãªå›æ•°ã‚’ã‚½ãƒ¼ãƒˆã—ãŸãƒªã‚¹ãƒˆã«å¤‰æ›
        available_counts = sorted(list(available_counts_set))
        
        
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "message": "success",
                "data": {
                    "available_counts": available_counts,
                    "image_counts": image_counts,
                    "grouped_image_ids": grouped_by_count
                }
            }
        )
        
    except Exception as e:
        print(f"âŒ get_clustering_countså‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
        traceback.print_exc()
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={
                "message": f"Internal server error: {str(e)}",
                "data": None
            }
        )