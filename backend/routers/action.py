import copy
import json
import re
import traceback
from pathlib import Path
from collections import defaultdict
from typing import List

import numpy as np
from fastapi import APIRouter, HTTPException, status, Response, BackgroundTasks, Query
from fastapi.responses import JSONResponse, FileResponse
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer, util

import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../')))

from db_utils.commons import create_connect_session, execute_query
from db_utils import action_queries, images_queries
from db_utils.validators import validate_data
from db_utils.models import CustomResponseModel, LoginUser, JoinUser
from config import (
    INIT_CLUSTERING_STATUS,
    CONTINUOUS_CLUSTERING_STATUS,
    DEFAULT_IMAGE_PATH,
    DEFAULT_OUTPUT_PATH,
    CAPTION_STOPWORDS,
    MAJOR_COLORS,
    MAJOR_SHAPES
)
from clustering.clustering_manager import ChromaDBManager, InitClusteringManager
from clustering.mongo_db_manager import MongoDBManager
from clustering.mongo_result_manager import ResultManager
from clustering.chroma_db_manager import ChromaDBManager
from clustering.embeddings_manager.image_embeddings_manager import ImageEmbeddingsManager
from clustering.utils import Utils
from clustering.word_analysis import WordAnalyzer

#åˆ†å‰²ã—ãŸã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ä½œæˆ
#ãƒ­ã‚°ã‚¤ãƒ³æ“ä½œ
action_endpoint = APIRouter()

@action_endpoint.get("/action/clustering/result/{mongo_result_id}",tags=["action"],description="åˆæœŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã‚’å–å¾—ã™ã‚‹")
def get_clustering_result(mongo_result_id:str):
    result_manager = ResultManager(mongo_result_id)
    
    # ResultManagerã®get_result()ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
    result_data = result_manager.get_result()
    
    if result_data:
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={"message": "success", "result": result_data}
        )
    else:
        return JSONResponse(
            status_code=status.HTTP_404_NOT_FOUND,
            content={"message": "Clustering result not found"}
        )


@action_endpoint.get("/action/clustering/all_nodes/{mongo_result_id}",tags=["action"],description="æŒ‡å®šã•ã‚ŒãŸmongo_result_idã®all_nodesã‚’å–å¾—ã™ã‚‹")
def get_all_nodes(mongo_result_id: str):
    """
    æŒ‡å®šã•ã‚ŒãŸmongo_result_idã«ç´ã¥ãall_nodesã‚’å–å¾—ã™ã‚‹
    
    Args:
        mongo_result_id (str): MongoDBã®çµæœID
        
    Returns:
        JSONResponse: all_nodesã®æƒ…å ±
    """
    try:
        # å…¥åŠ›ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if not mongo_result_id or not mongo_result_id.strip():
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "mongo_result_id is required", "data": None}
            )
        
        print(f"ğŸ“‹ get_all_nodeså‘¼ã³å‡ºã—: mongo_result_id={mongo_result_id}")
        
        # ResultManagerã‚’åˆæœŸåŒ–
        result_manager = ResultManager(mongo_result_id)
        
        # all_nodesã‚’å–å¾—
        all_nodes_data = result_manager.get_all_nodes()
        
        if all_nodes_data is None:
            return JSONResponse(
                status_code=status.HTTP_404_NOT_FOUND,
                content={
                    "message": "all_nodes not found for the given mongo_result_id",
                    "data": None
                }
            )
        
        print(f"âœ… all_nodeså–å¾—æˆåŠŸ: {len(all_nodes_data)}å€‹ã®ãƒãƒ¼ãƒ‰")
        
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "message": "success",
                "data": all_nodes_data
            }
        )
        
    except Exception as e:
        print(f"âŒ get_all_nodeså‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={
                "message": f"Internal server error occurred: {str(e)}",
                "data": None
            }
        )


@action_endpoint.post("/action/clustering/copy",tags=["action"],description="ä»–ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã‚’ã‚³ãƒ”ãƒ¼ã™ã‚‹")
def copy_clustering_data(
    source_user_id: int = Query(..., description="ã‚³ãƒ”ãƒ¼å…ƒã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ID"),
    target_user_id: int = Query(..., description="ã‚³ãƒ”ãƒ¼å…ˆã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ID"),
    project_id: int = Query(..., description="ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID")
):
    """
    ä»–ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœï¼ˆall_nodesã¨resultï¼‰ã‚’ã‚³ãƒ”ãƒ¼ã™ã‚‹
    
    Args:
        source_user_id: ã‚³ãƒ”ãƒ¼å…ƒã®ãƒ¦ãƒ¼ã‚¶ãƒ¼IDï¼ˆinit_clustering_stateãŒ2ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼‰
        target_user_id: ã‚³ãƒ”ãƒ¼å…ˆã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        project_id: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
        
    Returns:
        JSONResponse: ã‚³ãƒ”ãƒ¼çµæœ
    """
    connect_session = create_connect_session()
    
    if connect_session is None:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={"message": "failed to connect to database", "data": None}
        )
    
    try:
        # 1. ã‚³ãƒ”ãƒ¼å…ƒãƒ¦ãƒ¼ã‚¶ãƒ¼ã®init_clustering_stateãŒ2ï¼ˆå®Œäº†ï¼‰ã‹ãƒã‚§ãƒƒã‚¯
        source_result, _ = action_queries.get_membership_init_and_mongo(connect_session, source_user_id, project_id)
        
        if not source_result:
            return JSONResponse(
                status_code=status.HTTP_404_NOT_FOUND,
                content={"message": "source user membership not found", "data": None}
            )
        
        source_data = source_result.mappings().first()
        if source_data["init_clustering_state"] != INIT_CLUSTERING_STATUS.FINISHED:
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "source user has not completed init clustering", "data": None}
            )
        
        source_mongo_result_id = source_data["mongo_result_id"]
        
        # 2. ã‚³ãƒ”ãƒ¼å…ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®mongo_result_idã‚’å–å¾—
        target_result, _ = action_queries.get_membership_init_and_mongo(connect_session, target_user_id, project_id)
        
        if not target_result:
            return JSONResponse(
                status_code=status.HTTP_404_NOT_FOUND,
                content={"message": "target user membership not found", "data": None}
            )
        
        target_data = target_result.mappings().first()
        target_mongo_result_id = target_data["mongo_result_id"]
        
        # 3. ã‚³ãƒ”ãƒ¼å…ƒã®all_nodesã¨resultã‚’å–å¾—
        source_result_manager = ResultManager(source_mongo_result_id)
        source_all_nodes = source_result_manager.get_all_nodes()
        source_result_data = source_result_manager.get_result()
        
        if source_all_nodes is None or source_result_data is None:
            return JSONResponse(
                status_code=status.HTTP_404_NOT_FOUND,
                content={"message": "source clustering data not found", "data": None}
            )
        
        # 4. ã‚³ãƒ”ãƒ¼å…ˆã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆãƒ‡ã‚£ãƒ¼ãƒ—ã‚³ãƒ”ãƒ¼ã§å®Œå…¨ã«ç‹¬ç«‹ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼‰
        target_result_manager = ResultManager(target_mongo_result_id)
        copied_all_nodes = copy.deepcopy(source_all_nodes)
        copied_result = copy.deepcopy(source_result_data)
        
        target_result_manager.update_result(copied_result, copied_all_nodes)
        
        # 5. ã‚³ãƒ”ãƒ¼å…ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®init_clustering_stateã‚’2ï¼ˆå®Œäº†ï¼‰ã«æ›´æ–°
        _, _ = action_queries.update_init_state(connect_session, target_user_id, project_id, INIT_CLUSTERING_STATUS.FINISHED)
        
        # 6. ã‚³ãƒ”ãƒ¼å…ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¨ç”»åƒã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯
        _, _ = action_queries.mark_user_images_clustered(connect_session, target_user_id, project_id)
        
        print(f"âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼{source_user_id}ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼{target_user_id}ã«ã‚³ãƒ”ãƒ¼å®Œäº†")
        
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "message": "succeeded to copy clustering data",
                "data": {
                    "source_user_id": source_user_id,
                    "target_user_id": target_user_id,
                    "project_id": project_id,
                    "source_mongo_result_id": source_mongo_result_id,
                    "target_mongo_result_id": target_mongo_result_id
                }
            }
        )
        
    except Exception as e:
        print(f"âŒ copy_clustering_dataå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={
                "message": f"failed to copy clustering data: {str(e)}",
                "data": None
            }
        )


@action_endpoint.get("/action/clustering/init/{project_id}", tags=["action"], description="åˆæœŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè£…ã™ã‚‹")
def execute_init_clustering(
    project_id: int = None,
    user_id: int = None,
    background_tasks: BackgroundTasks = None
):
    if project_id is None or user_id is None:
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"message": "project_id and user_id is required", "data": None}
        )

    try:
        project_id = int(project_id)
        user_id = int(user_id)
    except:
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"message": "project_id or user_id is invalid", "data": None}
        )

    connect_session = create_connect_session()
    result, _ = action_queries.get_membership_and_project_info(connect_session, project_id, user_id)
    result_mappings = result.mappings().first()

    if result_mappings is None:
        return JSONResponse(
            status_code=status.HTTP_404_NOT_FOUND,
            content={"message": "project or membership not found", "data": None}
        )

    init_clustering_state = result_mappings["init_clustering_state"]
    original_images_folder_path = result_mappings["original_images_folder_path"]
    mongo_result_id = result_mappings["mongo_result_id"]

    if init_clustering_state == INIT_CLUSTERING_STATUS.EXECUTING or init_clustering_state ==INIT_CLUSTERING_STATUS.FINISHED:
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"message": "init clustering already started", "data": None}
        )

    # å¯¾è±¡ç”»åƒã®å–å¾—
    result, _ = action_queries.select_images_for_init(connect_session, project_id)
    if result is None:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={"message": "failed to get images", "data": None}
        )

    rows = result.mappings().all()
    # æ¤œç´¢ç”¨è¾æ›¸ã‚’ä½œæˆ
    by_clustering_id = {}
    by_chromadb_sentence_id = {}
    by_chromadb_image_id = {}

    for row in rows:
        cid = row["clustering_id"]
        sid = row["chromadb_sentence_id"]
        iid = row["chromadb_image_id"]
        
        by_clustering_id[cid] = {"sentence_id": sid, "image_id": iid}
        by_chromadb_sentence_id[sid] = {"clustering_id": cid, "image_id": iid}
        by_chromadb_image_id[iid] = {"clustering_id": cid,"sentence_id":sid}
    
    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ã«æ¸¡ã™é–¢æ•°
    def run_clustering(cid_dict: dict, sid_dict: dict, iid_dict: dict, project_id: int, original_images_folder_path: str):
        try:
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’å–å¾—
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’å–å¾—
            project_result, _ = action_queries.get_project_name(connect_session, project_id)
            project_mapping = project_result.mappings().first() if project_result else None
            project_name = project_mapping['name'] if project_mapping else f"Project_{project_id}"
            
            print(f"ğŸ·ï¸ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’å–å¾—: {project_name} (project_id: {project_id})")
            
            cl_module = InitClusteringManager(
                sentence_name_db=ChromaDBManager('sentence_name_embeddings'),
                sentence_usage_db=ChromaDBManager('sentence_usage_embeddings'),
                sentence_category_db=ChromaDBManager('sentence_category_embeddings'),
                image_db=ChromaDBManager("image_embeddings"),
                images_folder_path=f"./{DEFAULT_IMAGE_PATH}/{original_images_folder_path}",
                output_base_path=f"./{DEFAULT_OUTPUT_PATH}/{project_id}",
            )
            
            target_sentence_ids = list(sid_dict.keys())
            target_image_ids = list(iid_dict.keys())
            
            # sentence_name_dbã‹ã‚‰ç›´æ¥sentence_idã‚’ä½¿ç”¨ã—ã¦embeddingsã‚’å–å¾—
            sentence_data = cl_module.sentence_name_db.get_data_by_sentence_ids(target_sentence_ids)
            embeddings = sentence_data['embeddings']
            cluster_num, _ = cl_module.get_optimal_cluster_num(embeddings=embeddings)
            
            result_dict,all_nodes = cl_module.clustering(
                sentence_name_db_data=sentence_data,
                image_db_data=cl_module.image_db.get_data_by_ids(target_image_ids),
                clustering_id_dict=cid_dict,
                sentence_id_dict=sid_dict,  # å…ƒã®å½¢å¼ã«æˆ»ã™
                image_id_dict=iid_dict,
                cluster_num=cluster_num,
                overall_folder_name=project_name,
                output_folder=True,
                output_json=True
            )
            
            # all_nodesã‚’é…åˆ—ã‹ã‚‰è¾æ›¸å½¢å¼ã«å¤‰æ›ï¼ˆidã‚’ã‚­ãƒ¼ã¨ã—ã¦ï¼‰
            all_nodes_dict = {}
            for node in all_nodes:
                if 'id' in node:
                    all_nodes_dict[node['id']] = node
            
            result_manager = ResultManager(mongo_result_id)
            result_manager.update_result(result_dict, all_nodes_dict)
        except Exception as e:
            print(f"Error during clustering:{e}")
            
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯åˆæœŸåŒ–çŠ¶æ…‹ã‚’æ›´æ–°
            clustering_state = INIT_CLUSTERING_STATUS.FAILED
        else:
            clustering_state = INIT_CLUSTERING_STATUS.FINISHED
            
            # åˆæœŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æˆåŠŸæ™‚ã€è©²å½“ãƒ¦ãƒ¼ã‚¶ã®å…¨ç”»åƒã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯
            try:
                _, _ = action_queries.mark_user_images_clustered_with_executed_count(connect_session, user_id, project_id, 0)
                print(f"âœ… ãƒ¦ãƒ¼ã‚¶{user_id}ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ{project_id}å†…ã®å…¨ç”»åƒã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿(executed_clustering_count=0)ã¨ã—ã¦ãƒãƒ¼ã‚¯ã—ã¾ã—ãŸ")
            except Exception as mark_error:
                print(f"âš ï¸ user_image_clustering_statesæ›´æ–°ã‚¨ãƒ©ãƒ¼: {mark_error}")
        finally:
            
            # åˆæœŸåŒ–çŠ¶æ…‹ã‚’æ›´æ–°
            _, _ = action_queries.update_init_state(connect_session, user_id, project_id, clustering_state)
                
    # éåŒæœŸå®Ÿè¡Œ
    background_tasks.add_task(run_clustering, by_clustering_id, by_chromadb_sentence_id, by_chromadb_image_id, project_id, original_images_folder_path)
    
    # åˆæœŸåŒ–çŠ¶æ…‹ã‚’æ›´æ–°
    # åˆæœŸåŒ–çŠ¶æ…‹ã‚’æ›´æ–°
    _, _ = action_queries.update_init_state(connect_session, user_id, project_id, INIT_CLUSTERING_STATUS.EXECUTING)
    
    return JSONResponse(
        status_code=status.HTTP_200_OK,
        content={"message": "init clustering started in background", "data": project_id}
    )


@action_endpoint.get("/action/clustering/continuous/{project_id}", tags=["action"], description="ç¶™ç¶šçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè£…ã™ã‚‹")
def execute_continuous_clustering(
    project_id: int = None,
    user_id: int = None,
    background_tasks: BackgroundTasks = None
):
    """
    ç¶™ç¶šçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°: æ–°è¦è¿½åŠ ã•ã‚ŒãŸæœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒã‚’æ—¢å­˜ã®éšå±¤ã«è¿½åŠ ã™ã‚‹
    
    Args:
        project_id: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
        user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        background_tasks: ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯
        
    Returns:
        JSONResponse: ç¶™ç¶šçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°é–‹å§‹çµæœ
    """
    if project_id is None or user_id is None:
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"message": "project_id and user_id is required", "data": None}
        )

    try:
        project_id = int(project_id)
        user_id = int(user_id)
    except:
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"message": "project_id or user_id is invalid", "data": None}
        )

    connect_session = create_connect_session()

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ¡ãƒ³ãƒãƒ¼ã‚·ãƒƒãƒ—æƒ…å ±ã‚’å–å¾—
    result, _ = action_queries.get_membership_and_project_info(connect_session, project_id, user_id)
    result_mappings = result.mappings().first()

    if result_mappings is None:
        return JSONResponse(
            status_code=status.HTTP_404_NOT_FOUND,
            content={"message": "project or membership not found", "data": None}
        )

    init_clustering_state = result_mappings["init_clustering_state"]
    continuous_clustering_state = result_mappings["continuous_clustering_state"]
    mongo_result_id = result_mappings["mongo_result_id"]
    original_images_folder_path = result_mappings["original_images_folder_path"]

    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºåŠ›
    print(f"\nğŸ” ç¶™ç¶šçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯:")
    print(f"   init_clustering_state: {init_clustering_state} (æœŸå¾…å€¤: {INIT_CLUSTERING_STATUS.FINISHED})")
    print(f"   continuous_clustering_state: {continuous_clustering_state} (æœŸå¾…å€¤: {CONTINUOUS_CLUSTERING_STATUS.EXECUTABLE})")
    print(f"   mongo_result_id: {mongo_result_id}")
    
    # åˆæœŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãŒå®Œäº†ã—ã¦ã„ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
    if init_clustering_state != INIT_CLUSTERING_STATUS.FINISHED:
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={
                "message": "init clustering not completed yet", 
                "data": {
                    "current_init_state": init_clustering_state,
                    "required_init_state": INIT_CLUSTERING_STATUS.FINISHED
                }
            }
        )

    # ç¶™ç¶šçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãŒå®Ÿè¡Œå¯èƒ½ã§ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
    if continuous_clustering_state != CONTINUOUS_CLUSTERING_STATUS.EXECUTABLE:  # 2 = å®Ÿè¡Œå¯èƒ½
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={
                "message": "continuous clustering is not executable", 
                "data": {
                    "current_continuous_state": continuous_clustering_state,
                    "required_continuous_state": CONTINUOUS_CLUSTERING_STATUS.EXECUTABLE
                }
            }
        )

    # æœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒã®å–å¾—ï¼ˆç”»åƒã®è©³ç´°æƒ…å ±ã‚‚å«ã‚ã‚‹ï¼‰
    unclustered_images_query = f"""
        SELECT 
            i.id as image_id,
            i.name as image_name,
            i.clustering_id, 
            i.chromadb_sentence_id, 
            i.chromadb_image_id,
            i.caption,
            i.created_at
        FROM images i
        LEFT JOIN user_image_clustering_states uics 
            ON i.id = uics.image_id AND uics.user_id = {user_id}
        WHERE i.project_id = {project_id} 
            AND i.is_created_caption = TRUE
            AND (uics.is_clustered = 0 OR uics.is_clustered IS NULL);
    """

    result, _ = action_queries.get_unclustered_images(connect_session, project_id, user_id)
    
    if result is None:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={"message": "failed to get unclustered images", "data": None}
        )

    rows = result.mappings().all()
    
    print(f"\nğŸ“Š æœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒã®å–å¾—çµæœ:")
    print(f"   å–å¾—ã—ãŸç”»åƒæ•°: {len(rows)}")
    for row in rows:
        print(f"   - ç”»åƒID: {row['image_id']}, åå‰: {row['image_name']}")
    
    if len(rows) == 0:
        print(f"âš ï¸ æœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={
                "message": "no unclustered images found", 
                "data": {
                    "project_id": project_id,
                    "user_id": user_id,
                    "unclustered_count": 0
                }
            }
        )

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—
    user_result, _ = action_queries.get_user_info(connect_session, user_id)
    user_info = user_result.mappings().first() if user_result else None

    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«è©³ç´°æƒ…å ±ã‚’å‡ºåŠ›
    print("=" * 80)
    print("=" * 80)
    print(f"\nğŸ“‹ å¯¾è±¡ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±:")
    print(f"  - ãƒ¦ãƒ¼ã‚¶ãƒ¼ID: {user_id}")
    if user_info:
        print(f"  - ãƒ¦ãƒ¼ã‚¶ãƒ¼å: {user_info['name']}")
        print(f"  - ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹: {user_info['email']}")
    for idx, row in enumerate(rows, 1):
        print(f"\n  [{idx}] ç”»åƒæƒ…å ±:")
        print(f"      - ç”»åƒID: {row['image_id']}")
        print(f"      - ç”»åƒå: {row['image_name']}")
    print("\n" + "=" * 80)

    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ã«æ¸¡ã™é–¢æ•°
    def run_continuous_clustering(unclustered_rows: list, project_id: int, user_id: int, mongo_result_id: str):
        try:
            print(f"\nğŸ”„ ç¶™ç¶šçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†é–‹å§‹")
            print(f"   ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID: {project_id}")
            print(f"   ãƒ¦ãƒ¼ã‚¶ãƒ¼ID: {user_id}")
            print(f"   æœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒæ•°: {len(unclustered_rows)}")
            
            # ResultManagerã¨ChromaDBManagerã‚’åˆæœŸåŒ–
            result_manager = ResultManager(mongo_result_id)
            # æ–‡ç« åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã¨ç”»åƒåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ä¸¡æ–¹ã‚’ä½¿ç”¨
            sentence_name_db = ChromaDBManager("sentence_name_embeddings")
            image_db = ChromaDBManager("image_embeddings")
            
            # ç¾åœ¨ã®executed_clustering_countã‚’å–å¾—ã—ã¦+1
            count_result, _ = action_queries.get_executed_clustering_count(connect_session, user_id, project_id)
            current_count = count_result.mappings().first()['executed_clustering_count']
            new_count = current_count + 1
            
            print(f"ğŸ“Š ç¾åœ¨ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å›æ•°: {current_count} â†’ æ–°ã—ã„å›æ•°: {new_count}")
            
            # ã™ã¹ã¦ã®ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€ã‚’å–å¾—
            leaf_folders = result_manager.get_all_leaf_folders()
            print(f"ğŸ“‚ ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€æ•°: {len(leaf_folders)}")
            
            if len(leaf_folders) == 0:
                print("âŒ ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return
            
            # å„ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€ã®æ–‡ç« åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã¨ç”»åƒåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®å¹³å‡ã‚’è¨ˆç®—
            folder_sentence_embeddings = {}
            folder_image_embeddings = {}
            for folder in leaf_folders:
                folder_id = folder['id']
                
                # resultå†…ã§ãƒ•ã‚©ãƒ«ãƒ€IDã‚’æ¢ç´¢ã—ã¦dataã‚’å–å¾—
                folder_data_result = result_manager.get_folder_data_from_result(folder_id)
                
                if not folder_data_result['success']:
                    print(f"  âš ï¸ ãƒ•ã‚©ãƒ«ãƒ€ {folder_id} ({folder['name']}) ã®ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: {folder_data_result.get('error', 'Unknown error')}")
                    continue
                
                # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ç”»åƒã®clustering_idã‚’å–å¾—
                folder_data = folder_data_result['data']
                if not isinstance(folder_data, dict) or len(folder_data) == 0:
                    print(f"  âš ï¸ ãƒ•ã‚©ãƒ«ãƒ€ {folder_id} ({folder['name']}) ã¯ç©ºã§ã™")
                    continue
                
                clustering_ids = list(folder_data.keys())
                print(f"  ğŸ“ ãƒ•ã‚©ãƒ«ãƒ€ {folder['name']} ({folder_id}): {len(clustering_ids)}å€‹ã®ç”»åƒã‚’å«ã‚€")
                
                # clustering_idã‹ã‚‰chromadb_sentence_idã¨chromadb_image_idã‚’å–å¾—
                sentence_ids = []
                image_ids = []
                for cid in clustering_ids:
                    sent_result, _ = action_queries.get_chromadb_sentence_id_by_clustering_id(connect_session, cid, project_id)
                    if sent_result:
                        sent_mapping = sent_result.mappings().first()
                        if sent_mapping:
                            sentence_ids.append(sent_mapping['chromadb_sentence_id'])
                    
                    img_result, _ = action_queries.get_chromadb_image_id_by_clustering_id(connect_session, cid, project_id)
                    if img_result:
                        img_mapping = img_result.mappings().first()
                        if img_mapping:
                            image_ids.append(img_mapping['chromadb_image_id'])

                # ChromaDBã‹ã‚‰æ–‡ç« ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
                if len(sentence_ids) > 0:
                    try:
                        sentence_data = sentence_name_db.get_data_by_ids(sentence_ids)
                        sentence_embeddings = sentence_data['embeddings']
                        avg_sentence_embedding = np.mean(sentence_embeddings, axis=0)
                        folder_sentence_embeddings[folder_id] = avg_sentence_embedding
                        print(f"  âœ… ãƒ•ã‚©ãƒ«ãƒ€ {folder['name']} ({folder_id}): {len(sentence_embeddings)}å€‹ã®æ–‡ç« ã®å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—å®Œäº†")
                    except Exception as e:
                        print(f"  âš ï¸ ãƒ•ã‚©ãƒ«ãƒ€ {folder_id} ã®æ–‡ç« åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                
                # ChromaDBã‹ã‚‰ç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
                if len(image_ids) > 0:
                    try:
                        image_data = image_db.get_data_by_ids(image_ids)
                        image_embeddings = image_data['embeddings']
                        avg_image_embedding = np.mean(image_embeddings, axis=0)
                        folder_image_embeddings[folder_id] = avg_image_embedding
                        print(f"  âœ… ãƒ•ã‚©ãƒ«ãƒ€ {folder['name']} ({folder_id}): {len(image_embeddings)}å€‹ã®ç”»åƒã®å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—å®Œäº†")
                    except Exception as e:
                        print(f"  âš ï¸ ãƒ•ã‚©ãƒ«ãƒ€ {folder_id} ã®ç”»åƒåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            
            print(f"\nğŸ“Š æ–‡ç« åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŒã¤ãƒ•ã‚©ãƒ«ãƒ€æ•°: {len(folder_sentence_embeddings)}")
            print(f"ğŸ“Š ç”»åƒåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŒã¤ãƒ•ã‚©ãƒ«ãƒ€æ•°: {len(folder_image_embeddings)}")
            
            # å„æœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒã‚’å‡¦ç†
            for idx, row in enumerate(unclustered_rows, 1):
                try:
                    image_id = row['image_id']
                    image_name = row['image_name']
                    clustering_id = row['clustering_id']
                    chromadb_sentence_id = row['chromadb_sentence_id']
                    chromadb_image_id = row['chromadb_image_id']
                    
                    print(f"\n  [{idx}/{len(unclustered_rows)}] å‡¦ç†ä¸­: {image_name} (ID: {image_id})")
                    
                    # ChromaDBã‹ã‚‰æ–‡ç« ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
                    new_sentence_embedding = None
                    try:
                        new_sentence_data = sentence_name_db.get_data_by_ids([chromadb_sentence_id])
                        new_sentence_embedding = new_sentence_data['embeddings'][0]
                    except Exception as e:
                        print(f"    âš ï¸ æ–‡ç« åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                    
                    # ChromaDBã‹ã‚‰ç”»åƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
                    new_image_embedding = None
                    try:
                        new_image_data = image_db.get_data_by_ids([chromadb_image_id])
                        new_image_embedding = new_image_data['embeddings'][0]
                    except Exception as e:
                        print(f"    âš ï¸ ç”»åƒåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                    
                    # ä¸¡æ–¹ã®ãƒ™ã‚¯ãƒˆãƒ«ãŒå–å¾—ã§ããªã‹ã£ãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    if new_sentence_embedding is None and new_image_embedding is None:
                        print(f"    âš ï¸ åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                        continue
                    
                    # å„ãƒ•ã‚©ãƒ«ãƒ€ã¨ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆæ–‡ç« ã¨ç”»åƒã®ä¸¡æ–¹ï¼‰
                    max_similarity = -1
                    best_folder_id = None
                    best_similarity_type = None  # 'sentence' or 'image'
                    
                    # æ–‡ç« ãƒ™ã‚¯ãƒˆãƒ«ã§é¡ä¼¼åº¦è¨ˆç®—
                    if new_sentence_embedding is not None:
                        for folder_id, folder_embedding in folder_sentence_embeddings.items():
                            similarity = cosine_similarity(
                                [new_sentence_embedding],
                                [folder_embedding]
                            )[0][0]
                            
                            if similarity > max_similarity:
                                max_similarity = similarity
                                best_folder_id = folder_id
                                best_similarity_type = 'sentence'
                    
                    # ç”»åƒãƒ™ã‚¯ãƒˆãƒ«ã§é¡ä¼¼åº¦è¨ˆç®—
                    if new_image_embedding is not None:
                        for folder_id, folder_embedding in folder_image_embeddings.items():
                            similarity = cosine_similarity(
                                [new_image_embedding],
                                [folder_embedding]
                            )[0][0]
                            
                            if similarity > max_similarity:
                                max_similarity = similarity
                                best_folder_id = folder_id
                                best_similarity_type = 'image'
                    
                    if best_folder_id is None:
                        print(f"    âš ï¸ é©åˆ‡ãªãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                        continue
                    
                    print(f"    ğŸ“Š æœ€é«˜é¡ä¼¼åº¦: {max_similarity:.4f} (ã‚¿ã‚¤ãƒ—: {best_similarity_type})")
                    
                    # é¡ä¼¼åº¦é–¾å€¤ãƒã‚§ãƒƒã‚¯ï¼šé–¾å€¤ã‚’ä¸‹å›ã‚‹å ´åˆã¯æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
                    SIMILARITY_THRESHOLD = 0.4  # é¡ä¼¼åº¦é–¾å€¤ï¼ˆèª¿æ•´å¯èƒ½ï¼‰
                    
                    if max_similarity < SIMILARITY_THRESHOLD:
                        print(f"    âš ï¸ æœ€é«˜é¡ä¼¼åº¦ {max_similarity:.4f} ãŒé–¾å€¤ {SIMILARITY_THRESHOLD} ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™")
                        print(f"    ğŸ†• æ–°ã—ã„ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¾ã™...")
                        
                        # ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‹ã‚‰æ–°ãƒ•ã‚©ãƒ«ãƒ€åã‚’ç”Ÿæˆ
                        try:
                            caption_res, _ = images_queries.select_caption_by_clustering_id(connect_session, clustering_id)
                            if caption_res:
                                caption_row = caption_res.mappings().first()
                                if caption_row and 'caption' in caption_row and caption_row['caption']:
                                    caption = caption_row['caption']
                                    # ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‹ã‚‰ç‰¹å¾´çš„ãªå˜èªã‚’æŠ½å‡ºã—ã¦ãƒ•ã‚©ãƒ«ãƒ€åã‚’ç”Ÿæˆ
                                    # æœ€åˆã®æ–‡ç¯€ï¼ˆ.ã®å‰ï¼‰ã‹ã‚‰å˜èªã‚’æŠ½å‡º
                                    first_sentence = caption.split('.')[0] if '.' in caption else caption
                                    # 2-3å€‹ã®ç‰¹å¾´çš„ãªå˜èªã‚’æŠ½å‡ºï¼ˆã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å¤–ï¼‰

                                    words = WordAnalyzer.extract_words(first_sentence)
                                    # æœ€å¤§3å˜èªã§ãƒ•ã‚©ãƒ«ãƒ€åã‚’ä½œæˆ
                                    new_folder_name = ','.join(words[:3]) if len(words) > 0 else f"new_category_{idx}"
                                else:
                                    new_folder_name = f"new_category_{idx}"
                            else:
                                new_folder_name = f"new_category_{idx}"
                        except Exception as name_e:
                            print(f"    âš ï¸ ãƒ•ã‚©ãƒ«ãƒ€åç”Ÿæˆã‚¨ãƒ©ãƒ¼: {name_e}")
                            new_folder_name = f"new_category_{idx}"
                        
                        # imagesãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰image_pathã‚’å–å¾—
                        path_result, _ = action_queries.get_image_name_by_id(connect_session, image_id)
                        image_path = path_result.mappings().first()['name']
                        
                        # ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ï¼ˆparent_id=Noneï¼‰ã«æ–°ã—ã„ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
                        create_result = result_manager.create_new_leaf_folder(
                            folder_name=new_folder_name,
                            parent_id=None,  # ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã«ä½œæˆ
                            initial_clustering_id=clustering_id,
                            initial_image_path=image_path
                        )
                        
                        if create_result['success']:
                            new_folder_id = create_result['folder_id']
                            print(f"    âœ… æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¾ã—ãŸ: {new_folder_name} (ID: {new_folder_id})")
                            
                            # user_image_clustering_statesã‚’æ›´æ–°
                            _, _ = action_queries.update_user_image_state_for_image(connect_session, user_id, image_id, new_count)
                            
                            # æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿½åŠ ï¼ˆä¸¡æ–¹ï¼‰
                            if new_sentence_embedding is not None:
                                folder_sentence_embeddings[new_folder_id] = new_sentence_embedding
                            if new_image_embedding is not None:
                                folder_image_embeddings[new_folder_id] = new_image_embedding
                            
                            # leaf_foldersãƒªã‚¹ãƒˆã«ã‚‚è¿½åŠ 
                            leaf_folders.append({
                                'id': new_folder_id,
                                'name': new_folder_name,
                                'parent_id': None,
                                'is_leaf': True
                            })
                            
                            print(f"    â„¹ï¸ é¡ä¼¼åº¦ãŒä½ã„ãŸã‚ã€å¾Œç¶šã®ãƒ•ã‚©ãƒ«ãƒ€ç‰¹å¾´åˆ†æã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                            continue  # å¾Œç¶šã®å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
                        else:
                            print(f"    âŒ ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆã‚¨ãƒ©ãƒ¼: {create_result.get('error', 'Unknown error')}")
                            print(f"    â†’ æ—¢å­˜ã®ãƒ•ã‚©ãƒ«ãƒ€ã«é…ç½®ã‚’è©¦ã¿ã¾ã™")
                            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã¸ã®é…ç½®å‡¦ç†ã«é€²ã‚€
                    
                    best_folder = next((f for f in leaf_folders if f['id'] == best_folder_id), None)
                    folder_name = best_folder['name'] if best_folder else best_folder_id
                    
                    print(f"    ğŸ¯ æœ€ã‚‚é¡ä¼¼ã—ãŸãƒ•ã‚©ãƒ«ãƒ€: {folder_name} (é¡ä¼¼åº¦: {max_similarity:.4f})")
                    
                    # --- åˆ†é¡åŸºæº–ã‚’ä½¿ã£ãŸæŒ¯ã‚Šåˆ†ã‘ãƒ­ã‚¸ãƒƒã‚¯ ---
                    # å¾Œã§ä½¿ç”¨ã™ã‚‹ãŸã‚ã®å¤‰æ•°ã‚’åˆæœŸåŒ–
                    classification_criteria = {}
                    classification_words_found = []
                    target_folder_id_by_criteria = None
                    
                    # --- æŒ‡å®šã—ãŸãƒ•ã‚©ãƒ«ãƒ€ã¨åŒã˜éšå±¤ã«ã‚ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã‚’å–å¾— ---
                    try:
                        # all_nodesã‹ã‚‰æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€ï¼ˆbest_folderï¼‰ã®æƒ…å ±ã‚’å–å¾—
                        all_nodes = result_manager.get_all_nodes()
                        best_node = all_nodes.get(best_folder_id) if all_nodes else None
                        
                        if best_node:
                            parent_id_of_best = best_node.get('parent_id')
                            print(f"    ğŸ“ æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€ã®parent_id: {parent_id_of_best}")
                            
                            # åŒã˜parent_idã‚’æŒã¤ãƒ•ã‚©ãƒ«ãƒ€ã‚’å–å¾—
                            sibling_folders = []
                            for node_id, node_data in all_nodes.items():
                                if node_data.get('parent_id') == parent_id_of_best:
                                    sibling_folders.append({
                                        'id': node_id,
                                        'name': node_data.get('name'),
                                        'parent_id': node_data.get('parent_id'),
                                        'is_leaf': node_data.get('is_leaf', False)
                                    })
                            
                            print(f"    ğŸ“‚ åŒã˜éšå±¤ã®ãƒ•ã‚©ãƒ«ãƒ€ä¸€è¦§ (count={len(sibling_folders)}):")
                            for sib in sibling_folders:
                                print(f"       - ID: {sib['id']}, Name: {sib['name']}, is_leaf: {sib['is_leaf']}")
                            
                            # --- is_leafãƒ•ã‚©ãƒ«ãƒ€ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ä¸€è¦§ã‚’å–å¾— ---
                            sibling_leaf_folders = [f for f in sibling_folders if f['is_leaf']]
                            print(f"\n    ğŸ“ is_leafãƒ•ã‚©ãƒ«ãƒ€ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³åé›†é–‹å§‹ ({len(sibling_leaf_folders)}å€‹ã®ãƒ•ã‚©ãƒ«ãƒ€)")
                            
                            all_captions = []  # å…¨ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’æ ¼ç´
                            folder_captions_map = {}  # ãƒ•ã‚©ãƒ«ãƒ€ã”ã¨ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³
                            
                            for sib_folder in sibling_leaf_folders:
                                sib_folder_id = sib_folder['id']
                                sib_folder_name = sib_folder['name']
                                
                                # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®clustering_idã‚’å–å¾—
                                folder_data_result = result_manager.get_folder_data_from_result(sib_folder_id)
                                
                                if folder_data_result['success']:
                                    folder_data = folder_data_result['data']
                                    clustering_ids = list(folder_data.keys())
                                    
                                    folder_captions = []
                                    for cid in clustering_ids:
                                        try:
                                            caption_res, _ = images_queries.select_caption_by_clustering_id(connect_session, cid)
                                            if caption_res:
                                                caption_row = caption_res.mappings().first()
                                                if caption_row and 'caption' in caption_row and caption_row['caption']:
                                                    caption = caption_row['caption']
                                                    folder_captions.append(caption)
                                                    all_captions.append(caption)
                                        except Exception as cap_e:
                                            print(f"       âš ï¸ ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³å–å¾—ã‚¨ãƒ©ãƒ¼ (clustering_id: {cid}): {cap_e}")
                                    
                                    folder_captions_map[sib_folder_id] = {
                                        'folder_name': sib_folder_name,
                                        'caption_count': len(folder_captions),
                                        'captions': folder_captions
                                    }
                            
                            print(f"    âœ… åé›†å®Œäº†: å…¨{len(all_captions)}å€‹ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³")
                            
                            # --- é »å‡ºå˜èªãƒªã‚¹ãƒˆã®ä½œæˆ ---
                            print(f"\n    ğŸ“Š é »å‡ºå˜èªåˆ†æé–‹å§‹...")
                            
                            from collections import Counter
                            import re
                            
                            # å…¨ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‹ã‚‰å˜èªã‚’æŠ½å‡º
                            all_words = []
                            for caption in all_captions:
                                # å°æ–‡å­—åŒ–ã—ã¦å˜èªã«åˆ†å‰²
                                words = re.findall(r'\b[a-z]+\b', caption.lower())
                                all_words.extend(words)
                            
                            # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å¤–
                            stopwords_set = set(CAPTION_STOPWORDS)
                            filtered_words = [word for word in all_words if word not in stopwords_set]
                            
                            # å˜èªã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                            word_counter = Counter(filtered_words)
                            
                            # é »å‡ºé †ã«ã‚½ãƒ¼ãƒˆï¼ˆé‡è¤‡ãªã—ï¼‰
                            frequent_words = word_counter.most_common()
                            
                            # --- å„ãƒ•ã‚©ãƒ«ãƒ€ã®å›ºæœ‰å˜èªåˆ†æ ---
                            print(f"\n    ğŸ” å„ãƒ•ã‚©ãƒ«ãƒ€ã®ç‰¹å¾´çš„ãªå˜èªã‚’æŠ½å‡ºä¸­...")
                            
                            # å„ãƒ•ã‚©ãƒ«ãƒ€ã®å˜èªã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’ä½œæˆï¼ˆæ–‡ã®ä½ç½®ã«ã‚ˆã‚‹ãƒã‚¤ã‚¢ã‚¹ä»˜ãï¼‰
                            folder_word_counters = {}
                            for sib_folder_id, folder_info in folder_captions_map.items():
                                folder_words = []
                                for caption in folder_info['captions']:
                                    # ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’æ–‡ã«åˆ†å‰²ï¼ˆ.ã§åŒºåˆ‡ã‚‹ï¼‰
                                    sentences = caption.split('.')
                                    
                                    for sentence_idx, sentence in enumerate(sentences):
                                        if not sentence.strip():  # ç©ºã®æ–‡ã¯ã‚¹ã‚­ãƒƒãƒ—
                                            continue
                                        
                                        # æ–‡ã®ä½ç½®ã«ã‚ˆã‚‹é‡ã¿ï¼ˆ1æ–‡ç›®: 1.0, 2æ–‡ç›®: 0.85, 3æ–‡ç›®: 0.7, ãã‚Œä»¥é™: 0.6ï¼‰
                                        # æ¥µç«¯ã«ãªã‚‰ãªã„ã‚ˆã†ã«èª¿æ•´
                                        if sentence_idx == 0:
                                            position_weight = 1.0
                                        elif sentence_idx == 1:
                                            position_weight = 0.85
                                        elif sentence_idx == 2:
                                            position_weight = 0.7
                                        else:
                                            position_weight = 0.6
                                        
                                        words = re.findall(r'\b[a-z]+\b', sentence.lower())
                                        filtered_sentence_words = [w for w in words if w not in stopwords_set]
                                        
                                        # é‡ã¿ä»˜ãã§å˜èªã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆé‡ã¿ã«å¿œã˜ã¦è¤‡æ•°å›è¿½åŠ ï¼‰
                                        for word in filtered_sentence_words:
                                            # é‡ã¿ã‚’è€ƒæ…®ã™ã‚‹ãŸã‚ã€fractional countã¨ã—ã¦æ‰±ã†
                                            # Counterã¯æ•´æ•°ã—ã‹æ‰±ãˆãªã„ã®ã§ã€å¾Œã§ã‚¹ã‚³ã‚¢è¨ˆç®—æ™‚ã«é©ç”¨
                                            folder_words.append((word, position_weight))
                                
                                # é‡ã¿ä»˜ãã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’ä½œæˆ
                                weighted_counter = {}
                                for word, weight in folder_words:
                                    weighted_counter[word] = weighted_counter.get(word, 0.0) + weight
                                
                                folder_word_counters[sib_folder_id] = weighted_counter
                            
                            # å„ãƒ•ã‚©ãƒ«ãƒ€ã®ç‰¹å¾´çš„ãªå˜èªã‚’æŠ½å‡ºï¼ˆTF-IDFé¢¨ã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼‰
                            folder_unique_words = {}
                            TOP_N_UNIQUE_WORDS = 10  # å„ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰ä¸Šä½Nå€‹ã®ç‰¹å¾´çš„ãªå˜èªã‚’æŠ½å‡º
                            
                            for target_folder_id, target_counter in folder_word_counters.items():
                                # å„å˜èªã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆé‡ã¿ä»˜ãã‚«ã‚¦ãƒ³ãƒˆå¯¾å¿œï¼‰
                                word_scores = {}
                                
                                for word, count_in_target in target_counter.items():
                                    # ã“ã®ãƒ•ã‚©ãƒ«ãƒ€ã§ã®é‡ã¿ä»˜ãå‡ºç¾å›æ•°
                                    tf = count_in_target
                                    
                                    # ä»–ã®ãƒ•ã‚©ãƒ«ãƒ€ã§ã®é‡ã¿ä»˜ãå‡ºç¾å›æ•°ã®åˆè¨ˆ
                                    count_in_others = sum(
                                        other_counter.get(word, 0.0) 
                                        for other_id, other_counter in folder_word_counters.items() 
                                        if other_id != target_folder_id
                                    )
                                    
                                    # ã‚¹ã‚³ã‚¢è¨ˆç®—: (ã“ã®ãƒ•ã‚©ãƒ«ãƒ€ã§ã®é‡ã¿ä»˜ãå‡ºç¾å›æ•°) / (ä»–ã®ãƒ•ã‚©ãƒ«ãƒ€ã§ã®é‡ã¿ä»˜ãå‡ºç¾å›æ•° + 1)
                                    # +1ã¯0é™¤ç®—ã‚’é˜²ããŸã‚
                                    idf_like_score = tf / (count_in_others + 1.0)
                                    
                                    # æœ€çµ‚ã‚¹ã‚³ã‚¢: é‡ã¿ä»˜ãå‡ºç¾å›æ•° Ã— IDFé¢¨ã‚¹ã‚³ã‚¢
                                    final_score = tf * idf_like_score
                                    
                                    word_scores[word] = {
                                        'score': final_score,
                                        'count_in_folder': tf,
                                        'count_in_others': count_in_others,
                                        'ratio': idf_like_score
                                    }
                                
                                # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
                                sorted_words = sorted(
                                    word_scores.items(), 
                                    key=lambda x: x[1]['score'], 
                                    reverse=True
                                )
                                
                                # ä¸Šä½Nå€‹ã‚’å–å¾—
                                top_unique = sorted_words[:TOP_N_UNIQUE_WORDS]
                                
                                folder_unique_words[target_folder_id] = {
                                    'folder_name': folder_captions_map[target_folder_id]['folder_name'],
                                    'unique_words': [
                                        {
                                            'word': word,
                                            'score': round(info['score'], 2),
                                            'count_in_folder': info['count_in_folder'],
                                            'count_in_others': info['count_in_others'],
                                            'ratio': round(info['ratio'], 2)
                                        }
                                        for word, info in top_unique
                                    ]
                                }
                            
                            # --- ãƒ•ã‚©ãƒ«ãƒ€é–“ã®å…±é€šã‚«ãƒ†ã‚´ãƒªåˆ†æ ---
                            print(f"\n    ğŸ” ãƒ•ã‚©ãƒ«ãƒ€é–“ã®å…±é€šã‚«ãƒ†ã‚´ãƒªåˆ†æã‚’é–‹å§‹...")
                            
                            # å„ãƒ•ã‚©ãƒ«ãƒ€ã‹ã‚‰ä¸Šä½10å€‹ã®ç‰¹å¾´çš„å˜èªã‚’å–å¾—
                            folder_top_words_list = {}
                            for folder_id, unique_info in folder_unique_words.items():
                                top_10_words = [w['word'] for w in unique_info['unique_words'][:10]]
                                folder_top_words_list[folder_id] = top_10_words
                            
                            # å…¨ãƒ•ã‚©ãƒ«ãƒ€ã«å…±é€šã™ã‚‹å˜èªã‚’ç‰¹å®š
                            if len(folder_top_words_list) > 0:
                                # å„ãƒ•ã‚©ãƒ«ãƒ€ã®å˜èªã‚»ãƒƒãƒˆã‚’ä½œæˆ
                                folder_word_sets = [set(words) for words in folder_top_words_list.values()]
                                # å…¨ãƒ•ã‚©ãƒ«ãƒ€ã«å…±é€šã™ã‚‹å˜èªã‚’å–å¾—
                                common_to_all_folders = set.intersection(*folder_word_sets) if len(folder_word_sets) > 1 else set()
                                
                                print(f"\n    ğŸ” å…¨ãƒ•ã‚©ãƒ«ãƒ€ã«å…±é€šã™ã‚‹å˜èªã‚’é™¤å¤–...")
                                print(f"       - å…¨ãƒ•ã‚©ãƒ«ãƒ€æ•°: {len(folder_word_sets)}")
                                print(f"       - å…¨ãƒ•ã‚©ãƒ«ãƒ€ã«å…±é€šã™ã‚‹å˜èªæ•°: {len(common_to_all_folders)}")
                                
                                if len(common_to_all_folders) > 0:
                                    common_words_display = ', '.join(sorted(list(common_to_all_folders)))
                                    print(f"       - å…±é€šå˜èª: {common_words_display}")
                                    
                                    # å„ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒˆãƒƒãƒ—10å˜èªã‹ã‚‰å…±é€šå˜èªã‚’é™¤å¤–
                                    for folder_id in folder_top_words_list.keys():
                                        original_count = len(folder_top_words_list[folder_id])
                                        folder_top_words_list[folder_id] = [
                                            w for w in folder_top_words_list[folder_id] 
                                            if w not in common_to_all_folders
                                        ]
                                        removed_count = original_count - len(folder_top_words_list[folder_id])
                                        if removed_count > 0:
                                            folder_name = folder_unique_words[folder_id]['folder_name']
                                            print(f"       - ğŸ“ {folder_name}: {removed_count}å€‹ã®å…±é€šå˜èªã‚’é™¤å¤–")
                                else:
                                    print(f"       â„¹ï¸ å…¨ãƒ•ã‚©ãƒ«ãƒ€ã«å…±é€šã™ã‚‹å˜èªã¯ã‚ã‚Šã¾ã›ã‚“")
                            
                            # WordAnalyzerã‚’åˆæœŸåŒ–ï¼ˆæ—¢å­˜ã®WordNetãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ï¼‰
                            from sentence_transformers import SentenceTransformer
                            embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
                            word_analyzer = WordAnalyzer(embedding_model)
                            
                            # --- æ–°ã—ã„ãƒ­ã‚¸ãƒƒã‚¯: å…¨ãƒ•ã‚©ãƒ«ãƒ€ã§åŒã˜ã‚«ãƒ†ã‚´ãƒªã‚’æŒã¤å˜èªã®ã¿ã‚’æŠ½å‡º ---
                            print(f"\n    ğŸ” å…¨ãƒ•ã‚©ãƒ«ãƒ€é–“ã§å…±é€šã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã®å˜èªã‚’åˆ†æ...")
                            
                            folder_ids_list = list(folder_unique_words.keys())
                            
                            # å„å˜èªãŒã©ã®ã‚«ãƒ†ã‚´ãƒªã«å±ã™ã‚‹ã‹ã‚’ãƒ•ã‚©ãƒ«ãƒ€ã”ã¨ã«åˆ†æ
                            # {folder_id: {word: [(category, score, target_word), ...]}}
                            folder_word_categories = {}
                            
                            for folder_id in folder_ids_list:
                                folder_word_categories[folder_id] = {}
                                folder_words = folder_top_words_list[folder_id]
                                
                                for word in folder_words:
                                    # ã“ã®å˜èªã¨ä»–ã®å…¨ãƒ•ã‚©ãƒ«ãƒ€ã®å˜èªã‚’æ¯”è¼ƒã—ã¦ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—
                                    word_category_info = []  # [(category, score, other_folder_word), ...]
                                    
                                    for other_folder_id in folder_ids_list:
                                        if other_folder_id == folder_id:
                                            continue
                                        
                                        other_folder_words = folder_top_words_list[other_folder_id]
                                        
                                        for other_word in other_folder_words:
                                            common_categories, category_score = word_analyzer.get_common_category(word, other_word)
                                            
                                            if len(common_categories) > 0 and category_score >= 3.0:
                                                # ã‚¹ã‚³ã‚¢3.0ä»¥ä¸Šã®å…±é€šã‚«ãƒ†ã‚´ãƒªã®ã¿
                                                for cat in common_categories[:1]:  # æœ€ä¸Šä½ã‚«ãƒ†ã‚´ãƒªã®ã¿
                                                    word_category_info.append((cat, category_score, other_word, other_folder_id))
                                    
                                    if len(word_category_info) > 0:
                                        folder_word_categories[folder_id][word] = word_category_info
                            
                            # å…¨ãƒ•ã‚©ãƒ«ãƒ€ã§å…±é€šã—ã¦å‡ºç¾ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã‚’ç‰¹å®š
                            print(f"\n    ğŸ” å…¨ãƒ•ã‚©ãƒ«ãƒ€ã§å…±é€šã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã‚’ç‰¹å®šä¸­...")
                            
                            from collections import defaultdict
                            category_occurrence = defaultdict(lambda: {
                                'folders': set(),
                                'words_by_folder': defaultdict(list),  # {folder_id: [(word, avg_score)]}
                                'word_category_scores': defaultdict(list)  # {word: [scores]}
                            })
                            
                            # å„ãƒ•ã‚©ãƒ«ãƒ€ã®å„å˜èªãŒå±ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã‚’é›†è¨ˆ
                            for folder_id, word_cats in folder_word_categories.items():
                                for word, cat_info_list in word_cats.items():
                                    if len(cat_info_list) == 0:
                                        continue
                                    
                                    # ã“ã®wordãŒæœ€ã‚‚å±ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã‚’æ±ºå®šï¼ˆã‚¹ã‚³ã‚¢ã®å¹³å‡ãŒæœ€ã‚‚é«˜ã„ã‚«ãƒ†ã‚´ãƒªï¼‰
                                    cat_scores = defaultdict(list)
                                    for cat, score, other_word, other_folder_id in cat_info_list:
                                        cat_scores[cat].append(score)
                                    
                                    # å„ã‚«ãƒ†ã‚´ãƒªã®å¹³å‡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                                    best_category = None
                                    best_avg_score = -1
                                    for cat, scores in cat_scores.items():
                                        avg_score = sum(scores) / len(scores)
                                        if avg_score > best_avg_score:
                                            best_avg_score = avg_score
                                            best_category = cat
                                    
                                    if best_category:
                                        category_occurrence[best_category]['folders'].add(folder_id)
                                        category_occurrence[best_category]['words_by_folder'][folder_id].append((word, best_avg_score))
                                        category_occurrence[best_category]['word_category_scores'][word].append(best_avg_score)
                            
                            # å…¨ãƒ•ã‚©ãƒ«ãƒ€ã«å‡ºç¾ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                            num_folders = len(folder_ids_list)
                            common_categories_across_all_folders = {}
                            
                            for category, info in category_occurrence.items():
                                if len(info['folders']) == num_folders:
                                    # å…¨ãƒ•ã‚©ãƒ«ãƒ€ã«å‡ºç¾ã™ã‚‹ã‚«ãƒ†ã‚´ãƒª
                                    common_categories_across_all_folders[category] = info
                            
                            print(f"       âœ… å…¨{num_folders}å€‹ã®ãƒ•ã‚©ãƒ«ãƒ€ã«å…±é€šã™ã‚‹ã‚«ãƒ†ã‚´ãƒªæ•°: {len(common_categories_across_all_folders)}")
                            
                            # åˆ†é¡åŸºæº–ã®æ¨å®š
                            classification_criteria = {}
                            
                            if len(common_categories_across_all_folders) > 0:
                                # å„ã‚«ãƒ†ã‚´ãƒªã®å¹³å‡ã‚¹ã‚³ã‚¢ã¨å˜èªæ•°ã§è©•ä¾¡
                                sorted_categories = []
                                for category, info in common_categories_across_all_folders.items():
                                    # å„å˜èªã®å¹³å‡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                                    word_scores = []
                                    words_with_scores = []  # [(word, avg_score)]
                                    
                                    for word, scores_list in info['word_category_scores'].items():
                                        avg_score = sum(scores_list) / len(scores_list)
                                        word_scores.append(avg_score)
                                        words_with_scores.append((word, avg_score))
                                    
                                    # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
                                    words_with_scores.sort(key=lambda x: x[1], reverse=True)
                                    
                                    category_avg_score = sum(word_scores) / len(word_scores) if len(word_scores) > 0 else 0.0
                                    
                                    sorted_categories.append((category, {
                                        'words_with_scores': words_with_scores,
                                        'avg_score': category_avg_score,
                                        'word_count': len(words_with_scores),
                                        'folders': info['folders']
                                    }))
                                
                                # å¹³å‡ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
                                sorted_categories.sort(key=lambda x: x[1]['avg_score'], reverse=True)
                                
                                print(f"\n    ğŸ“‹ æ¨å®šã•ã‚Œã‚‹åˆ†é¡åŸºæº–ï¼ˆå…¨ãƒ•ã‚©ãƒ«ãƒ€å…±é€šã‚«ãƒ†ã‚´ãƒªï¼‰:")
                                
                                for rank, (category, info) in enumerate(sorted_categories[:10], 1):
                                    words_display = ', '.join([f"{w}({s:.2f})" for w, s in info['words_with_scores'][:5]])
                                    folders_list = sorted([folder_unique_words[fid]['folder_name'] for fid in info['folders']])
                                    
                                    # å„ãƒ•ã‚©ãƒ«ãƒ€ã§ã“ã®ã‚«ãƒ†ã‚´ãƒªã«å±ã™ã‚‹å˜èªã‚’è¡¨ç¤º
                                    folder_words_display = []
                                    for fid in info['folders']:
                                        fname = folder_unique_words[fid]['folder_name']
                                        # ã“ã®ãƒ•ã‚©ãƒ«ãƒ€ã§ã“ã®ã‚«ãƒ†ã‚´ãƒªã«å±ã™ã‚‹å˜èª
                                        fwords = [w for w, s in info['words_with_scores'] if any(
                                            w == fw[0] for fw in common_categories_across_all_folders[category]['words_by_folder'].get(fid, [])
                                        )]
                                        if len(fwords) > 0:
                                            folder_words_display.append(f"{fname}:{fwords[0]}")
                                    
                                    classification_criteria[category] = {
                                        'rank': rank,
                                        'category': category,
                                        'words': [w for w, s in info['words_with_scores']],
                                        'words_with_scores': info['words_with_scores'],
                                        'word_count': info['word_count'],
                                        'avg_score': round(info['avg_score'], 2),
                                        'folders': folders_list,
                                        'folder_words': folder_words_display
                                    }
                                    
                                    print(f"\n       {rank}. ã‚«ãƒ†ã‚´ãƒª: {category}")
                                    print(f"          - å¹³å‡ã‚¹ã‚³ã‚¢: {info['avg_score']:.2f}")
                                    print(f"          - å˜èªæ•°: {info['word_count']}")
                                    print(f"          - å…¨ãƒ•ã‚©ãƒ«ãƒ€æ•°: {len(info['folders'])}")
                                    print(f"          - è©²å½“å˜èª (ã‚¹ã‚³ã‚¢é †ä¸Šä½5): {words_display}")
                                    print(f"          - ãƒ•ã‚©ãƒ«ãƒ€åˆ¥å˜èª: {', '.join(folder_words_display[:5])}")
                                
                                # æœ€ã‚‚æ”¯é…çš„ãªã‚«ãƒ†ã‚´ãƒªã‚’åˆ†é¡åŸºæº–ã¨ã—ã¦ç‰¹å®š
                                if len(sorted_categories) > 0:
                                    top_category = sorted_categories[0][0]
                                    top_info = sorted_categories[0][1]
                                    top_words = ', '.join([w for w, s in top_info['words_with_scores'][:5]])
                                    
                                    print(f"\n    ğŸ¯ æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„åˆ†é¡åŸºæº–:")
                                    print(f"       ã‚«ãƒ†ã‚´ãƒª: {top_category}")
                                    print(f"       å¹³å‡ã‚¹ã‚³ã‚¢: {top_info['avg_score']:.2f}")
                                    print(f"       è©²å½“å˜èª: {top_words}")
                                    
                            else:
                                print(f"       âš ï¸ å…¨ãƒ•ã‚©ãƒ«ãƒ€ã«å…±é€šã™ã‚‹ã‚«ãƒ†ã‚´ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                            
                            # ãƒ‡ãƒãƒƒã‚°ç”¨JSONå‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
                            debug_output = {
                                'summary': {
                                    'total_captions': len(all_captions),
                                    'total_words_before_filtering': len(all_words),
                                    'total_words_after_filtering': len(filtered_words),
                                    'unique_words': len(word_counter),
                                    'sibling_leaf_folder_count': len(sibling_leaf_folders),
                                    'common_to_all_folders_count': len(common_to_all_folders) if 'common_to_all_folders' in locals() else 0,
                                    'common_categories_count': len(common_categories_across_all_folders) if 'common_categories_across_all_folders' in locals() else 0,
                                    'classification_criteria_count': len(classification_criteria)
                                },
                                'common_to_all_folders': sorted(list(common_to_all_folders)) if 'common_to_all_folders' in locals() else [],
                                'common_categories_across_all_folders': {
                                    cat: {
                                        'words': [w for w, s in info['words_with_scores']] if 'words_with_scores' in info else [],
                                        'avg_score': info.get('avg_score', 0.0),
                                        'folders': list(info.get('folders', []))
                                    }
                                    for cat, info in (dict(sorted_categories) if 'sorted_categories' in locals() and sorted_categories else {}).items()
                                } if 'sorted_categories' in locals() else {},
                                'classification_criteria': classification_criteria,
                                'folder_captions': folder_captions_map,
                                'frequent_words': [
                                    {
                                        'word': word,
                                        'count': count,
                                        'rank': idx + 1
                                    }
                                    for idx, (word, count) in enumerate(frequent_words)
                                ],
                                'top_20_words': [
                                    {
                                        'word': word,
                                        'count': count
                                    }
                                    for word, count in frequent_words[:20]
                                ],
                                'folder_unique_words': folder_unique_words
                            }
                            
                            # JSONå½¢å¼ã§å‡ºåŠ›
                            import json
                            from datetime import datetime
                            
                            # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            json_filename = f"sibling_captions_analysis_{timestamp}.json"
                            json_filepath = os.path.join("./", json_filename)
                            
                            try:
                                with open(json_filepath, 'w', encoding='utf-8') as f:
                                    json.dump(debug_output, f, indent=2, ensure_ascii=False)
                                print(f"\n    ğŸ’¾ JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {json_filepath}")
                            except Exception as json_e:
                                print(f"    âš ï¸ JSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {json_e}")
                            
                            # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã‚‚å‡ºåŠ›ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                            print(f"\n    ğŸ“‹ ãƒ‡ãƒãƒƒã‚°ç”¨JSONå‡ºåŠ›ã‚µãƒãƒªãƒ¼:")
                            print(f"       - ç·ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³æ•°: {debug_output['summary']['total_captions']}")
                            print(f"       - ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªæ•°: {debug_output['summary']['unique_words']}")
                            print(f"       - ãƒ•ã‚©ãƒ«ãƒ€æ•°: {debug_output['summary']['sibling_leaf_folder_count']}")
                            
                            print(f"\n    ğŸ” Top 20 é »å‡ºå˜èª:")
                            for idx, (word, count) in enumerate(frequent_words[:20], 1):
                                print(f"       {idx:2d}. {word:20s} : {count:4d}å›")
                            
                            print(f"\n    ğŸ¯ å„ãƒ•ã‚©ãƒ«ãƒ€ã®ç‰¹å¾´çš„ãªå˜èª (Top {TOP_N_UNIQUE_WORDS}):")
                            for folder_id, unique_info in folder_unique_words.items():
                                folder_name = unique_info['folder_name']
                                print(f"\n       ğŸ“ {folder_name} (ID: {folder_id}):")
                                for rank, word_info in enumerate(unique_info['unique_words'], 1):
                                    print(f"          {rank:2d}. {word_info['word']:20s} | "
                                          f"ã‚¹ã‚³ã‚¢: {word_info['score']:6.2f} | "
                                          f"ã“ã®ãƒ•ã‚©ãƒ«ãƒ€: {word_info['count_in_folder']:6.2f}å› | "
                                          f"ä»–ãƒ•ã‚©ãƒ«ãƒ€: {word_info['count_in_others']:6.2f}å› | "
                                          f"æ¯”ç‡: {word_info['ratio']:5.2f}")
                            
                            
                            # --- åˆ†é¡åŸºæº–ã‚’ä½¿ã£ãŸæ–°è¦ç”»åƒã®æŒ¯ã‚Šåˆ†ã‘ãƒ­ã‚¸ãƒƒã‚¯ ---
                            print(f"\n    ğŸ” åˆ†é¡åŸºæº–ã‚’ä½¿ã£ãŸæŒ¯ã‚Šåˆ†ã‘ãƒ­ã‚¸ãƒƒã‚¯ã‚’é–‹å§‹...")
                            
                            # æ–°è¦ç”»åƒã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—
                            try:
                                new_image_caption_res, _ = images_queries.select_caption_by_clustering_id(connect_session, clustering_id)
                                new_image_caption = None
                                if new_image_caption_res:
                                    caption_row = new_image_caption_res.mappings().first()
                                    if caption_row and 'caption' in caption_row and caption_row['caption']:
                                        new_image_caption = caption_row['caption'].lower()
                                
                                if new_image_caption:
                                    print(f"       ğŸ“ æ–°è¦ç”»åƒã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³: {new_image_caption[:100]}...")
                                    
                                    # åˆ†é¡åŸºæº–ã‹ã‚‰æœ€ä¸Šä½ã‚«ãƒ†ã‚´ãƒªã®ã¿ã‚’ä½¿ç”¨
                                    if len(classification_criteria) > 0:
                                        # rankã§ã‚½ãƒ¼ãƒˆï¼ˆã™ã§ã«rankãŒã¤ã„ã¦ã„ã‚‹ï¼‰
                                        sorted_criteria = sorted(
                                            classification_criteria.items(),
                                            key=lambda x: x[1].get('rank', 999)
                                        )
                                        
                                        # æœ€ä¸Šä½ã‚«ãƒ†ã‚´ãƒªã®ã¿ã‚’ä½¿ç”¨
                                        top_category, top_info = sorted_criteria[0]
                                        top_category_words_with_scores = top_info.get('words_with_scores', [])
                                        
                                        print(f"\n       ğŸ¯ åˆ†é¡åŸºæº–ã‚«ãƒ†ã‚´ãƒª: {top_category}")
                                        print(f"          è©²å½“å˜èª: {', '.join([f'{w}({s:.2f})' for w, s in top_category_words_with_scores[:5]])}")
                                        
                                        # æ–°è¦ç”»åƒã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‹ã‚‰å˜èªã‚’æŠ½å‡º
                                        new_image_words = set(re.findall(r'\b[a-z]+\b', new_image_caption))
                                        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å¤–
                                        new_image_words = new_image_words - stopwords_set
                                        
                                        # ã“ã®ã‚«ãƒ†ã‚´ãƒªã®å˜èªãƒªã‚¹ãƒˆ
                                        top_category_words = set([w for w, s in top_category_words_with_scores])
                                        
                                        # ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³å†…ã«åˆ†é¡åŸºæº–å˜èªãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
                                        found_exact_words = new_image_words & top_category_words
                                        
                                        # WordNetã‚’ä½¿ã£ã¦ã“ã®ã‚«ãƒ†ã‚´ãƒªã«å±ã™ã‚‹å˜èªã‚’æ¢ã™
                                        category_matched_words = []  # [(word, avg_score)]
                                        
                                        # é™¤å¤–ã™ã‚‹å˜èªã‚»ãƒƒãƒˆ: ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ + å…¨ãƒ•ã‚©ãƒ«ãƒ€å…±é€šå˜èª
                                        exclude_words_for_matching = stopwords_set.copy()
                                        if 'common_to_all_folders' in locals() and len(common_to_all_folders) > 0:
                                            exclude_words_for_matching.update(common_to_all_folders)
                                        
                                        for new_word in new_image_words:
                                            if new_word in top_category_words:
                                                # ã™ã§ã«å®Œå…¨ä¸€è‡´ã§è¦‹ã¤ã‹ã£ã¦ã„ã‚‹
                                                continue
                                            
                                            # é™¤å¤–å˜èªãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—
                                            if new_word in exclude_words_for_matching:
                                                continue
                                            
                                            # æ–°è¦å˜èªãŒã“ã®ã‚«ãƒ†ã‚´ãƒªã«å±ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                                            scores_for_word = []
                                            for category_word, cat_word_score in top_category_words_with_scores:
                                                common_categories, category_score = word_analyzer.get_common_category(new_word, category_word)
                                                
                                                # æœ€ä¸Šä½ã®å…±é€šã‚«ãƒ†ã‚´ãƒªãŒtop_categoryã¨ä¸€è‡´ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                                                if len(common_categories) > 0 and common_categories[0] == top_category and category_score >= 3.0:
                                                    scores_for_word.append(category_score)
                                                    print(f"       ğŸ”— '{new_word}' ã¯ '{category_word}' ã¨åŒã˜ã‚«ãƒ†ã‚´ãƒª '{top_category}' (ã‚¹ã‚³ã‚¢: {category_score:.2f})")
                                            
                                            if len(scores_for_word) > 0:
                                                # ã“ã®å˜èªã®ã‚«ãƒ†ã‚´ãƒªã¨ã®å¹³å‡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                                                avg_score = sum(scores_for_word) / len(scores_for_word)
                                                category_matched_words.append((new_word, avg_score))
                                        
                                        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
                                        category_matched_words.sort(key=lambda x: x[1], reverse=True)
                                        
                                        if len(found_exact_words) > 0:
                                            print(f"\n       âœ… ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³å†…ã«åˆ†é¡åŸºæº–å˜èªã‚’ç™ºè¦‹ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰: {', '.join(found_exact_words)}")
                                        
                                        if len(category_matched_words) > 0:
                                            print(f"\n       ğŸ” ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³å†…ã«åˆ†é¡åŸºæº–ã‚«ãƒ†ã‚´ãƒª '{top_category}' ã«å±ã™ã‚‹å˜èªã‚’ç™ºè¦‹:")
                                            top_matched = [f"{w}({s:.2f})" for w, s in category_matched_words[:5]]
                                            print(f"          {', '.join(top_matched)}")
                                        
                                        # å®Œå…¨ä¸€è‡´ + ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒã‚’çµ±åˆ
                                        all_found_words = found_exact_words.copy()
                                        for word, score in category_matched_words:
                                            all_found_words.add(word)
                                        
                                        if len(all_found_words) > 0:
                                            print(f"\n       ğŸ“‚ æ—¢å­˜ã®å…„å¼Ÿãƒ•ã‚©ãƒ«ãƒ€ã¨ç…§åˆã—ã¾ã™...")
                                            
                                            # å€™è£œãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚¹ã‚³ã‚¢ä»˜ãã§æ ¼ç´
                                            folder_candidates = []
                                            
                                            # ã¾ãšå®Œå…¨ä¸€è‡´ã®å˜èªã‚’å„ªå…ˆã—ã¦ãƒã‚§ãƒƒã‚¯
                                            for word in found_exact_words:
                                                for sib_folder in sibling_leaf_folders:
                                                    sib_folder_name = sib_folder['name'].lower()
                                                    
                                                    # å®Œå…¨ä¸€è‡´: ãƒ•ã‚©ãƒ«ãƒ€å == å˜èª ã¾ãŸã¯ ãƒ•ã‚©ãƒ«ãƒ€åã«å˜èªãŒå«ã¾ã‚Œã‚‹
                                                    if sib_folder_name == word or word in sib_folder_name.split(','):
                                                        # ã“ã®ãƒ•ã‚©ãƒ«ãƒ€ã®TF-IDFã‚¹ã‚³ã‚¢ã‚’å–å¾—
                                                        folder_score = 0.0
                                                        if sib_folder['id'] in folder_unique_words:
                                                            for w_info in folder_unique_words[sib_folder['id']]['unique_words']:
                                                                if w_info['word'] == word:
                                                                    folder_score = w_info['score']
                                                                    break
                                                        
                                                        folder_candidates.append({
                                                            'folder': sib_folder,
                                                            'word': word,
                                                            'score': folder_score + 1000,  # å®Œå…¨ä¸€è‡´ã‚’å„ªå…ˆ
                                                            'match_type': 'exact'
                                                        })
                                                        print(f"       ğŸ¯ å€™è£œãƒ•ã‚©ãƒ«ãƒ€ç™ºè¦‹ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰: '{word}' â†’ '{sib_folder['name']}' (ã‚¹ã‚³ã‚¢: {folder_score:.2f})")
                                            
                                            # å®Œå…¨ä¸€è‡´å€™è£œãŒãªã‘ã‚Œã°ã€ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒå˜èªã‚’ãƒã‚§ãƒƒã‚¯
                                            if len(folder_candidates) == 0:
                                                # ã‚«ãƒ†ã‚´ãƒªã«æœ€ã‚‚è¿‘ã„å˜èªã‚’å„ªå…ˆï¼ˆã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆæ¸ˆã¿ï¼‰
                                                for word, word_category_score in category_matched_words:
                                                    for sib_folder in sibling_leaf_folders:
                                                        sib_folder_name = sib_folder['name'].lower()
                                                        if sib_folder_name == word or word in sib_folder_name.split(','):
                                                            folder_score = 0.0
                                                            if sib_folder['id'] in folder_unique_words:
                                                                for w_info in folder_unique_words[sib_folder['id']]['unique_words']:
                                                                    if w_info['word'] == word:
                                                                        folder_score = w_info['score']
                                                                        break
                                                            
                                                            # ã‚«ãƒ†ã‚´ãƒªã‚¹ã‚³ã‚¢ã¨ãƒ•ã‚©ãƒ«ãƒ€ã‚¹ã‚³ã‚¢ã‚’çµ„ã¿åˆã‚ã›ã¦è©•ä¾¡
                                                            combined_score = folder_score + (word_category_score * 0.1)
                                                            
                                                            folder_candidates.append({
                                                                'folder': sib_folder,
                                                                'word': word,
                                                                'score': combined_score,
                                                                'match_type': 'category',
                                                                'category_score': word_category_score
                                                            })
                                                            print(f"       ğŸ¯ å€™è£œãƒ•ã‚©ãƒ«ãƒ€ç™ºè¦‹ï¼ˆã‚«ãƒ†ã‚´ãƒªä¸€è‡´ï¼‰: '{word}' (ã‚«ãƒ†ã‚´ãƒªã‚¹ã‚³ã‚¢: {word_category_score:.2f}) â†’ '{sib_folder['name']}' (ç·åˆã‚¹ã‚³ã‚¢: {combined_score:.2f})")
                                            
                                            # å€™è£œã‹ã‚‰æœ€é©ãªãƒ•ã‚©ãƒ«ãƒ€ã‚’é¸æŠ
                                            matched_folder = None
                                            matched_word = None
                                            
                                            if len(folder_candidates) > 0:
                                                # ã‚¹ã‚³ã‚¢ãŒé«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
                                                folder_candidates.sort(key=lambda x: x['score'], reverse=True)
                                                best_candidate = folder_candidates[0]
                                                matched_folder = best_candidate['folder']
                                                matched_word = best_candidate['word']
                                                
                                                print(f"       â­ æœ€é©ãƒ•ã‚©ãƒ«ãƒ€ã‚’é¸æŠ: '{matched_word}' â†’ '{matched_folder['name']}' (ã‚¹ã‚³ã‚¢: {best_candidate['score']:.2f}, ã‚¿ã‚¤ãƒ—: {best_candidate['match_type']})")
                                            
                                            if matched_folder:
                                                # æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã«æŒ¿å…¥
                                                target_folder_id_by_criteria = matched_folder['id']
                                                print(f"       ğŸ“‚ æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã¸æŒ¿å…¥äºˆå®š: {matched_folder['name']} (ID: {target_folder_id_by_criteria})")
                                            else:
                                                # æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆï¼ˆå®Œå…¨ä¸€è‡´ã‚’å„ªå…ˆã€ãªã‘ã‚Œã°ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒã§æœ€ã‚‚ã‚¹ã‚³ã‚¢ã®é«˜ã„å˜èªã‚’ä½¿ç”¨ï¼‰
                                                new_folder_word = None
                                                if len(found_exact_words) > 0:
                                                    new_folder_word = list(found_exact_words)[0]
                                                else:
                                                    # ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒã‹ã‚‰æœ€ã‚‚ã‚«ãƒ†ã‚´ãƒªã«è¿‘ã„å˜èªã‚’å–å¾—ï¼ˆæœ€é«˜ã‚¹ã‚³ã‚¢ï¼‰
                                                    if len(category_matched_words) > 0:
                                                        new_folder_word = category_matched_words[0][0]  # æœ€é«˜ã‚¹ã‚³ã‚¢ã®å˜èª
                                                
                                                if new_folder_word:
                                                    print(f"       ğŸ†• æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆäºˆå®š: ãƒ•ã‚©ãƒ«ãƒ€å='{new_folder_word}'")
                                                    
                                                    # è¦ªãƒ•ã‚©ãƒ«ãƒ€IDã‚’å–å¾—ï¼ˆbest_folderã¨åŒã˜éšå±¤ï¼‰
                                                    parent_id_for_new = parent_id_of_best if 'parent_id_of_best' in locals() else None
                                                    
                                                    # ç”»åƒãƒ‘ã‚¹ã‚’å–å¾—
                                                    path_result_temp, _ = action_queries.get_image_name_by_id(connect_session, image_id)
                                                    image_path_temp = path_result_temp.mappings().first()['name']
                                                    
                                                    # æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
                                                    create_result = result_manager.create_new_leaf_folder(
                                                        folder_name=new_folder_word,
                                                        parent_id=parent_id_for_new,
                                                        initial_clustering_id=clustering_id,
                                                        initial_image_path=image_path_temp
                                                    )
                                                    
                                                    if create_result['success']:
                                                        new_folder_id = create_result['folder_id']
                                                        target_folder_id_by_criteria = new_folder_id
                                                        print(f"       âœ… æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆæˆåŠŸ: '{new_folder_word}' (ID: {new_folder_id})")
                                                        
                                                        # user_image_clustering_statesã‚’æ›´æ–°
                                                        _, _ = action_queries.update_user_image_state_for_image(connect_session, user_id, image_id, new_count)
                                                        
                                                        # æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿½åŠ 
                                                        if new_sentence_embedding is not None:
                                                            folder_sentence_embeddings[new_folder_id] = new_sentence_embedding
                                                        if new_image_embedding is not None:
                                                            folder_image_embeddings[new_folder_id] = new_image_embedding
                                                        
                                                        # leaf_foldersãƒªã‚¹ãƒˆã«ã‚‚è¿½åŠ 
                                                        leaf_folders.append({
                                                            'id': new_folder_id,
                                                            'name': new_folder_word,
                                                            'parent_id': parent_id_for_new,
                                                            'is_leaf': True
                                                        })
                                                        
                                                        # sibling_leaf_foldersã«ã‚‚è¿½åŠ 
                                                        sibling_leaf_folders.append({
                                                            'id': new_folder_id,
                                                            'name': new_folder_word,
                                                            'parent_id': parent_id_for_new,
                                                            'is_leaf': True
                                                        })
                                                        
                                                        print(f"       â„¹ï¸ æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆã«ã‚ˆã‚Šã€å¾Œç¶šã®æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€æŒ¿å…¥å‡¦ç†ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                                                        # æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆãŒæˆåŠŸã—ãŸã®ã§ã€å¾Œç¶šã®æŒ¿å…¥å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
                                                        continue
                                                    else:
                                                        print(f"       âŒ æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆå¤±æ•—: {create_result.get('error', 'Unknown error')}")
                                                        print(f"       â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦æœ€ã‚‚é¡ä¼¼ã—ãŸãƒ•ã‚©ãƒ«ãƒ€ã«æŒ¿å…¥ã—ã¾ã™")
                                                        target_folder_id_by_criteria = None
                                                    print(f"       â„¹ï¸ æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆã«ã‚ˆã‚Šã€å¾Œç¶šã®æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€æŒ¿å…¥å‡¦ç†ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                                                    # æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆãŒæˆåŠŸã—ãŸã®ã§ã€å¾Œç¶šã®æŒ¿å…¥å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
                                                    continue
                                                else:
                                                    print(f"       âŒ æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆå¤±æ•—: {create_result.get('error', 'Unknown error')}")
                                                    print(f"       â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦æœ€ã‚‚é¡ä¼¼ã—ãŸãƒ•ã‚©ãƒ«ãƒ€ã«æŒ¿å…¥ã—ã¾ã™")
                                                    target_folder_id_by_criteria = None
                                        else:
                                            print(f"       â„¹ï¸ ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³å†…ã«åˆ†é¡åŸºæº–å˜èªãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
                                            print(f"       â†’ æœ€ã‚‚é¡ä¼¼ã—ãŸãƒ•ã‚©ãƒ«ãƒ€ï¼ˆ{folder_name}ï¼‰ã«æŒ¿å…¥ã—ã¾ã™")
                                    else:
                                        print(f"       â„¹ï¸ åˆ†é¡åŸºæº–ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“")
                                        print(f"       â†’ æœ€ã‚‚é¡ä¼¼ã—ãŸãƒ•ã‚©ãƒ«ãƒ€ï¼ˆ{folder_name}ï¼‰ã«æŒ¿å…¥ã—ã¾ã™")
                                else:
                                    print(f"       âš ï¸ æ–°è¦ç”»åƒã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                                    print(f"       â†’ æœ€ã‚‚é¡ä¼¼ã—ãŸãƒ•ã‚©ãƒ«ãƒ€ï¼ˆ{folder_name}ï¼‰ã«æŒ¿å…¥ã—ã¾ã™")
                            
                            except Exception as criteria_e:
                                print(f"       âš ï¸ åˆ†é¡åŸºæº–ã«ã‚ˆã‚‹æŒ¯ã‚Šåˆ†ã‘å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {criteria_e}")
                                traceback.print_exc()
                                print(f"       â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦æœ€ã‚‚é¡ä¼¼ã—ãŸãƒ•ã‚©ãƒ«ãƒ€ã«æŒ¿å…¥ã—ã¾ã™")
                            
                        else:
                            print(f"    âš ï¸ æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€ {best_folder_id} ãŒall_nodesã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    except Exception as sib_e:
                        print(f"    âš ï¸ åŒéšå±¤ãƒ•ã‚©ãƒ«ãƒ€å–å¾—ã‚¨ãƒ©ãƒ¼: {sib_e}")
                        traceback.print_exc()
                    
                    # æœ€çµ‚çš„ãªæŒ¿å…¥å…ˆãƒ•ã‚©ãƒ«ãƒ€ã‚’æ±ºå®š
                    final_target_folder_id = target_folder_id_by_criteria if target_folder_id_by_criteria else best_folder_id
                    
                    print(f"\n    ğŸ“Œ æœ€çµ‚æŒ¿å…¥å…ˆãƒ•ã‚©ãƒ«ãƒ€: ID={final_target_folder_id}")
                    
                    # ç”»åƒã‚’ãƒ•ã‚©ãƒ«ãƒ€ã«æŒ¿å…¥
                    # imagesãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰image_pathã‚’å–å¾—
                    path_result, _ = action_queries.get_image_name_by_id(connect_session, image_id)
                    image_path = path_result.mappings().first()['name']
                    
                    insert_result = result_manager.insert_image_to_leaf_folder(
                        clustering_id=clustering_id,
                        image_path=image_path,
                        target_folder_id=final_target_folder_id
                    )

                    if insert_result['success']:
                        print(f"    âœ… ãƒ•ã‚©ãƒ«ãƒ€ã«ç”»åƒã‚’æŒ¿å…¥ã—ã¾ã—ãŸ")

                        # user_image_clustering_statesã‚’æ›´æ–°
                        _, _ = action_queries.update_user_image_state_for_image(connect_session, user_id, image_id, new_count)

                        # ãƒ•ã‚©ãƒ«ãƒ€ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å†è¨ˆç®—ï¼ˆæ–°ã—ã„ç”»åƒã‚’è¿½åŠ ã—ãŸãŸã‚ï¼‰
                        print(f"    ğŸ”„ ãƒ•ã‚©ãƒ«ãƒ€åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å†è¨ˆç®—ä¸­...")
                        try:
                            # resultå†…ã§ãƒ•ã‚©ãƒ«ãƒ€IDã‚’æ¢ç´¢ã—ã¦dataã‚’å–å¾—
                            folder_data_result = result_manager.get_folder_data_from_result(final_target_folder_id)
                            if folder_data_result['success']:
                                folder_data = folder_data_result['data']
                                clustering_ids = list(folder_data.keys())

                                # æ–‡ç« åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®å†è¨ˆç®—
                                sentence_ids = []
                                for cid in clustering_ids:
                                    sent_result, _ = action_queries.get_chromadb_sentence_id_by_clustering_id(connect_session, cid, project_id)
                                    if sent_result:
                                        sent_mapping = sent_result.mappings().first()
                                        if sent_mapping:
                                            sentence_ids.append(sent_mapping['chromadb_sentence_id'])

                                if len(sentence_ids) > 0:
                                    updated_sentence_data = sentence_name_db.get_data_by_ids(sentence_ids)
                                    updated_sentence_embeddings = updated_sentence_data['embeddings']
                                    folder_sentence_embeddings[final_target_folder_id] = np.mean(updated_sentence_embeddings, axis=0)
                                    print(f"    âœ… ãƒ•ã‚©ãƒ«ãƒ€æ–‡ç« åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å†è¨ˆç®—å®Œäº† ({len(sentence_ids)}å€‹ã®æ–‡ç« )")
                                
                                # ç”»åƒåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®å†è¨ˆç®—
                                image_ids = []
                                for cid in clustering_ids:
                                    img_result, _ = action_queries.get_chromadb_image_id_by_clustering_id(connect_session, cid, project_id)
                                    if img_result:
                                        img_mapping = img_result.mappings().first()
                                        if img_mapping:
                                            image_ids.append(img_mapping['chromadb_image_id'])

                                if len(image_ids) > 0:
                                    updated_image_data = image_db.get_data_by_ids(image_ids)
                                    updated_image_embeddings = updated_image_data['embeddings']
                                    folder_image_embeddings[final_target_folder_id] = np.mean(updated_image_embeddings, axis=0)
                                    print(f"    âœ… ãƒ•ã‚©ãƒ«ãƒ€ç”»åƒåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å†è¨ˆç®—å®Œäº† ({len(image_ids)}å€‹ã®ç”»åƒ)")
                            else:
                                print(f"    âš ï¸ ãƒ•ã‚©ãƒ«ãƒ€ãƒ‡ãƒ¼ã‚¿ã®å†å–å¾—å¤±æ•—: {folder_data_result.get('error', 'Unknown error')}")
                        except Exception as e:
                            print(f"    âš ï¸ ãƒ•ã‚©ãƒ«ãƒ€åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«å†è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                            traceback.print_exc()
                    else:
                        print(f"    âŒ ç”»åƒæŒ¿å…¥ã‚¨ãƒ©ãƒ¼: {insert_result.get('error', 'Unknown error')}")
                        
                except Exception as img_error:
                    print(f"    âŒ ç”»åƒå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {img_error}")
                    continue
            
            # project_membershipsã®executed_clustering_countã‚’æ›´æ–°
            _, _ = action_queries.update_project_executed_clustering_count(connect_session, user_id, project_id, new_count)
            
            # æœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒãŒæ®‹ã£ã¦ã„ã‚‹ã‹ç¢ºèª
            check_result, _ = action_queries.get_unclustered_count_for_project(connect_session, user_id, project_id)
            remaining_unclustered = check_result.mappings().first()['unclustered_count']
            
            print(f"\nğŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Œäº†å¾Œã®çŠ¶æ…‹ç¢ºèª:")
            print(f"   æ®‹ã‚Šã®æœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒæ•°: {remaining_unclustered}")
            
            # æœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒãŒæ®‹ã£ã¦ã„ã‚Œã°2ï¼ˆå®Ÿè¡Œå¯èƒ½ï¼‰ã€ãªã‘ã‚Œã°0ï¼ˆå®Ÿè¡Œä¸å¯èƒ½ï¼‰
            new_state = 2 if remaining_unclustered > 0 else 0
            state_description = "å®Ÿè¡Œå¯èƒ½" if new_state == 2 else "å®Ÿè¡Œä¸å¯èƒ½"
            
            _, _ = action_queries.update_continuous_state(connect_session, user_id, project_id, new_state)
            
            print(f"   continuous_clustering_state: {new_state} ({state_description})")
            print(f"\nâœ… ç¶™ç¶šçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†å®Œäº†")
            print(f"   å‡¦ç†ã—ãŸç”»åƒæ•°: {len(unclustered_rows)}")
            print(f"   æ–°ã—ã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å›æ•°: {new_count}")
            
        except Exception as e:
            print(f"âŒ ç¶™ç¶šçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
            traceback.print_exc()
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚æœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒã®æœ‰ç„¡ã‚’ç¢ºèªã—ã¦çŠ¶æ…‹ã‚’è¨­å®š
            try:
                check_result, _ = action_queries.get_unclustered_count_for_project(connect_session, user_id, project_id)
                remaining_unclustered = check_result.mappings().first()['unclustered_count']
                
                # æœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒãŒæ®‹ã£ã¦ã„ã‚Œã°2ï¼ˆå®Ÿè¡Œå¯èƒ½ï¼‰ã€ãªã‘ã‚Œã°0ï¼ˆå®Ÿè¡Œä¸å¯èƒ½ï¼‰
                new_state = 2 if remaining_unclustered > 0 else 0
                
                _, _ = action_queries.update_continuous_state(connect_session, user_id, project_id, new_state)
                print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼å¾Œã®çŠ¶æ…‹æ›´æ–°: continuous_clustering_state = {new_state} (æœªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”»åƒ: {remaining_unclustered})")
            except Exception as state_error:
                print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼å¾Œã®çŠ¶æ…‹æ›´æ–°ã«å¤±æ•—: {state_error}")
                
    # continuous_clustering_stateã‚’1ï¼ˆå®Ÿè¡Œä¸­ï¼‰ã«æ›´æ–°
    _, _ = action_queries.update_continuous_state(connect_session, user_id, project_id, 1)
    
    # éåŒæœŸå®Ÿè¡Œ
    background_tasks.add_task(run_continuous_clustering, rows, project_id, user_id, mongo_result_id)
    
    return JSONResponse(
        status_code=status.HTTP_200_OK,
        content={
            "message": "continuous clustering started in background",
            "data": {
                "project_id": project_id,
                "user_id": user_id,
                "unclustered_image_count": len(rows)
            }
        }
    )


@action_endpoint.put("/action/clustering/move/{mongo_result_id}", tags=["action"], description="ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã®ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã‚’å¤‰æ›´ã™ã‚‹")
def move_clustering_items(
    mongo_result_id: str,
    source_type: str = Query(..., description="ç§»å‹•ã™ã‚‹ã‚½ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒ—: 'folders' ã¾ãŸã¯ 'images'"),
    sources: List[str] = Query(..., description="ç§»å‹•ã™ã‚‹è¦ç´ ã®åå‰ã®é…åˆ—"),
    destination_folder: str = Query(..., description="ç§»å‹•å…ˆã®ãƒ•ã‚©ãƒ«ãƒ€å")
):
    """
    æŒ‡å®šã—ãŸmongo_result_idã«ç´ã¤ã„ãŸjsonã‚’ç·¨é›†ã—ã¦ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã‚’ä½œã‚Šæ›¿ãˆã¦nosqlãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚³ãƒŸãƒƒãƒˆã™ã‚‹
    
    Args:
        mongo_result_id: MongoDBã®çµæœID
        source_type: ç§»å‹•ã™ã‚‹ã‚½ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒ— ("folders" ã¾ãŸã¯ "images")
        sources: ç§»å‹•ã™ã‚‹è¦ç´ ã®åå‰ã®é…åˆ—
        destination_folder: ç§»å‹•å…ˆã®ãƒ•ã‚©ãƒ«ãƒ€å
    """
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¤œè¨¼
    if source_type not in ["folders", "images"]:
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"message": "source_type must be 'folders' or 'images'", "data": None}
        )
    
    if not sources or len(sources) == 0:
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"message": "sources must not be empty", "data": None}
        )
    
    if not destination_folder:
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"message": "destination_folder is required", "data": None}
        )

    result_manager = ResultManager(mongo_result_id)
    
    if source_type == "folders":
        try:
            result_manager.move_folder_node(sources, destination_folder)
        except Exception as e:
            return JSONResponse(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                content={"message": f"ãƒ•ã‚©ãƒ«ãƒ€ç§»å‹•ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}", "data": None}
            )
            
    elif source_type == "images":
        try:
            for source in sources:
                result_manager.move_file_node(source, destination_folder)
        except Exception as e:
            return JSONResponse(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                content={"message": f"ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}", "data": None}
            )
            
    else:
        return JSONResponse(
            status_code=status.HTTP_400_BAD_REQUEST,
            content={"message": "source_type must be 'folders' or 'images'", "data": None}
        )
    


    return JSONResponse(
        status_code=status.HTTP_200_OK,
        content={"message": "success", "data": None}
    )
    

@action_endpoint.delete("/action/folders/{mongo_result_id}", tags=["action"], description="æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤")
async def delete_folders(mongo_result_id: str, sources: List[str] = Query(...)):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€IDãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€çµæœã‹ã‚‰å‰Šé™¤ã™ã‚‹
    
    Args:
        mongo_result_id (str): MongoDBã®çµæœID
        sources (List[str]): å‰Šé™¤å¯¾è±¡ã®ãƒ•ã‚©ãƒ«ãƒ€IDãƒªã‚¹ãƒˆ
    
    Returns:
        JSONResponse: å‰Šé™¤å‡¦ç†ã®çµæœ
    """
    try:
        # å…¥åŠ›ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if not mongo_result_id or not mongo_result_id.strip():
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "mongo_result_id is required"}
            )
        
        if not sources or len(sources) == 0:
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "sources parameter is required and must contain at least one folder ID"}
            )
        
        print(f"ğŸ—‚ï¸ delete_folderså‘¼ã³å‡ºã—: mongo_result_id={mongo_result_id}")
        print(f"ğŸ“‹ å—ã‘å–ã£ãŸãƒ•ã‚©ãƒ«ãƒ€IDãƒªã‚¹ãƒˆ (sources): {sources}")
        print(f"ğŸ“Š å‰Šé™¤å¯¾è±¡ãƒ•ã‚©ãƒ«ãƒ€æ•°: {len(sources)}")
        
        # ResultManagerã‚’åˆæœŸåŒ–
        result_manager = ResultManager(mongo_result_id)
        
        # ãƒ•ã‚©ãƒ«ãƒ€ã‚’çµæœã‹ã‚‰å‰Šé™¤
        is_success = result_manager.remove_folders_from_result(sources)
        
        if is_success:
            print(f"âœ… ãƒ•ã‚©ãƒ«ãƒ€å‰Šé™¤æˆåŠŸ: {len(sources)}å€‹ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            return JSONResponse(
                status_code=status.HTTP_200_OK,
                content={
                    "message": "success", 
                    "data": {
                        "deleted_folder_count": len(sources),
                        "deleted_folders": sources
                    }
                }
            )
        else:
            print(f"âŒ ãƒ•ã‚©ãƒ«ãƒ€å‰Šé™¤å¤±æ•—: remove_folders_from_result returned False")
            return JSONResponse(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                content={
                    "message": "Failed to delete folders from result",
                    "data": {
                        "mongo_result_id": mongo_result_id,
                        "attempted_folder_ids": sources,
                        "error": "remove_folders_from_result operation failed"
                    }
                }
            )
            
    except Exception as e:
        print(f"âŒ delete_folderså‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
        return JSONResponse(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                content={
                    "message": "Internal server error occurred during folder deletion",
                    "data": {
                        "mongo_result_id": mongo_result_id,
                        "attempted_folder_ids": sources,
                        "error": str(e)
                    }
                }
            )


@action_endpoint.get("/action/clustering/node/{mongo_result_id}/{node_id}", tags=["action"], description="æŒ‡å®šã•ã‚ŒãŸãƒãƒ¼ãƒ‰ã®æƒ…å ±ã‚’å–å¾—ã™ã‚‹")
async def get_node_info(mongo_result_id: str, node_id: str):
    """
    æŒ‡å®šã•ã‚ŒãŸnode_idã®ãƒãƒ¼ãƒ‰æƒ…å ±ã‚’all_nodesã‹ã‚‰å–å¾—ã™ã‚‹
    
    Args:
        mongo_result_id (str): MongoDBã®çµæœIDï¼ˆãƒ‘ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
        node_id (str): ãƒãƒ¼ãƒ‰IDï¼ˆãƒ‘ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
        
    Returns:
        JSONResponse: ãƒãƒ¼ãƒ‰æƒ…å ±
    """
    try:
        # å…¥åŠ›ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if not mongo_result_id or not mongo_result_id.strip():
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "mongo_result_id is required"}
            )
        
        if not node_id or not node_id.strip():
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "node_id is required"}
            )
        
        print(f"ğŸ” get_node_infoå‘¼ã³å‡ºã—: mongo_result_id={mongo_result_id}, node_id={node_id}")
        
        # ResultManagerã‚’åˆæœŸåŒ–ã—ã¦ãƒãƒ¼ãƒ‰æƒ…å ±ã‚’å–å¾—
        result_manager = ResultManager(mongo_result_id)
        node_data = result_manager.get_node_info(node_id=node_id)
        
        # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯HTTPExceptionã‚’ç™ºç”Ÿ
        if not node_data["success"]:
            if "not found" in node_data.get("error", "").lower():
                return JSONResponse(
                    status_code=status.HTTP_404_NOT_FOUND,
                    content={
                        "message": node_data["error"],
                        "data": {
                            "mongo_result_id": mongo_result_id,
                            "node_id": node_id
                        }
                    }
                )
            else:
                return JSONResponse(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    content={
                        "message": node_data["error"],
                        "data": {
                            "mongo_result_id": mongo_result_id,
                            "node_id": node_id
                        }
                    }
                )
        
        
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "message": "success",
                "data": node_data["data"]  # all_nodesã®å€¤ã®ã¿ã‚’è¿”ã™
            }
        )
        
    except Exception as e:
        print(f"âŒ get_node_infoå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={
                "message": "Internal server error occurred during node retrieval",
                "data": {
                    "mongo_result_id": mongo_result_id,
                    "node_id": node_id,
                    "error": str(e)
                }
            }
        )


@action_endpoint.get("/action/clustering/captions/{mongo_result_id}", tags=["action"], description="æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€å†…ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°IDã«å¯¾å¿œã™ã‚‹ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—ã™ã‚‹")
async def get_captions_for_folder(mongo_result_id: str, folder_id: str = Query(..., description="ãƒ•ã‚©ãƒ«ãƒ€ã® node_id")):
    """
    mongo_result_id (ãƒ‘ã‚¹) ã¨ folder_id (ã‚¯ã‚¨ãƒª) ã‚’å—ã‘å–ã‚Šã€ãã®ãƒ•ã‚©ãƒ«ãƒ€ã«å«ã¾ã‚Œã‚‹ç”»åƒã® clustering_id ã‚’å–å¾—ã—ã€
    images ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ caption ã‚’å–å¾—ã—ã¦ {clustering_id: caption} ã®ãƒãƒƒãƒ—ã‚’è¿”ã™ã€‚
    """
    try:
        if not mongo_result_id or not mongo_result_id.strip():
            return JSONResponse(status_code=status.HTTP_400_BAD_REQUEST, content={"message": "mongo_result_id is required"})

        if not folder_id or not folder_id.strip():
            return JSONResponse(status_code=status.HTTP_400_BAD_REQUEST, content={"message": "folder_id is required"})

        # ResultManagerã‚’åˆæœŸåŒ–ã—ã¦ãƒ•ã‚©ãƒ«ãƒ€å†…ã®clustering_idä¸€è¦§ã‚’å–å¾—
        result_manager = ResultManager(mongo_result_id)
        clustering_ids_result = result_manager.get_leaf_folder_image_clustering_ids(folder_id)

        # debug logs for tracing
        print(f"ğŸ” get_captions_for_folder: clustering_ids_result={clustering_ids_result}")

        if not clustering_ids_result.get('success', False):
            return JSONResponse(status_code=status.HTTP_404_NOT_FOUND, content={"message": clustering_ids_result.get('error', 'folder not found'), "data": None})

        clustering_ids = clustering_ids_result.get('data', [])
        print(f"ğŸ” get_captions_for_folder: clustering_ids (count={len(clustering_ids)}): {clustering_ids[:50]}")

        captions_map = {}

        # DBã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¦1ã¤ãšã¤captionã‚’å–å¾—ï¼ˆå°†æ¥çš„ã«INã‚¯ã‚¨ãƒªã¸æœ€é©åŒ–å¯ï¼‰
        connect_session = create_connect_session()
        if connect_session is None:
            return JSONResponse(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, content={"message": "failed to connect to database"})

        for cid in clustering_ids:
            try:
                res, _ = images_queries.select_caption_by_clustering_id(connect_session, cid)
                if res is None:
                    captions_map[cid] = None
                    continue
                mapping = res.mappings().first()
                captions_map[cid] = mapping['caption'] if mapping and 'caption' in mapping else None
            except Exception as q_e:
                # å€‹åˆ¥å–å¾—ã«å¤±æ•—ã—ã¦ã‚‚ä»–ã®çµæœã¯è¿”ã™
                captions_map[cid] = None

        return JSONResponse(status_code=status.HTTP_200_OK, content={"message": "success", "data": {"folder_id": folder_id, "captions": captions_map}})

    except Exception as e:
        print(f"âŒ get_captions_for_folderã‚¨ãƒ©ãƒ¼: {e}")
        return JSONResponse(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, content={"message": f"Internal server error: {str(e)}"})


@action_endpoint.put("/action/folders/{mongo_result_id}/{node_id}", tags=["action"], description="ãƒ•ã‚©ãƒ«ãƒ€ã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«ã®åå‰ã‚’å¤‰æ›´")
async def rename_folder_or_file(
    mongo_result_id: str,
    node_id: str,
    name: str = Query(None, description="æ–°ã—ã„åå‰"),
    is_leaf: bool = Query(None, description="ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã‹ã©ã†ã‹")
):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒãƒ¼ãƒ‰ã®åå‰ã‚’å¤‰æ›´ã™ã‚‹
    
    Args:
        mongo_result_id (str): MongoDBã®çµæœID
        node_id (str): å¤‰æ›´å¯¾è±¡ã®ãƒãƒ¼ãƒ‰ID
        name (str, optional): æ–°ã—ã„åå‰
        is_leaf (bool, optional): ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã‹ã©ã†ã‹
    
    Returns:
        JSONResponse: åå‰å¤‰æ›´å‡¦ç†ã®çµæœ
    """
    try:
        # å…¥åŠ›ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if not mongo_result_id or not mongo_result_id.strip():
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "mongo_result_id is required"}
            )
        
        if not node_id or not node_id.strip():
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "node_id is required"}
            )
        
        # nameã¨is_leafã®ä¸¡æ–¹ãŒNoneã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼
        if name is None and is_leaf is None:
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "At least one of 'name' or 'is_leaf' parameters is required"}
            )
        
        # nameãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ç©ºæ–‡å­—ãƒã‚§ãƒƒã‚¯
        if name is not None and not name.strip():
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "name parameter must not be empty when provided"}
            )
        
        print(f"ğŸ·ï¸ rename_folder_or_fileå‘¼ã³å‡ºã—: mongo_result_id={mongo_result_id}, node_id={node_id}")
        print(f"ğŸ“ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: name={name}, is_leaf={is_leaf}")
        
        # ResultManagerã‚’åˆæœŸåŒ–
        result_manager = ResultManager(mongo_result_id)
        
        # åå‰ãƒ»is_leafå¤‰æ›´å‡¦ç†
        update_result = result_manager.rename_node(
            node_id=node_id, 
            new_name=name.strip() if name is not None else None, 
            is_leaf=is_leaf
        )
        
        print(f"âœ… æ›´æ–°çµæœ: {update_result}")
        
        if update_result.get("success", False):
            return JSONResponse(
                status_code=status.HTTP_200_OK,
                content={
                    "message": "success",
                    "data": {
                        "node_id": node_id,
                        "updated_fields": update_result.get("updated_fields", {}),
                        "is_leaf": is_leaf
                    }
                }
            )
        else:
            return JSONResponse(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                content={
                    "message": "Failed to update node",
                    "data": {
                        "mongo_result_id": mongo_result_id,
                        "node_id": node_id,
                        "attempted_name": name,
                        "attempted_is_leaf": is_leaf,
                        "error": update_result.get("error", "Unknown error")
                    }
                }
            )
            
    except Exception as e:
        print(f"âŒ rename_folder_or_fileå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={
                "message": "Internal server error occurred during rename operation",
                "data": {
                    "mongo_result_id": mongo_result_id,
                    "node_id": node_id,
                    "attempted_name": name,
                    "error": str(e)
                }
            }
        )


@action_endpoint.get("/action/clustering/download/{project_id}", tags=["action"], description="åˆ†é¡çµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹")
async def download_classification_result(
    project_id: int,
    user_id: int = Query(..., description="ãƒ¦ãƒ¼ã‚¶ãƒ¼ID")
):
    """
    åˆ†é¡çµæœã‚’ZIPãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹
    
    ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹:
    - result.json: åˆ†é¡çµæœã®éšå±¤æ§‹é€ 
    - all_nodes.json: å…¨ãƒãƒ¼ãƒ‰ã®æƒ…å ±
    - images/: åˆ†é¡çµæœã«åŸºã¥ã„ãŸãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«
    
    Args:
        project_id: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
        user_id: ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        
    Returns:
        FileResponse: ZIPãƒ•ã‚¡ã‚¤ãƒ«
    """
    connect_session = create_connect_session()
    
    if connect_session is None:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={"message": "failed to connect to database", "data": None}
        )
    
    try:
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã¨mongo_result_idã‚’å–å¾—
        query_text = f"""
            SELECT 
                p.name as project_name,
                p.original_images_folder_path,
                pm.mongo_result_id,
                pm.init_clustering_state
            FROM projects p
            JOIN project_memberships pm ON p.id = pm.project_id
            WHERE p.id = {project_id} AND pm.user_id = {user_id};
        """
        
        result, _ = action_queries.get_project_info_and_mongo(connect_session, project_id, user_id)
        result_mapping = result.mappings().first()
        
        if result_mapping is None:
            return JSONResponse(
                status_code=status.HTTP_404_NOT_FOUND,
                content={"message": "project or membership not found", "data": None}
            )
        
        project_name = result_mapping['project_name']
        original_images_folder_path = result_mapping['original_images_folder_path']
        mongo_result_id = result_mapping['mongo_result_id']
        init_clustering_state = result_mapping['init_clustering_state']
        
        # åˆæœŸã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãŒå®Œäº†ã—ã¦ã„ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
        if init_clustering_state != INIT_CLUSTERING_STATUS.FINISHED:
            return JSONResponse(
                status_code=status.HTTP_400_BAD_REQUEST,
                content={"message": "clustering not completed yet", "data": None}
            )
        
        print(f"ğŸ“¦ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†é–‹å§‹:")
        print(f"   ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {project_name}")
        print(f"   ãƒ¦ãƒ¼ã‚¶ãƒ¼ID: {user_id}")
        print(f"   mongo_result_id: {mongo_result_id}")
        
        # ResultManagerã‹ã‚‰åˆ†é¡çµæœãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        result_manager = ResultManager(mongo_result_id)
        export_data = result_manager.export_classification_data()
        
        if not export_data['success']:
            return JSONResponse(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                content={
                    "message": f"failed to export classification data: {export_data.get('error', 'Unknown error')}",
                    "data": None
                }
            )
        
        result_dict = export_data['result']
        all_nodes_dict = export_data['all_nodes']
        
        # ç”»åƒãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹
        source_images_path = Path(f"./{DEFAULT_IMAGE_PATH}/{original_images_folder_path}")
        
        if not source_images_path.exists():
            return JSONResponse(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                content={
                    "message": f"source images folder not found: {source_images_path}",
                    "data": None
                }
            )
        
        print(f"   ç”»åƒãƒ•ã‚©ãƒ«ãƒ€: {source_images_path}")
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ä½œæˆ
        try:
            zip_path = Utils.create_classification_download_package(
                result_dict=result_dict,
                all_nodes_dict=all_nodes_dict,
                source_images_path=source_images_path,
                project_name=project_name
            )
            
            print(f"   ZIPãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†: {zip_path}")
            
            # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            return FileResponse(
                path=str(zip_path),
                media_type='application/zip',
                filename=f"{project_name}.zip",
                headers={
                    "Content-Disposition": f'attachment; filename="{project_name}.zip"'
                }
            )
            
        except Exception as create_error:
            print(f"âŒ ZIPãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {create_error}")
            traceback.print_exc()
            return JSONResponse(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                content={
                    "message": f"failed to create download package: {str(create_error)}",
                    "data": None
                }
            )
        
    except Exception as e:
        print(f"âŒ download_classification_resultå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
        traceback.print_exc()
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={
                "message": f"Internal server error: {str(e)}",
                "data": None
            }
        )


@action_endpoint.get("/action/clustering/counts/{project_id}", tags=["action"], description="ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®ç”»åƒã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å›æ•°æƒ…å ±ã‚’å–å¾—ã™ã‚‹")
async def get_clustering_counts(
    project_id: int,
    user_id: int = Query(..., description="ãƒ¦ãƒ¼ã‚¶ãƒ¼ID")
):
    """
    ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®å…¨ç”»åƒã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å›æ•°æƒ…å ±ã‚’å–å¾—ã™ã‚‹
    
    Returns:
        {
            "available_counts": [0, 1, 2, ...],  # å®Ÿè¡Œã•ã‚ŒãŸå›æ•°ã®ãƒªã‚¹ãƒˆ
            "image_counts": {
                "clustering_id_1": 0,  # å„ç”»åƒã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å›æ•°
                "clustering_id_2": 1,
                ...
            }
        }
    """
    connect_session = create_connect_session()
    
    if connect_session is None:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={"message": "failed to connect to database", "data": None}
        )
    
    try:
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ¡ãƒ³ãƒãƒ¼ã‚·ãƒƒãƒ—ã‚’ç¢ºèªï¼ˆCOUNTã§å­˜åœ¨ç¢ºèªã™ã‚‹ã€‚
        # ä¸€éƒ¨ç’°å¢ƒã§ project_memberships ã« id ã‚«ãƒ©ãƒ ãŒç„¡ã„å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€å˜ç´”ãªå­˜åœ¨ç¢ºèªã‚’ä½¿ã†ï¼‰
        membership_result, _ = action_queries.membership_exists(connect_session, project_id, user_id)

        # execute_query ãŒå¤±æ•—ã—ã¦ None ã‚’è¿”ã™å ´åˆã‚’å®‰å…¨ã«æ‰±ã†
        if membership_result is None:
            return JSONResponse(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                content={"message": "failed to query project_memberships", "data": None}
            )

        membership_row = membership_result.mappings().first()
        if membership_row is None or membership_row.get('cnt', 0) == 0:
            return JSONResponse(
                status_code=status.HTTP_404_NOT_FOUND,
                content={"message": "project membership not found", "data": None}
            )
        
        result, _ = action_queries.get_image_counts_for_clustering_counts(connect_session, user_id, project_id)
        rows = result.mappings().all()

        # executed_clustering_count ã”ã¨ã« clustering_id ã®é…åˆ—ã‚’ä½œæˆ
        grouped_by_count: dict[str, list] = {}
        # clustering_id -> executed_clustering_count ã®è¾æ›¸
        image_counts: dict = {}
        available_counts_set = set()

        for row in rows:
            clustering_id = row.get('clustering_id')
            count = row.get('exec_count')

            # clustering_id ã¾ãŸã¯ count ãŒç„¡ã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if clustering_id is None or count is None:
                continue

            # image_counts ãƒãƒƒãƒ—
            image_counts[clustering_id] = int(count)

            # grouped map: key ã‚’æ–‡å­—åˆ—ã«ã—ã¦è¿”ã™ï¼ˆä¾‹: '0', '1', ...ï¼‰
            key = str(int(count))
            if key not in grouped_by_count:
                grouped_by_count[key] = []
            # é‡è¤‡ã‚’é¿ã‘ã¦è¿½åŠ 
            if clustering_id not in grouped_by_count[key]:
                grouped_by_count[key].append(clustering_id)

            available_counts_set.add(int(count))

        # åˆ©ç”¨å¯èƒ½ãªå›æ•°ã‚’ã‚½ãƒ¼ãƒˆã—ãŸãƒªã‚¹ãƒˆã«å¤‰æ›
        available_counts = sorted(list(available_counts_set))
        
        
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "message": "success",
                "data": {
                    "available_counts": available_counts,
                    "image_counts": image_counts,
                    "grouped_image_ids": grouped_by_count
                }
            }
        )
        
    except Exception as e:
        print(f"âŒ get_clustering_countså‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
        traceback.print_exc()
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={
                "message": f"Internal server error: {str(e)}",
                "data": None
            }
        )