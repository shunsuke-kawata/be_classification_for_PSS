"""
ç¶™ç¶šçš„éšå±¤åˆ†é¡ã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¯ãƒ©ã‚¹

å®Ÿè¡Œã”ã¨ã«ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã€æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã«ä¿å­˜ã™ã‚‹ã€‚
ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ : output/{project_name}/{user_name}/{yyyymmddhhmmss}/{image_name}.txt
"""

import os
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from clustering.clustering_metrics import ClusteringMetrics


class ContinuousClusteringReporter:
    """ç¶™ç¶šçš„éšå±¤åˆ†é¡ã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, project_name: str, user_name: str, output_base_dir: str = "output"):
        """
        Args:
            project_name: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå
            user_name: ãƒ¦ãƒ¼ã‚¶ãƒ¼å
            output_base_dir: å‡ºåŠ›ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: "output"ï¼‰
        """
        self.project_name = self._sanitize_dirname(project_name)
        self.user_name = self._sanitize_dirname(user_name)
        self.output_base_dir = output_base_dir
        self.timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’æ§‹ç¯‰
        self.report_dir = Path(output_base_dir) / self.project_name / self.user_name / self.timestamp
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–
        self.metrics_calculator = ClusteringMetrics()
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        self._create_directories()
        
    def _sanitize_dirname(self, name: str) -> str:
        """
        ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã¨ã—ã¦ä½¿ç”¨å¯èƒ½ãªæ–‡å­—åˆ—ã«å¤‰æ›
        
        Args:
            name: å…ƒã®åå‰
            
        Returns:
            ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã•ã‚ŒãŸåå‰
        """
        # ä½¿ç”¨ã§ããªã„æ–‡å­—ã‚’ç½®æ›
        invalid_chars = ['/', '\\', ':', '*', '?', '"', '<', '>', '|']
        sanitized = name
        for char in invalid_chars:
            sanitized = sanitized.replace(char, '_')
        return sanitized
    
    def _create_directories(self):
        """ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
        try:
            self.report_dir.mkdir(parents=True, exist_ok=True)
            print(f"ğŸ“ ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ: {self.report_dir}")
        except Exception as e:
            print(f"âŒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def generate_image_report(self, report_data: Dict[str, Any]) -> str:
        """
        ç”»åƒã”ã¨ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
        
        Args:
            report_data: ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆè¾æ›¸å½¢å¼ï¼‰
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        # ãƒ‡ãƒãƒƒã‚°: ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ¼ã‚’å‡ºåŠ›
        print(f"  ğŸ” ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚­ãƒ¼: {list(report_data.keys())}")
        
        image_name = report_data.get('image_name', 'unknown')
        safe_image_name = self._sanitize_dirname(image_name)
        
        # æ‹¡å¼µå­ã‚’é™¤å»ã—ã¦txtã«ç½®æ›
        base_name = os.path.splitext(safe_image_name)[0]
        report_filename = f"{base_name}.txt"
        report_path = self.report_dir / report_filename
        
        print(f"  ğŸ“ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹: {report_filename}")
        
        # ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹ã‚’ç”Ÿæˆ
        report_content = self._format_report(report_data)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report_content)
            print(f"  ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ: {report_path}")
            return str(report_path)
        except Exception as e:
            print(f"  âŒ ãƒ¬ãƒãƒ¼ãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _format_report(self, data: Dict[str, Any]) -> str:
        """
        ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        
        Args:
            data: ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿
            
        Returns:
            ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆæ–‡å­—åˆ—
        """
        lines = []
        lines.append("=" * 80)
        lines.append("ç¶™ç¶šçš„éšå±¤åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ")
        lines.append("=" * 80)
        lines.append("")
        
        # å®Ÿè¡Œæƒ…å ±
        lines.append("ã€å®Ÿè¡Œæƒ…å ±ã€‘")
        lines.append(f"  å®Ÿè¡Œæ—¥æ™‚: {data.get('execution_time', 'N/A')}")
        lines.append(f"  ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {data.get('project_name', 'N/A')}")
        lines.append(f"  ãƒ¦ãƒ¼ã‚¶ãƒ¼: {data.get('user_name', 'N/A')}")
        lines.append(f"  ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å›æ•°: {data.get('clustering_count', 'N/A')}")
        lines.append("")
        
        # ç”»åƒæƒ…å ±
        lines.append("ã€ç”»åƒæƒ…å ±ã€‘")
        lines.append(f"  ç”»åƒID: {data.get('image_id', 'N/A')}")
        lines.append(f"  ãƒ•ã‚¡ã‚¤ãƒ«å: {data.get('image_name', 'N/A')}")
        lines.append(f"  Clustering ID: {data.get('clustering_id', 'N/A')}")
        lines.append(f"  ChromaDB Sentence ID: {data.get('chromadb_sentence_id', 'N/A')}")
        lines.append(f"  ChromaDB Image ID: {data.get('chromadb_image_id', 'N/A')}")
        lines.append("")
        
        # ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³æƒ…å ±
        caption = data.get('caption', 'N/A')
        lines.append("ã€ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã€‘")
        lines.append(f"  {caption}")
        lines.append("")
        
        # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«æƒ…å ±
        lines.append("ã€åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«æƒ…å ±ã€‘")
        lines.append(f"  æ–‡ç« ãƒ™ã‚¯ãƒˆãƒ«å–å¾—: {'æˆåŠŸ' if data.get('sentence_embedding_available', False) else 'å¤±æ•—'}")
        lines.append(f"  ç”»åƒãƒ™ã‚¯ãƒˆãƒ«å–å¾—: {'æˆåŠŸ' if data.get('image_embedding_available', False) else 'å¤±æ•—'}")
        lines.append("")
        
        # é¡ä¼¼åº¦è¨ˆç®—çµæœ
        lines.append("ã€é¡ä¼¼åº¦è¨ˆç®—çµæœã€‘")
        lines.append(f"  å¯¾è±¡ãƒ•ã‚©ãƒ«ãƒ€æ•°: {data.get('total_folders_checked', 0)}")
        lines.append("")
        
        # ä¸Šä½ã®é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢
        similarity_scores = data.get('similarity_scores', [])
        if similarity_scores:
            lines.append("  ä¸Šä½ã®é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢:")
            for i, score_info in enumerate(similarity_scores[:10], 1):  # ä¸Šä½10ä»¶
                folder_name = score_info.get('folder_name', 'Unknown')
                folder_id = score_info.get('folder_id', 'N/A')
                similarity = score_info.get('similarity', 0.0)
                sim_type = score_info.get('type', 'N/A')
                lines.append(f"    [{i}] {folder_name} (ID: {folder_id})")
                lines.append(f"        é¡ä¼¼åº¦: {similarity:.6f} (ã‚¿ã‚¤ãƒ—: {sim_type})")
            lines.append("")
        
        # æœ€çµ‚çš„ãªåˆ†é¡å…ˆ
        lines.append("ã€åˆ†é¡çµæœã€‘")
        final_folder_name = data.get('final_folder_name', 'N/A')
        final_folder_id = data.get('final_folder_id', 'N/A')
        final_similarity = data.get('final_similarity', 0.0)
        final_similarity_type = data.get('final_similarity_type', 'N/A')
        
        lines.append(f"  åˆ†é¡å…ˆãƒ•ã‚©ãƒ«ãƒ€: {final_folder_name}")
        lines.append(f"  ãƒ•ã‚©ãƒ«ãƒ€ID: {final_folder_id}")
        lines.append(f"  æœ€çµ‚é¡ä¼¼åº¦: {final_similarity:.6f}")
        lines.append(f"  é¡ä¼¼åº¦ã‚¿ã‚¤ãƒ—: {final_similarity_type}")
        lines.append("")
        
        # ãƒ•ã‚©ãƒ«ãƒ€å¹³å‡ã¨ã®é¡ä¼¼åº¦ï¼ˆæ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã«è¿½åŠ ã•ã‚ŒãŸå ´åˆï¼‰
        if 'folder_average_sentence_similarity' in data or 'folder_average_image_similarity' in data:
            lines.append("ã€ãƒ•ã‚©ãƒ«ãƒ€å¹³å‡ã¨ã®é¡ä¼¼åº¦ã€‘")
            lines.append("  â€»æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã«è¿½åŠ ã•ã‚ŒãŸãŸã‚ã€ãƒ•ã‚©ãƒ«ãƒ€å†…ç”»åƒã®å¹³å‡ãƒ™ã‚¯ãƒˆãƒ«ã¨ã®é¡ä¼¼åº¦")
            if 'folder_average_sentence_similarity' in data:
                sent_sim = data['folder_average_sentence_similarity']
                lines.append(f"  æ–‡ç« ç‰¹å¾´é‡ã®é¡ä¼¼åº¦: {sent_sim:.6f}")
            if 'folder_average_image_similarity' in data:
                img_sim = data['folder_average_image_similarity']
                lines.append(f"  ç”»åƒç‰¹å¾´é‡ã®é¡ä¼¼åº¦: {img_sim:.6f}")
            lines.append("")
        
        # æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆæƒ…å ±
        if data.get('new_folder_created', False):
            lines.append("ã€æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆã€‘")
            lines.append(f"  âœ“ æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¾ã—ãŸ")
            lines.append(f"  ãƒ•ã‚©ãƒ«ãƒ€å: {data.get('new_folder_name', 'N/A')}")
            lines.append(f"  ãƒ•ã‚©ãƒ«ãƒ€ID: {data.get('new_folder_id', 'N/A')}")
            lines.append(f"  ä½œæˆç†ç”±: é¡ä¼¼åº¦ãŒé–¾å€¤ {data.get('similarity_threshold', 0.4)} ã‚’ä¸‹å›ã£ãŸãŸã‚")
            lines.append("")
        
        # åˆ†é¡åŸºæº–ã«ã‚ˆã‚‹å†åˆ¤å®šæƒ…å ±
        criteria_used = data.get('classification_criteria_used', False)
        if criteria_used:
            lines.append("ã€åˆ†é¡åŸºæº–ã«ã‚ˆã‚‹å†åˆ¤å®šã€‘")
            lines.append(f"  å†åˆ¤å®šå®Ÿè¡Œ: ã¯ã„")
            
            classification_words = data.get('classification_words_found', [])
            if classification_words:
                lines.append(f"  æ¤œå‡ºã•ã‚ŒãŸåˆ†é¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:")
                for word_info in classification_words:
                    word = word_info.get('word', 'N/A')
                    count = word_info.get('count', 0)
                    target_folder = word_info.get('target_folder', 'N/A')
                    lines.append(f"    - '{word}' (å‡ºç¾å›æ•°: {count}, å¯¾è±¡ãƒ•ã‚©ãƒ«ãƒ€: {target_folder})")
            
            override_folder = data.get('criteria_target_folder_name', None)
            if override_folder:
                lines.append(f"  å†åˆ¤å®šçµæœãƒ•ã‚©ãƒ«ãƒ€: {override_folder}")
            lines.append("")
        
        # å…„å¼Ÿãƒ•ã‚©ãƒ«ãƒ€ã®TF-IDFã‚¹ã‚³ã‚¢è¡¨ï¼ˆåˆ†é¡åŸºæº–å‡¦ç†ãŒå®Ÿè¡Œã•ã‚ŒãŸå ´åˆï¼‰
        if data.get('classification_criteria_process_executed', False):
            sibling_tfidf = data.get('sibling_folder_tfidf_scores', {})
            if sibling_tfidf:
                lines.append("ã€å…„å¼Ÿãƒ•ã‚©ãƒ«ãƒ€ã®ç‰¹å¾´çš„å˜èªï¼ˆTF-IDFã‚¹ã‚³ã‚¢è¡¨ï¼‰ã€‘")
                lines.append("  â€»å„ãƒ•ã‚©ãƒ«ãƒ€ã‚’æœ€ã‚‚ä»£è¡¨ã™ã‚‹å˜èªã¨ãã®ã‚¹ã‚³ã‚¢")
                lines.append("")
                
                # ãƒ•ã‚©ãƒ«ãƒ€ã”ã¨ã«ã‚¹ã‚³ã‚¢è¡¨ã‚’å‡ºåŠ›
                for folder_id, folder_info in sibling_tfidf.items():
                    folder_name = folder_info.get('folder_name', 'Unknown')
                    unique_words = folder_info.get('unique_words', [])
                    
                    lines.append(f"  ğŸ“ {folder_name} (ID: {folder_id})")
                    lines.append(f"     é †ä½ | å˜èª              | ç·åˆ  | ä»£è¡¨æ€§ | è­˜åˆ¥æ€§ | TF     | é›†ä¸­åº¦ | ä¸€è²«æ€§ | IDF  ")
                    lines.append(f"     " + "-" * 95)
                    
                    for idx, word_data in enumerate(unique_words[:10], 1):  # ä¸Šä½10å€‹
                        word = word_data.get('word', '')
                        score = float(word_data.get('score', 0.0))
                        score_repr = float(word_data.get('score_repr', 0.0))
                        score_dist = float(word_data.get('score_dist', 0.0))
                        tf = float(word_data.get('tf', 0.0))
                        concentration = float(word_data.get('concentration', 0.0))
                        consistency = float(word_data.get('consistency', 0.0))
                        base_idf = float(word_data.get('base_idf', 0.0))
                        
                        lines.append(f"     {idx:2d}   | {word:16s} | {score:5.1f} | {score_repr:6.1f} | {score_dist:6.1f} | {tf:6.4f} | {concentration:6.4f} | {consistency:6.4f} | {base_idf:4.2f}")
                    
                    lines.append("")
                    lines.append(f"     â€» ç·åˆã‚¹ã‚³ã‚¢ = 0.7 Ã— ä»£è¡¨æ€§ + 0.3 Ã— è­˜åˆ¥æ€§")
                    lines.append(f"     â€» ä»£è¡¨æ€§ = TF Ã— é›†ä¸­åº¦ Ã— âˆšä¸€è²«æ€§ Ã— 1000: ãƒ•ã‚©ãƒ«ãƒ€ã®å†…å®¹ã‚’è¡¨ã™å˜èª")
                    lines.append(f"     â€» è­˜åˆ¥æ€§ = TF Ã— IDF Ã— é›†ä¸­åº¦ Ã— 100: ä»–ãƒ•ã‚©ãƒ«ãƒ€ã¨åŒºåˆ¥ã™ã‚‹å˜èª")
                    lines.append(f"     â€» TF = æ–‡ä½ç½®é‡ã¿ä»˜ãå‡ºç¾æ¯”ç‡, é›†ä¸­åº¦ = ã“ã®ãƒ•ã‚©ãƒ«ãƒ€å‡ºç¾/å…¨ä½“å‡ºç¾, ä¸€è²«æ€§ = å˜èªã‚’å«ã‚€ç”»åƒç‡")
                    lines.append(f"     â€» IDF = log((ç·ãƒ•ã‚©ãƒ«ãƒ€æ•°+1)/(å‡ºç¾ãƒ•ã‚©ãƒ«ãƒ€æ•°+1)): ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªå¸Œå°‘æ€§")
                    lines.append("")
                
                # åˆ†é¡åŸºæº–ã®è©³ç´°æƒ…å ±ã‚‚è¿½åŠ 
                criteria_details = data.get('classification_criteria_details', {})
                if criteria_details:
                    lines.append("  ã€åˆ†é¡åŸºæº–ã®è©³ç´°ã€‘")
                    for category, info in criteria_details.items():
                        rank = info.get('rank', '-')
                        avg_score = float(info.get('avg_score', 0.0))
                        word_count = int(info.get('word_count', 0))
                        words = info.get('words', [])
                        lines.append(f"    ç¬¬{rank}ä½: {category}")
                        lines.append(f"      å¹³å‡ã‚¹ã‚³ã‚¢: {avg_score:.2f}, å˜èªæ•°: {word_count}")
                        lines.append(f"      å˜èª: {', '.join(words[:10])}")
                    lines.append("")
        
        # è©³ç´°ãªç‰¹å¾´åˆ†æçµæœ
        feature_analysis = data.get('feature_analysis', {})
        if feature_analysis:
            lines.append("ã€ç‰¹å¾´åˆ†æçµæœã€‘")
            
            # TF-IDFã‚¹ã‚³ã‚¢
            tfidf_scores = feature_analysis.get('tfidf_scores', {})
            if tfidf_scores:
                lines.append("  TF-IDFã‚¹ã‚³ã‚¢:")
                for folder_name, scores in tfidf_scores.items():
                    lines.append(f"    {folder_name}:")
                    for word, score in sorted(scores.items(), key=lambda x: x[1], reverse=True)[:5]:
                        lines.append(f"      - {word}: {score:.6f}")
            
            # è‰²æƒ…å ±
            color_info = feature_analysis.get('color_analysis', {})
            if color_info:
                lines.append("  è‰²åˆ†æ:")
                for key, value in color_info.items():
                    lines.append(f"    {key}: {value}")
            
            # å½¢çŠ¶æƒ…å ±
            shape_info = feature_analysis.get('shape_analysis', {})
            if shape_info:
                lines.append("  å½¢çŠ¶åˆ†æ:")
                for key, value in shape_info.items():
                    lines.append(f"    {key}: {value}")
            
            lines.append("")
        
        # åŒéšå±¤ãƒ•ã‚©ãƒ«ãƒ€æƒ…å ±
        sibling_info = data.get('sibling_folders_info', {})
        if sibling_info:
            lines.append("ã€åŒéšå±¤ãƒ•ã‚©ãƒ«ãƒ€æƒ…å ±ã€‘")
            lines.append(f"  åŒéšå±¤ãƒ•ã‚©ãƒ«ãƒ€æ•°: {sibling_info.get('total_siblings', 0)}")
            lines.append(f"  ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€æ•°: {sibling_info.get('leaf_siblings', 0)}")
            
            sibling_list = sibling_info.get('sibling_list', [])
            if sibling_list:
                lines.append("  ãƒ•ã‚©ãƒ«ãƒ€ä¸€è¦§:")
                for sib in sibling_list:
                    lines.append(f"    - {sib.get('name', 'N/A')} (ID: {sib.get('id', 'N/A')}, Leaf: {sib.get('is_leaf', False)})")
            lines.append("")
        
        # ãã®ä»–ã®æƒ…å ±
        additional_info = data.get('additional_info', {})
        if additional_info:
            lines.append("ã€è¿½åŠ æƒ…å ±ã€‘")
            for key, value in additional_info.items():
                lines.append(f"  {key}: {value}")
            lines.append("")
        
        # ã‚¨ãƒ©ãƒ¼æƒ…å ±
        errors = data.get('errors', [])
        if errors:
            lines.append("ã€ã‚¨ãƒ©ãƒ¼ãƒ»è­¦å‘Šã€‘")
            for error in errors:
                lines.append(f"  âš  {error}")
            lines.append("")
        
        lines.append("=" * 80)
        lines.append("ãƒ¬ãƒãƒ¼ãƒˆçµ‚äº†")
        lines.append("=" * 80)
        
        return "\n".join(lines)
    
    def generate_summary_report(self, all_reports_data: List[Dict[str, Any]]) -> str:
        """
        å®Ÿè¡Œå…¨ä½“ã®ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
        
        Args:
            all_reports_data: å…¨ç”»åƒã®ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        summary_path = self.report_dir / "SUMMARY.txt"
        
        lines = []
        lines.append("=" * 80)
        lines.append("ç¶™ç¶šçš„éšå±¤åˆ†é¡ å®Ÿè¡Œã‚µãƒãƒªãƒ¼")
        lines.append("=" * 80)
        lines.append("")
        
        # å®Ÿè¡Œæƒ…å ±
        if all_reports_data:
            first_report = all_reports_data[0]
            lines.append("ã€å®Ÿè¡Œæƒ…å ±ã€‘")
            lines.append(f"  å®Ÿè¡Œæ—¥æ™‚: {first_report.get('execution_time', 'N/A')}")
            lines.append(f"  ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {first_report.get('project_name', 'N/A')}")
            lines.append(f"  ãƒ¦ãƒ¼ã‚¶ãƒ¼: {first_report.get('user_name', 'N/A')}")
            lines.append(f"  ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å›æ•°: {first_report.get('clustering_count', 'N/A')}")
            lines.append("")
        
        # çµ±è¨ˆæƒ…å ±
        lines.append("ã€çµ±è¨ˆæƒ…å ±ã€‘")
        lines.append(f"  å‡¦ç†ç”»åƒæ•°: {len(all_reports_data)}")
        
        new_folders_count = sum(1 for r in all_reports_data if r.get('new_folder_created', False))
        lines.append(f"  æ–°è¦ä½œæˆãƒ•ã‚©ãƒ«ãƒ€æ•°: {new_folders_count}")
        
        criteria_used_count = sum(1 for r in all_reports_data if r.get('classification_criteria_used', False))
        lines.append(f"  åˆ†é¡åŸºæº–ã«ã‚ˆã‚‹å†åˆ¤å®š: {criteria_used_count}ä»¶")
        
        # å¹³å‡é¡ä¼¼åº¦
        similarities = [r.get('final_similarity', 0.0) for r in all_reports_data if r.get('final_similarity')]
        if similarities:
            avg_similarity = sum(similarities) / len(similarities)
            max_similarity = max(similarities)
            min_similarity = min(similarities)
            lines.append(f"  å¹³å‡é¡ä¼¼åº¦: {avg_similarity:.6f}")
            lines.append(f"  æœ€é«˜é¡ä¼¼åº¦: {max_similarity:.6f}")
            lines.append(f"  æœ€ä½é¡ä¼¼åº¦: {min_similarity:.6f}")
        lines.append("")
        
        # ç”»åƒã”ã¨ã®ç°¡æ˜“ã‚µãƒãƒªãƒ¼
        lines.append("ã€å‡¦ç†ç”»åƒä¸€è¦§ã€‘")
        for i, report in enumerate(all_reports_data, 1):
            image_name = report.get('image_name', 'Unknown')
            folder_name = report.get('final_folder_name', 'N/A')
            similarity = report.get('final_similarity', 0.0)
            new_folder = "âœ“" if report.get('new_folder_created', False) else ""
            lines.append(f"  [{i}] {image_name}")
            lines.append(f"      â†’ {folder_name} (é¡ä¼¼åº¦: {similarity:.4f}) {new_folder}")
        
        lines.append("")
        lines.append("=" * 80)
        
        try:
            with open(summary_path, 'w', encoding='utf-8') as f:
                f.write("\n".join(lines))
            print(f"ğŸ“„ ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ: {summary_path}")
            return str(summary_path)
        except Exception as e:
            print(f"âŒ ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def generate_metrics_report(
        self,
        all_reports_data: List[Dict[str, Any]],
        folder_data: Dict[str, Any] = None,
        similarity_threshold: float = 0.4
    ) -> str:
        """
        è©•ä¾¡æŒ‡æ¨™ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
        
        Args:
            all_reports_data: å…¨ç”»åƒã®ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            folder_data: ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ãƒ‡ãƒ¼ã‚¿
            similarity_threshold: é¡ä¼¼åº¦é–¾å€¤
            
        Returns:
            è©•ä¾¡æŒ‡æ¨™ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        metrics_path = self.report_dir / "METRICS_REPORT.txt"
        
        # è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—
        metrics = self.metrics_calculator.calculate_all_metrics(
            all_reports_data,
            folder_data or {},
            similarity_threshold
        )
        
        lines = []
        lines.append("=" * 80)
        lines.append("ç¶™ç¶šçš„éšå±¤åˆ†é¡ è©•ä¾¡æŒ‡æ¨™ãƒ¬ãƒãƒ¼ãƒˆ")
        lines.append("=" * 80)
        lines.append("")
        
        # å®Ÿè¡Œæƒ…å ±
        if all_reports_data:
            first_report = all_reports_data[0]
            lines.append("ã€å®Ÿè¡Œæƒ…å ±ã€‘")
            lines.append(f"  å®Ÿè¡Œæ—¥æ™‚: {first_report.get('execution_time', 'N/A')}")
            lines.append(f"  ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {first_report.get('project_name', 'N/A')}")
            lines.append(f"  ãƒ¦ãƒ¼ã‚¶ãƒ¼: {first_report.get('user_name', 'N/A')}")
            lines.append(f"  ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å›æ•°: {first_report.get('clustering_count', 'N/A')}")
            lines.append(f"  é¡ä¼¼åº¦é–¾å€¤: {similarity_threshold}")
            lines.append("")
        
        # 1. åŸºæœ¬çµ±è¨ˆ
        if 'basic_stats' in metrics:
            lines.append("=" * 80)
            lines.append("1. åŸºæœ¬çµ±è¨ˆ")
            lines.append("=" * 80)
            stats = metrics['basic_stats']
            lines.append(f"  å‡¦ç†ç”»åƒç·æ•°: {stats.get('total_images', 0)}")
            lines.append(f"  æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆæ•°: {stats.get('new_folders_created', 0)}")
            lines.append(f"  æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆç‡: {stats.get('new_folder_ratio', 0):.2%}")
            lines.append(f"  æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã¸ã®åˆ†é¡æ•°: {stats.get('existing_folder_assignments', 0)}")
            lines.append(f"  æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã¸ã®åˆ†é¡ç‡: {stats.get('existing_folder_ratio', 0):.2%}")
            lines.append(f"  åˆ†é¡åŸºæº–ä½¿ç”¨å›æ•°: {stats.get('criteria_based_classifications', 0)}")
            lines.append(f"  åˆ†é¡åŸºæº–ä½¿ç”¨ç‡: {stats.get('criteria_usage_ratio', 0):.2%}")
            lines.append(f"  ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿå›æ•°: {stats.get('errors_occurred', 0)}")
            lines.append(f"  ã‚¨ãƒ©ãƒ¼ç‡: {stats.get('error_ratio', 0):.2%}")
            lines.append("")
        
        # 2. åˆ†é¡å“è³ªæŒ‡æ¨™
        if 'classification_success' in metrics:
            lines.append("=" * 80)
            lines.append("2. åˆ†é¡å“è³ªæŒ‡æ¨™")
            lines.append("=" * 80)
            success = metrics['classification_success']
            lines.append(f"  é©åˆ‡ãªåˆ†é¡æ•°ï¼ˆé–¾å€¤åŸºæº–ï¼‰: {success.get('appropriate_classifications', 0)}")
            lines.append(f"  é©åˆ‡ãªåˆ†é¡ç‡: {success.get('appropriate_classification_ratio', 0):.2%}")
            lines.append(f"  é«˜ä¿¡é ¼åº¦ã§ã®æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€åˆ†é¡: {success.get('high_confidence_existing_folder', 0)}")
            lines.append(f"  é«˜ä¿¡é ¼åº¦æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€åˆ†é¡ç‡: {success.get('high_confidence_existing_ratio', 0):.2%}")
            lines.append(f"  é©åˆ‡ãªæ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ: {success.get('appropriate_new_folders', 0)}")
            lines.append(f"  é©åˆ‡ãªæ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆç‡: {success.get('appropriate_new_folder_ratio', 0):.2%}")
            lines.append("")
        
        # 3. é¡ä¼¼åº¦çµ±è¨ˆ
        if 'similarity_stats' in metrics:
            lines.append("=" * 80)
            lines.append("3. é¡ä¼¼åº¦çµ±è¨ˆ")
            lines.append("=" * 80)
            sim_stats = metrics['similarity_stats']
            lines.append(f"  å¹³å‡é¡ä¼¼åº¦: {sim_stats.get('mean_similarity', 0):.6f}")
            lines.append(f"  ä¸­å¤®å€¤é¡ä¼¼åº¦: {sim_stats.get('median_similarity', 0):.6f}")
            lines.append(f"  æ¨™æº–åå·®: {sim_stats.get('std_similarity', 0):.6f}")
            lines.append(f"  æœ€å°é¡ä¼¼åº¦: {sim_stats.get('min_similarity', 0):.6f}")
            lines.append(f"  æœ€å¤§é¡ä¼¼åº¦: {sim_stats.get('max_similarity', 0):.6f}")
            
            quartiles = sim_stats.get('quartiles', {})
            lines.append(f"  ç¬¬1å››åˆ†ä½æ•° (Q1): {quartiles.get('q1', 0):.6f}")
            lines.append(f"  ç¬¬2å››åˆ†ä½æ•° (Q2/ä¸­å¤®å€¤): {quartiles.get('q2', 0):.6f}")
            lines.append(f"  ç¬¬3å››åˆ†ä½æ•° (Q3): {quartiles.get('q3', 0):.6f}")
            
            lines.append(f"  æ–‡ç« ãƒ™ãƒ¼ã‚¹åˆ†é¡ç‡: {sim_stats.get('sentence_based_ratio', 0):.2%}")
            lines.append(f"  ç”»åƒãƒ™ãƒ¼ã‚¹åˆ†é¡ç‡: {sim_stats.get('image_based_ratio', 0):.2%}")
            lines.append("")
        
        # 4. ãƒ•ã‚©ãƒ«ãƒ€ãƒãƒ©ãƒ³ã‚¹æŒ‡æ¨™
        if 'folder_balance' in metrics:
            lines.append("=" * 80)
            lines.append("4. ãƒ•ã‚©ãƒ«ãƒ€ãƒãƒ©ãƒ³ã‚¹æŒ‡æ¨™")
            lines.append("=" * 80)
            balance = metrics['folder_balance']
            lines.append(f"  ä½¿ç”¨ãƒ•ã‚©ãƒ«ãƒ€ç·æ•°: {balance.get('total_folders_used', 0)}")
            lines.append(f"  ãƒ•ã‚©ãƒ«ãƒ€ã‚ãŸã‚Šå¹³å‡ç”»åƒæ•°: {balance.get('mean_images_per_folder', 0):.2f}")
            lines.append(f"  æ¨™æº–åå·®: {balance.get('std_images_per_folder', 0):.2f}")
            lines.append(f"  æœ€å°ç”»åƒæ•°: {balance.get('min_images_per_folder', 0)}")
            lines.append(f"  æœ€å¤§ç”»åƒæ•°: {balance.get('max_images_per_folder', 0)}")
            lines.append(f"  ã‚¸ãƒ‹ä¿‚æ•°ï¼ˆä¸å‡è¡¡åº¦ï¼‰: {balance.get('gini_coefficient', 0):.4f}")
            lines.append(f"  å¤‰å‹•ä¿‚æ•° (CV): {balance.get('coefficient_of_variation', 0):.4f}")
            lines.append(f"  ãƒãƒ©ãƒ³ã‚¹ã‚¹ã‚³ã‚¢ï¼ˆ0-1ï¼‰: {balance.get('balance_score', 0):.4f}")
            lines.append("    â€» ãƒãƒ©ãƒ³ã‚¹ã‚¹ã‚³ã‚¢ãŒ1ã«è¿‘ã„ã»ã©å‡ç­‰ã«åˆ†æ•£")
            lines.append("")
        
        # 5. åˆ†é¡åŸºæº–ã®ä¸€è²«æ€§
        if 'criteria_consistency' in metrics:
            lines.append("=" * 80)
            lines.append("5. åˆ†é¡åŸºæº–ã®ä¸€è²«æ€§")
            lines.append("=" * 80)
            consistency = metrics['criteria_consistency']
            lines.append(f"  åˆ†é¡åŸºæº–ä½¿ç”¨å›æ•°: {consistency.get('criteria_used_count', 0)}")
            lines.append(f"  æˆåŠŸå›æ•°: {consistency.get('criteria_success_count', 0)}")
            lines.append(f"  ä¸€è²«æ€§ã‚¹ã‚³ã‚¢: {consistency.get('consistency_score', 0):.4f}")
            lines.append(f"  ä¸€è²«æ€§ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸: {consistency.get('consistency_percentage', 0):.2f}%")
            if 'note' in consistency:
                lines.append(f"  å‚™è€ƒ: {consistency['note']}")
            lines.append("")
        
        # 6. ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
        if 'confidence_scores' in metrics:
            lines.append("=" * 80)
            lines.append("6. åˆ†é¡ä¿¡é ¼åº¦ã®åˆ†å¸ƒ")
            lines.append("=" * 80)
            confidence = metrics['confidence_scores']
            thresholds = confidence.get('thresholds', {})
            lines.append(f"  é«˜ä¿¡é ¼åº¦ï¼ˆâ‰¥{thresholds.get('high', 0.7)}ï¼‰:")
            lines.append(f"    ä»¶æ•°: {confidence.get('high_confidence_count', 0)}")
            lines.append(f"    å‰²åˆ: {confidence.get('high_confidence_ratio', 0):.2%}")
            lines.append(f"  ä¸­ä¿¡é ¼åº¦ï¼ˆ{thresholds.get('medium', 0.5)} - {thresholds.get('high', 0.7)}ï¼‰:")
            lines.append(f"    ä»¶æ•°: {confidence.get('medium_confidence_count', 0)}")
            lines.append(f"    å‰²åˆ: {confidence.get('medium_confidence_ratio', 0):.2%}")
            lines.append(f"  ä½ä¿¡é ¼åº¦ï¼ˆ<{thresholds.get('medium', 0.5)}ï¼‰:")
            lines.append(f"    ä»¶æ•°: {confidence.get('low_confidence_count', 0)}")
            lines.append(f"    å‰²åˆ: {confidence.get('low_confidence_ratio', 0):.2%}")
            lines.append("")
        
        # 7. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
        if 'performance' in metrics:
            lines.append("=" * 80)
            lines.append("7. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™")
            lines.append("=" * 80)
            perf = metrics['performance']
            lines.append(f"  æ–‡ç« åŸ‹ã‚è¾¼ã¿å–å¾—æˆåŠŸç‡: {perf.get('sentence_embedding_success_rate', 0):.2%}")
            lines.append(f"  ç”»åƒåŸ‹ã‚è¾¼ã¿å–å¾—æˆåŠŸç‡: {perf.get('image_embedding_success_rate', 0):.2%}")
            lines.append(f"  ä¸¡æ–¹å–å¾—æˆåŠŸæ•°: {perf.get('both_embeddings_available', 0)}")
            lines.append(f"  ä¸¡æ–¹å–å¾—æˆåŠŸç‡: {perf.get('both_embeddings_available_rate', 0):.2%}")
            lines.append("")
        
        # 8. ã‚¨ãƒ©ãƒ¼åˆ†æ
        if 'error_analysis' in metrics:
            lines.append("=" * 80)
            lines.append("8. ã‚¨ãƒ©ãƒ¼åˆ†æ")
            lines.append("=" * 80)
            error_analysis = metrics['error_analysis']
            lines.append(f"  ç·ã‚¨ãƒ©ãƒ¼æ•°: {error_analysis.get('total_errors', 0)}")
            lines.append(f"  ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿç”»åƒæ•°: {error_analysis.get('images_with_errors', 0)}")
            lines.append(f"  ã‚¨ãƒ©ãƒ¼ãªã—ç‡: {error_analysis.get('error_free_ratio', 0):.2%}")
            
            error_types = error_analysis.get('error_type_distribution', {})
            if error_types:
                lines.append("  ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥åˆ†å¸ƒ:")
                for error_type, count in error_types.items():
                    lines.append(f"    - {error_type}: {count}")
            lines.append("")
        
        lines.append("=" * 80)
        lines.append("è©•ä¾¡æŒ‡æ¨™ãƒ¬ãƒãƒ¼ãƒˆçµ‚äº†")
        lines.append("=" * 80)
        lines.append("")
        lines.append("ã€æŒ‡æ¨™ã®è§£é‡ˆã€‘")
        lines.append("- æ–°è¦ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆç‡ãŒé«˜ã„ â†’ æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã¨ã®é¡ä¼¼åº¦ãŒä½ã„æ–°ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå¤šã„")
        lines.append("- ã‚¸ãƒ‹ä¿‚æ•°ãŒä½ã„ï¼ˆãƒãƒ©ãƒ³ã‚¹ã‚¹ã‚³ã‚¢ãŒé«˜ã„ï¼‰ â†’ ãƒ•ã‚©ãƒ«ãƒ€é–“ã§ç”»åƒãŒå‡ç­‰ã«åˆ†æ•£")
        lines.append("- å¹³å‡é¡ä¼¼åº¦ãŒé«˜ã„ â†’ æ—¢å­˜ãƒ•ã‚©ãƒ«ãƒ€ã¨ã®é©åˆåº¦ãŒé«˜ã„")
        lines.append("- å¤‰å‹•ä¿‚æ•°(CV)ãŒä½ã„ â†’ ãƒ•ã‚©ãƒ«ãƒ€ã‚ãŸã‚Šã®ç”»åƒæ•°ãŒå®‰å®šã—ã¦ã„ã‚‹")
        lines.append("- ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ãŒé«˜ã„ â†’ åˆ†é¡ã®ç¢ºå®Ÿæ€§ãŒé«˜ã„")
        
        try:
            with open(metrics_path, 'w', encoding='utf-8') as f:
                f.write("\n".join(lines))
            print(f"ğŸ“Š è©•ä¾¡æŒ‡æ¨™ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ: {metrics_path}")
            
            # JSONå½¢å¼ã§ã‚‚ä¿å­˜
            json_path = self.report_dir / "METRICS_DATA.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(metrics, f, indent=2, ensure_ascii=False)
            print(f"ğŸ“Š è©•ä¾¡æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ï¼ˆJSONï¼‰ä¿å­˜: {json_path}")
            
            return str(metrics_path)
        except Exception as e:
            print(f"âŒ è©•ä¾¡æŒ‡æ¨™ãƒ¬ãƒãƒ¼ãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise

