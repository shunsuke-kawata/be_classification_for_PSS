import json
import os
from pathlib import Path
import shutil
import numpy as np
import copy
import matplotlib.pyplot as plt
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score
from .chroma_db_manager import ChromaDBManager
import re
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction import text
from .embeddings_manager.sentence_embeddings_manager import SentenceEmbeddingsManager
from .embeddings_manager.image_embeddings_manager import ImageEmbeddingsManager
from .utils import Utils
from config import DEFAULT_IMAGE_PATH, MAJOR_COLORS, MAJOR_SHAPES, CAPTION_STOPWORDS

class ClusteringUtils:
    @classmethod
    def get_tfidf_from_documents_array(cls, documents: list[str], max_words: int = 10, order: str = 'high',extra_stop_words:list[str]=None) -> list[tuple[str, float]]:
        """
        æ–‡æ›¸é…åˆ—ã‹ã‚‰TF-IDFã‚’ä½¿ç”¨ã—ã¦ã‚¹ã‚³ã‚¢ã®é«˜ã„/ä½ã„èªã‚’å–å¾—ã™ã‚‹
        ç©ºã®èªå½™ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã€æ®µéšçš„ã«ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’æ¸›ã‚‰ã—ã¦å†è©¦è¡Œã™ã‚‹
        
        Args:
            documents: æ–‡æ›¸ã®é…åˆ—
            max_words: å–å¾—ã™ã‚‹æœ€å¤§èªæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰
            order: ã‚½ãƒ¼ãƒˆé † 'high'ï¼ˆé«˜ã„é †ï¼‰ã¾ãŸã¯ 'low'ï¼ˆä½ã„é †ï¼‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 'high'ï¼‰
            extra_stop_words: è¿½åŠ ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰è¨­å®š
        
        Returns:
            list[tuple[str, float]]: (èª, ã‚¹ã‚³ã‚¢)ã®ã‚¿ãƒ—ãƒ«ã®é…åˆ—
        """
        if not documents or len(documents) == 0:
            return []
        
        # ãƒ™ãƒ¼ã‚¹ã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’è¨­å®š
        base_stop_words = list(text.ENGLISH_STOP_WORDS)
        extra_stop_words = extra_stop_words or []
        
        # æ®µéšçš„ã«ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’æ¸›ã‚‰ã—ã¦è©¦è¡Œ
        stop_word_configurations = [
            # 1å›ç›®: å…¨ã¦ã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
            base_stop_words + extra_stop_words,
            # 2å›ç›®: MAJOR_COLORSã‚’é™¤å¤–
            base_stop_words + [word for word in extra_stop_words if word not in MAJOR_COLORS],
            # 3å›ç›®: 'shape'ã‚‚é™¤å¤–
            base_stop_words + [word for word in extra_stop_words if word not in MAJOR_COLORS and word != 'shape'],
            # 4å›ç›®: ãƒ™ãƒ¼ã‚¹ã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã®ã¿
            base_stop_words,
            # 5å›ç›®: ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ãªã—
            None
        ]
        
        for attempt, stop_words in enumerate(stop_word_configurations, 1):
            try:
                vectorizer = TfidfVectorizer(stop_words=stop_words)
                tfidf_matrix = vectorizer.fit_transform(documents)
                
                # å„èªå½™ã®ã‚¹ã‚³ã‚¢ï¼ˆæ–‡æ›¸å…¨ä½“ã§ã®åˆè¨ˆã‚¹ã‚³ã‚¢ï¼‰
                sum_scores = np.asarray(tfidf_matrix.sum(axis=0))[0]
                terms = vectorizer.get_feature_names_out()
                
                # ã‚¹ã‚³ã‚¢é †ã«ä¸¦ã¹ã‚‹
                if order.lower() == 'high':
                    # é«˜ã„é †ï¼ˆé™é †ï¼‰
                    sorted_idx = sum_scores.argsort()[::-1]
                elif order.lower() == 'low':
                    # ä½ã„é †ï¼ˆæ˜‡é †ï¼‰
                    sorted_idx = sum_scores.argsort()
                else:
                    raise ValueError("order parameter must be 'high' or 'low'")
                
                # æŒ‡å®šã•ã‚ŒãŸæœ€å¤§èªæ•°ã¾ã§çµæœã‚’å–å¾—
                result = []
                for i, idx in enumerate(sorted_idx[:max_words]):
                    word = terms[idx]
                    score = sum_scores[idx]
                    result.append((word, float(score)))
                
                if attempt > 1:
                    print(f"TF-IDFæˆåŠŸ: {attempt}å›ç›®ã®è©¦è¡Œã§ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰è¨­å®šã‚’èª¿æ•´ã—ã¾ã—ãŸ")
                
                return result
                
            except ValueError as e:
                if "empty vocabulary" in str(e):
                    print(f"TF-IDFè©¦è¡Œ {attempt}: ç©ºã®èªå½™ã‚¨ãƒ©ãƒ¼ - ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’èª¿æ•´ã—ã¦å†è©¦è¡Œ")
                    continue
                else:
                    raise e
        
        # å…¨ã¦ã®è©¦è¡ŒãŒå¤±æ•—ã—ãŸå ´åˆ
        print("è­¦å‘Š: TF-IDFã§èªå½™ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã—ã¾ã™ã€‚")
        return []
        
        
class InitClusteringManager:
    
    COHESION_THRESHOLD = 0.85  # å‡é›†åº¦ã®é–¾å€¤ã‚’0.75â†’0.85ã«å¼•ãä¸Šã’ï¼ˆã‚ˆã‚Šå³ã—ãï¼‰
    MERGE_SIMILARITY_THRESHOLD = 0.90  # ã‚¯ãƒ©ã‚¹ã‚¿é–“é¡ä¼¼åº¦ã®é–¾å€¤ã‚’0.85â†’0.90ã«å¼•ãä¸Šã’ï¼ˆã‚ˆã‚Šå³ã—ãï¼‰
    
    def __init__(self, sentence_name_db: ChromaDBManager, sentence_usage_db: ChromaDBManager, sentence_category_db: ChromaDBManager, image_db: ChromaDBManager, images_folder_path: str, output_base_path: str = './results'):
        def _is_valid_path(path: str) -> bool:
            if not isinstance(path, str) or not path.strip():
                return False

            if not (path.startswith("./") or path.startswith("../") or path.startswith("/")):
                return False

            if path.endswith("/"):
                return False

            if re.search(r'[<>:"|?*]', path):
                return False

            return True

        if not (_is_valid_path(images_folder_path) and _is_valid_path(output_base_path)):
            raise ValueError(f" Error Folder Path: {images_folder_path}, {output_base_path}")
        
        self._sentence_name_db = sentence_name_db
        self._sentence_usage_db = sentence_usage_db
        self._sentence_category_db = sentence_category_db
        self._image_db = image_db
        self._images_folder_path = Path(images_folder_path)
        self._output_base_path = Path(output_base_path)
    
    @property
    def sentence_name_db(self) -> ChromaDBManager:
        return self._sentence_name_db

    @property
    def sentence_usage_db(self) -> ChromaDBManager:
        return self._sentence_usage_db

    @property
    def sentence_category_db(self) -> ChromaDBManager:
        return self._sentence_category_db

    @property
    def image_db(self)->ChromaDBManager:
        return self._image_db

    @property
    def images_folder_path(self) -> Path:
        return self._images_folder_path
    
    @property
    def output_base_path(self) -> Path:
        return self._output_base_path
    
    def register_split_document(self, document: str, chroma_sentence_id: str, metadata: 'ChromaDBManager.ChromaMetaData'):
        """
        åˆ†å‰²ã•ã‚ŒãŸæ–‡æ›¸ã‚’3ã¤ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²ã™ã‚‹
        
        Args:
            document: å…ƒã®æ–‡æ›¸ï¼ˆ3æ–‡ã§æ§‹æˆï¼‰
            chroma_sentence_id: chroma_sentence_idã¨ãªã‚‹ID
            metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        """
        # æ–‡æ›¸ã‚’3ã¤ã«åˆ†å‰²
        name_part, usage_part, category_part = ChromaDBManager.split_sentence_document(document)
        
        # å„éƒ¨åˆ†ã®embeddingã‚’ç”Ÿæˆ
        name_embedding = SentenceEmbeddingsManager.sentence_to_embedding(name_part)
        usage_embedding = SentenceEmbeddingsManager.sentence_to_embedding(usage_part)
        category_embedding = SentenceEmbeddingsManager.sentence_to_embedding(category_part)
        
        # IDã«æ¥å°¾è¾ã‚’ä»˜ã‘ã¦åŒºåˆ¥
        # related_ids = ChromaDBManager.generate_related_ids(chroma_sentence_id)
        
        # å„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç”¨ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        name_metadata = ChromaDBManager.ChromaMetaData(
            path=metadata.path,
            document=name_part,
            is_success=metadata.is_success,
            id=chroma_sentence_id
        )
        usage_metadata = ChromaDBManager.ChromaMetaData(
            path=metadata.path,
            document=usage_part,
            is_success=metadata.is_success,
            id=chroma_sentence_id
        )
        category_metadata = ChromaDBManager.ChromaMetaData(
            path=metadata.path,
            document=category_part,
            is_success=metadata.is_success,
            id=chroma_sentence_id
        )
        
        # å„ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²
        self._sentence_name_db.add_one(name_part, name_metadata, name_embedding)
        self._sentence_usage_db.add_one(usage_part, usage_metadata, usage_embedding)
        self._sentence_category_db.add_one(category_part, category_metadata, category_embedding)
        
        return chroma_sentence_id
    
    
    def get_optimal_cluster_num(self, embeddings: list[float], min_cluster_num: int = 5, max_cluster_num: int = 30) -> tuple[int, float]:
        """
        X-meansã‚’ä½¿ç”¨ã—ã¦ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’è‡ªå‹•æ±ºå®šã™ã‚‹
        
        Args:
            embeddings: åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒªã‚¹ãƒˆ
            min_cluster_num: æœ€å°ã‚¯ãƒ©ã‚¹ã‚¿æ•°ï¼ˆåˆæœŸä¸­å¿ƒæ•°ã¨ã—ã¦ä½¿ç”¨ï¼‰
            max_cluster_num: æœ€å¤§ã‚¯ãƒ©ã‚¹ã‚¿æ•°
        
        Returns:
            tuple[int, float]: (ã‚¯ãƒ©ã‚¹ã‚¿æ•°, ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢)
        """
        embeddings_np = np.array(embeddings)
        n_samples = len(embeddings_np)

        if n_samples < 3:
            print("ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã™ãã¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã§ãã¾ã›ã‚“")
            return 1, -1.0

        try:
            best_score = -1
            best_n_clusters = min_cluster_num
            
            # ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’å¤‰ãˆãªãŒã‚‰éšå±¤å‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’è©¦è¡Œ
            for n_clusters in range(min_cluster_num, min(max_cluster_num + 1, n_samples)):
                if n_clusters < 2:
                    continue
                    
                clustering = AgglomerativeClustering(
                    n_clusters=n_clusters,
                    metric='cosine',
                    linkage='average'
                )
                labels = clustering.fit_predict(embeddings_np)
                
                # ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                score = silhouette_score(embeddings_np, labels, metric='cosine')
                
                if score > best_score:
                    best_score = score
                    best_n_clusters = n_clusters
            
            print(f"éšå±¤å‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æœ€é©åŒ–çµæœ: ã‚¯ãƒ©ã‚¹ã‚¿æ•°={best_n_clusters}, ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢={best_score:.4f}")
            
            return best_n_clusters, float(best_score)
            
        except Exception as e:
            print(f"éšå±¤å‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
            print(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: min_cluster_num={min_cluster_num}ã‚’ä½¿ç”¨")
            return min_cluster_num, -1.0
    
    def _calculate_cluster_cohesion(self, embeddings: list, cluster_indices: list) -> float:
        """
        ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®å‡é›†åº¦ã‚’è¨ˆç®—ã™ã‚‹
        
        Args:
            embeddings: å…¨åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
            cluster_indices: ã‚¯ãƒ©ã‚¹ã‚¿ã«å±ã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆ
        
        Returns:
            float: å‡é›†åº¦ã‚¹ã‚³ã‚¢ï¼ˆ0-1ã€é«˜ã„ã»ã©å‡é›†åº¦ãŒé«˜ã„ï¼‰
        """
        if len(cluster_indices) <= 1:
            return 1.0
        
        cluster_embeddings = np.array([embeddings[i] for i in cluster_indices])
        centroid = np.mean(cluster_embeddings, axis=0)
        
        # å„ç‚¹ã‹ã‚‰ã‚»ãƒ³ãƒˆãƒ­ã‚¤ãƒ‰ã¾ã§ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®å¹³å‡
        similarities = []
        for emb in cluster_embeddings:
            similarity = np.dot(emb, centroid) / (np.linalg.norm(emb) * np.linalg.norm(centroid))
            similarities.append(similarity)
        
        return float(np.mean(similarities))
    
    def _calculate_cluster_center(self, embeddings: list, cluster_indices: list) -> np.ndarray:
        """
        ã‚¯ãƒ©ã‚¹ã‚¿ã®ä¸­å¿ƒã‚’è¨ˆç®—ã™ã‚‹
        
        Args:
            embeddings: å…¨åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
            cluster_indices: ã‚¯ãƒ©ã‚¹ã‚¿ã«å±ã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆ
        
        Returns:
            np.ndarray: ã‚¯ãƒ©ã‚¹ã‚¿ã®ä¸­å¿ƒãƒ™ã‚¯ãƒˆãƒ«
        """
        cluster_embeddings = np.array([embeddings[i] for i in cluster_indices])
        return np.mean(cluster_embeddings, axis=0)
    
    def _merge_singleton_clusters(self, clusters: dict, image_embeddings_dict: dict) -> dict:
        """
        è¦ç´ æ•°ãŒ1ã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’ã€ãã®éšå±¤ã«ã‚ã‚‹å…¨ã¦ã®ç”»åƒç‰¹å¾´é‡ã«å¯¾ã—ã¦é¡ä¼¼åº¦æ¤œç´¢ã‚’è¡Œã„ã€
        æœ€ã‚‚é¡ä¼¼åº¦ã®é«˜ã„ã‚¯ãƒ©ã‚¹ã‚¿ã«çµ±åˆã™ã‚‹
        
        Args:
            clusters: {cluster_id: [sentence_ids]} ã®è¾æ›¸
            image_embeddings_dict: {sentence_id: image_embedding} ã®è¾æ›¸
        
        Returns:
            dict: çµ±åˆå¾Œã®ã‚¯ãƒ©ã‚¹ã‚¿è¾æ›¸
        """
        if len(clusters) <= 1:
            return clusters
        
        # è¦ç´ æ•°ãŒ1ã®ã‚¯ãƒ©ã‚¹ã‚¿ã¨2ä»¥ä¸Šã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’åˆ†é›¢
        singleton_clusters = {}
        multi_element_clusters = {}
        
        for cluster_id, sentence_ids in clusters.items():
            if len(sentence_ids) == 1:
                singleton_clusters[cluster_id] = sentence_ids
            else:
                multi_element_clusters[cluster_id] = sentence_ids
        
        # ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãŒãªã„å ´åˆã¯ãã®ã¾ã¾è¿”ã™
        if not singleton_clusters:
            return clusters
        
        # çµ±åˆå…ˆãŒãªã„å ´åˆï¼ˆå…¨ã¦ãŒã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰ã¯æœ€ã‚‚é¡ä¼¼åº¦ã®é«˜ã„ãƒšã‚¢ã‚’çµ±åˆ
        if not multi_element_clusters:
            print(f"  è­¦å‘Š: å…¨ã¦ã®ã‚¯ãƒ©ã‚¹ã‚¿ãŒè¦ç´ æ•°1ã§ã™ã€‚æœ€ã‚‚é¡ä¼¼åº¦ã®é«˜ã„ãƒšã‚¢ã‹ã‚‰çµ±åˆã—ã¾ã™")
            return self._merge_all_singletons(singleton_clusters, image_embeddings_dict)
        
        print(f"  è¦ç´ æ•°1ã®ã‚¯ãƒ©ã‚¹ã‚¿: {len(singleton_clusters)}å€‹ã‚’çµ±åˆå‡¦ç†")
        # è¦ç´ æ•°1ã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’æœ€ã‚‚é¡ä¼¼åº¦ã®é«˜ã„ã‚¯ãƒ©ã‚¹ã‚¿ã«çµ±åˆ
        merged_clusters = dict(multi_element_clusters)  # ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ
        print()
        for singleton_id, sentence_ids in singleton_clusters.items():
            sentence_id = sentence_ids[0]
            
            if sentence_id not in image_embeddings_dict:
                # ç”»åƒåŸ‹ã‚è¾¼ã¿ãŒãªã„å ´åˆã¯çµ±åˆã›ãšå€‹åˆ¥ã«ä¿æŒ
                merged_clusters[singleton_id] = sentence_ids
                continue
            
            singleton_embedding = image_embeddings_dict[sentence_id]
            
            # ãã®éšå±¤ã«ã‚ã‚‹å…¨ã¦ã®ç”»åƒç‰¹å¾´é‡ã«å¯¾ã—ã¦é¡ä¼¼åº¦æ¤œç´¢
            best_similarity = -1
            best_cluster_id = None
            best_target_sentence_id = None
            
            # å„ã‚¯ãƒ©ã‚¹ã‚¿ã®å…¨ã¦ã®ç”»åƒã«å¯¾ã—ã¦é¡ä¼¼åº¦ã‚’è¨ˆç®—
            for cluster_id, cluster_sentence_ids in multi_element_clusters.items():
                for target_sentence_id in cluster_sentence_ids:
                    if target_sentence_id not in image_embeddings_dict:
                        continue
                    
                    target_embedding = image_embeddings_dict[target_sentence_id]
                    
                    # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—
                    similarity = np.dot(singleton_embedding, target_embedding) / (
                        np.linalg.norm(singleton_embedding) * np.linalg.norm(target_embedding)
                    )
                    
                    if similarity > best_similarity:
                        best_similarity = similarity
                        best_cluster_id = cluster_id
                        best_target_sentence_id = target_sentence_id
            
            # æœ€ã‚‚é¡ä¼¼åº¦ã®é«˜ã„ã‚¯ãƒ©ã‚¹ã‚¿ã«çµ±åˆ
            if best_cluster_id is not None:
                merged_clusters[best_cluster_id].append(sentence_id)
                print(f"    ã‚¯ãƒ©ã‚¹ã‚¿{singleton_id}(è¦ç´ æ•°1)ã‚’ã‚¯ãƒ©ã‚¹ã‚¿{best_cluster_id}ã«çµ±åˆ")
                print(f"      é¡ä¼¼åº¦: {best_similarity:.3f} (å¯¾è±¡: {best_target_sentence_id})")
            else:
                # çµ±åˆå…ˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å€‹åˆ¥ã«ä¿æŒ
                merged_clusters[singleton_id] = sentence_ids
        
        return merged_clusters
    
    def _merge_all_singletons(self, singleton_clusters: dict, image_embeddings_dict: dict) -> dict:
        """
        å…¨ã¦ã®ã‚¯ãƒ©ã‚¹ã‚¿ãŒè¦ç´ æ•°1ã®å ´åˆã«ã€æœ€ã‚‚é¡ä¼¼åº¦ã®é«˜ã„ãƒšã‚¢ã‹ã‚‰é †ã«çµ±åˆã—ã¦ã„ã
        
        Args:
            singleton_clusters: {cluster_id: [sentence_id]} ã®è¾æ›¸ï¼ˆå…¨ã¦è¦ç´ æ•°1ï¼‰
            image_embeddings_dict: {sentence_id: image_embedding} ã®è¾æ›¸
        
        Returns:
            dict: çµ±åˆå¾Œã®ã‚¯ãƒ©ã‚¹ã‚¿è¾æ›¸
        """
        # å„ãƒšã‚¢ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—
        similarities = []
        cluster_ids = list(singleton_clusters.keys())
        
        for i, cluster_id_1 in enumerate(cluster_ids):
            sentence_id_1 = singleton_clusters[cluster_id_1][0]
            if sentence_id_1 not in image_embeddings_dict:
                continue
            
            embedding_1 = image_embeddings_dict[sentence_id_1]
            
            for j in range(i + 1, len(cluster_ids)):
                cluster_id_2 = cluster_ids[j]
                sentence_id_2 = singleton_clusters[cluster_id_2][0]
                
                if sentence_id_2 not in image_embeddings_dict:
                    continue
                
                embedding_2 = image_embeddings_dict[sentence_id_2]
                
                # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—
                similarity = np.dot(embedding_1, embedding_2) / (
                    np.linalg.norm(embedding_1) * np.linalg.norm(embedding_2)
                )
                
                similarities.append((similarity, cluster_id_1, cluster_id_2))
        
        if not similarities:
            print(f"    é¡ä¼¼åº¦ã‚’è¨ˆç®—ã§ãã‚‹ãƒšã‚¢ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ƒã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’è¿”ã—ã¾ã™")
            return singleton_clusters
        
        # é¡ä¼¼åº¦ã®é«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
        similarities.sort(reverse=True, key=lambda x: x[0])
        
        # ä¸Šä½ã®ãƒšã‚¢ã‚’çµ±åˆï¼ˆæœ€å¤§ã§åŠåˆ†ã¾ã§çµ±åˆï¼‰
        merged_clusters = dict(singleton_clusters)
        merged_count = 0
        max_merges = max(1, len(singleton_clusters) // 2)  # å°‘ãªãã¨ã‚‚1ãƒšã‚¢ã¯çµ±åˆ
        used_clusters = set()
        
        for similarity, cluster_id_1, cluster_id_2 in similarities:
            if merged_count >= max_merges:
                break
            
            if cluster_id_1 in used_clusters or cluster_id_2 in used_clusters:
                continue
            
            # cluster_id_2ã‚’cluster_id_1ã«çµ±åˆ
            merged_clusters[cluster_id_1].extend(merged_clusters[cluster_id_2])
            del merged_clusters[cluster_id_2]
            
            used_clusters.add(cluster_id_1)
            used_clusters.add(cluster_id_2)
            merged_count += 1
            
            print(f"    ã‚¯ãƒ©ã‚¹ã‚¿{cluster_id_2}ã‚’ã‚¯ãƒ©ã‚¹ã‚¿{cluster_id_1}ã«çµ±åˆ (é¡ä¼¼åº¦: {similarity:.3f})")
        
        print(f"    {merged_count}ãƒšã‚¢ã‚’çµ±åˆã—ã¾ã—ãŸ")
        return merged_clusters
    
    def _merge_similar_clusters(self, clusters: dict, embeddings: list, image_embeddings_dict: dict) -> dict:
        """
        å‡é›†åº¦ãŒé«˜ãã€ä»–ã‚¯ãƒ©ã‚¹ã‚¿ã¨é¡ä¼¼ã—ã¦ã„ã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ã‚’ãƒãƒ¼ã‚¸ã™ã‚‹
        ç”»åƒãƒ™ã‚¯ãƒˆãƒ«ã¨æ–‡ç« ãƒ™ã‚¯ãƒˆãƒ«ã®ä¸¡æ–¹ã‚’ä½¿ç”¨ã—ã¦ãƒãƒ¼ã‚¸åˆ¤å®šã‚’è¡Œã†
        
        Args:
            clusters: {cluster_id: [sentence_ids]} ã®è¾æ›¸
            embeddings: æ–‡ç« åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒªã‚¹ãƒˆ
            image_embeddings_dict: {sentence_id: image_embedding} ã®è¾æ›¸
        
        Returns:
            dict: ãƒãƒ¼ã‚¸å¾Œã®ã‚¯ãƒ©ã‚¹ã‚¿è¾æ›¸
        """
        if len(clusters) <= 1:
            return clusters
        
        # sentence_idã¨embeddingsã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
        sentence_id_to_embedding_index = {}
        for cluster_id, sentence_ids in clusters.items():
            for idx, sid in enumerate(sentence_ids):
                sentence_id_to_embedding_index[sid] = idx
        
        # å„ã‚¯ãƒ©ã‚¹ã‚¿ã®å‡é›†åº¦ã¨ä¸­å¿ƒã‚’è¨ˆç®—ï¼ˆç”»åƒã¨æ–‡ç« ã®ä¸¡æ–¹ï¼‰
        cluster_info = {}
        for cluster_id, sentence_ids in clusters.items():
            # ç”»åƒåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
            cluster_image_embeddings = []
            cluster_sentence_embeddings = []
            valid_indices = []
            
            for idx, sid in enumerate(sentence_ids):
                # ç”»åƒåŸ‹ã‚è¾¼ã¿
                if sid in image_embeddings_dict:
                    cluster_image_embeddings.append(image_embeddings_dict[sid])
                    valid_indices.append(idx)
                
                # æ–‡ç« åŸ‹ã‚è¾¼ã¿
                if sid in sentence_id_to_embedding_index:
                    emb_idx = sentence_id_to_embedding_index[sid]
                    if emb_idx < len(embeddings):
                        cluster_sentence_embeddings.append(embeddings[emb_idx])
            
            if not cluster_image_embeddings and not cluster_sentence_embeddings:
                # ä¸¡æ–¹ã®åŸ‹ã‚è¾¼ã¿ãŒãªã„å ´åˆã§ã‚‚ã‚¯ãƒ©ã‚¹ã‚¿ã¯ä¿æŒ
                cluster_info[cluster_id] = {
                    'sentence_ids': sentence_ids,
                    'image_cohesion': 0.0,
                    'sentence_cohesion': 0.0,
                    'image_center': None,
                    'sentence_center': None,
                    'size': len(sentence_ids)
                }
                continue
            
            # ç”»åƒç‰¹å¾´é‡ã§ã®å‡é›†åº¦ã¨ä¸­å¿ƒã‚’è¨ˆç®—
            image_cohesion = 0.0
            image_center = None
            if cluster_image_embeddings:
                image_cohesion = self._calculate_cluster_cohesion(
                    cluster_image_embeddings, 
                    list(range(len(cluster_image_embeddings)))
                )
                image_center = self._calculate_cluster_center(
                    cluster_image_embeddings, 
                    list(range(len(cluster_image_embeddings)))
                )
            
            # æ–‡ç« ç‰¹å¾´é‡ã§ã®å‡é›†åº¦ã¨ä¸­å¿ƒã‚’è¨ˆç®—
            sentence_cohesion = 0.0
            sentence_center = None
            if cluster_sentence_embeddings:
                sentence_cohesion = self._calculate_cluster_cohesion(
                    cluster_sentence_embeddings, 
                    list(range(len(cluster_sentence_embeddings)))
                )
                sentence_center = self._calculate_cluster_center(
                    cluster_sentence_embeddings, 
                    list(range(len(cluster_sentence_embeddings)))
                )
            
            cluster_info[cluster_id] = {
                'sentence_ids': sentence_ids,
                'image_cohesion': image_cohesion,
                'sentence_cohesion': sentence_cohesion,
                'image_center': image_center,
                'sentence_center': sentence_center,
                'size': len(sentence_ids)
            }
        
        # cluster_infoãŒç©ºã®å ´åˆã¯å…ƒã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’ãã®ã¾ã¾è¿”ã™
        if not cluster_info:
            print(f"  è­¦å‘Š: ãƒãƒ¼ã‚¸å‡¦ç†ã§cluster_infoãŒç©ºã§ã™ã€‚å…ƒã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’è¿”ã—ã¾ã™ã€‚")
            return clusters
        
        # ãƒãƒ¼ã‚¸å¯¾è±¡ã‚’æ¢ç´¢
        merged = {}
        used_clusters = set()
        cluster_ids = list(cluster_info.keys())
        
        for i, cluster_id_1 in enumerate(cluster_ids):
            if cluster_id_1 in used_clusters:
                continue
            
            info_1 = cluster_info[cluster_id_1]
            
            # ç”»åƒã¨æ–‡ç« ã®ä¸¡æ–¹ã®å‡é›†åº¦ãŒé–¾å€¤ä»¥ä¸Šã€ã‹ã¤ä¸¡æ–¹ã®ä¸­å¿ƒãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ãƒãƒ¼ã‚¸ã‚’æ¤œè¨
            image_cohesion_ok = info_1['image_cohesion'] >= self.COHESION_THRESHOLD and info_1['image_center'] is not None
            sentence_cohesion_ok = info_1['sentence_cohesion'] >= self.COHESION_THRESHOLD and info_1['sentence_center'] is not None
            
            if not (image_cohesion_ok and sentence_cohesion_ok):
                merged[cluster_id_1] = info_1['sentence_ids']
                used_clusters.add(cluster_id_1)
                continue
            
            # ä»–ã®ã‚¯ãƒ©ã‚¹ã‚¿ã¨ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆç”»åƒã¨æ–‡ç« ã®ä¸¡æ–¹ï¼‰
            merge_candidates = [cluster_id_1]
            
            for j, cluster_id_2 in enumerate(cluster_ids[i+1:], start=i+1):
                if cluster_id_2 in used_clusters:
                    continue
                
                info_2 = cluster_info[cluster_id_2]
                
                # ä¸¡æ–¹ã®ã‚¯ãƒ©ã‚¹ã‚¿ãŒç”»åƒãƒ»æ–‡ç« ã¨ã‚‚ã«å‡é›†åº¦ãŒé«˜ãã€ä¸­å¿ƒãŒã‚ã‚‹å ´åˆã®ã¿ãƒãƒ¼ã‚¸ã‚’æ¤œè¨
                image_cohesion_ok_2 = info_2['image_cohesion'] >= self.COHESION_THRESHOLD and info_2['image_center'] is not None
                sentence_cohesion_ok_2 = info_2['sentence_cohesion'] >= self.COHESION_THRESHOLD and info_2['sentence_center'] is not None
                
                if not (image_cohesion_ok_2 and sentence_cohesion_ok_2):
                    continue
                
                # ã‚¯ãƒ©ã‚¹ã‚¿ä¸­å¿ƒé–“ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆç”»åƒã¨æ–‡ç« ã®ä¸¡æ–¹ï¼‰
                image_similarity = np.dot(info_1['image_center'], info_2['image_center']) / (
                    np.linalg.norm(info_1['image_center']) * np.linalg.norm(info_2['image_center'])
                )
                
                sentence_similarity = np.dot(info_1['sentence_center'], info_2['sentence_center']) / (
                    np.linalg.norm(info_1['sentence_center']) * np.linalg.norm(info_2['sentence_center'])
                )
                
                # ç”»åƒã¨æ–‡ç« ã®ä¸¡æ–¹ã®é¡ä¼¼åº¦ãŒé–¾å€¤ä»¥ä¸Šã®å ´åˆã®ã¿ãƒãƒ¼ã‚¸
                if image_similarity >= self.MERGE_SIMILARITY_THRESHOLD and sentence_similarity >= self.MERGE_SIMILARITY_THRESHOLD:
                    merge_candidates.append(cluster_id_2)
                    used_clusters.add(cluster_id_2)
                    print(f"    ãƒãƒ¼ã‚¸å€™è£œè¿½åŠ : ã‚¯ãƒ©ã‚¹ã‚¿{cluster_id_1} â† ã‚¯ãƒ©ã‚¹ã‚¿{cluster_id_2}")
                    print(f"      ç”»åƒå‡é›†åº¦: {info_1['image_cohesion']:.3f}, {info_2['image_cohesion']:.3f}")
                    print(f"      æ–‡ç« å‡é›†åº¦: {info_1['sentence_cohesion']:.3f}, {info_2['sentence_cohesion']:.3f}")
                    print(f"      ç”»åƒé¡ä¼¼åº¦: {image_similarity:.3f}, æ–‡ç« é¡ä¼¼åº¦: {sentence_similarity:.3f}")
            
            # ãƒãƒ¼ã‚¸å®Ÿè¡Œ
            if len(merge_candidates) > 1:
                print(f"  âœ… ãƒãƒ¼ã‚¸: {len(merge_candidates)}å€‹ã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’çµ±åˆ")
                print(f"     ç”»åƒå‡é›†åº¦: {info_1['image_cohesion']:.3f}, æ–‡ç« å‡é›†åº¦: {info_1['sentence_cohesion']:.3f}")
                merged_sentence_ids = []
                for cid in merge_candidates:
                    merged_sentence_ids.extend(cluster_info[cid]['sentence_ids'])
                merged[cluster_id_1] = merged_sentence_ids
            else:
                merged[cluster_id_1] = info_1['sentence_ids']
            
            used_clusters.add(cluster_id_1)
        
        # ãƒãƒ¼ã‚¸çµæœãŒç©ºã®å ´åˆã¯å…ƒã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’è¿”ã™
        if not merged:
            print(f"  è­¦å‘Š: ãƒãƒ¼ã‚¸çµæœãŒç©ºã§ã™ã€‚å…ƒã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’è¿”ã—ã¾ã™ã€‚")
            return clusters
        
        return merged
    
    def _merge_folders_by_name(self, folder_dict: dict) -> dict:
        """
        åŒã˜åå‰ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ã¾ã¨ã‚ã‚‹
        
        Args:
            folder_dict: ãƒ•ã‚©ãƒ«ãƒ€ã®è¾æ›¸ {folder_id: {data, is_leaf, name}}
            
        Returns:
            dict: åŒã˜åå‰ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ã¾ã¨ã‚ãŸè¾æ›¸
        """
        if not folder_dict:
            return folder_dict
            
        # åå‰ã”ã¨ã«ãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        name_groups = {}
        for folder_id, folder_info in folder_dict.items():
            folder_name = folder_info.get('name', folder_id)
            if folder_name not in name_groups:
                name_groups[folder_name] = []
            name_groups[folder_name].append((folder_id, folder_info))
        
        # åå‰ãŒé‡è¤‡ã—ã¦ã„ãªã„ãƒ•ã‚©ãƒ«ãƒ€ã¯ãã®ã¾ã¾ã€é‡è¤‡ã—ã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã¯ã¾ã¨ã‚ã‚‹
        merged_dict = {}
        for folder_name, folder_list in name_groups.items():
            if len(folder_list) == 1:
                # é‡è¤‡ãªã—ï¼šãã®ã¾ã¾è¿½åŠ 
                folder_id, folder_info = folder_list[0]
                merged_dict[folder_id] = folder_info
            else:
                # é‡è¤‡ã‚ã‚Šï¼šã¾ã¨ã‚ã‚‹
                print(f"åŒã˜åå‰ '{folder_name}' ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ {len(folder_list)} å€‹ã¾ã¨ã‚ã¾ã™")
                
                # æ–°ã—ã„ãƒ•ã‚©ãƒ«ãƒ€IDã‚’ç”Ÿæˆ
                merged_folder_id = Utils.generate_uuid()
                merged_data = {}
                all_is_leaf = True
                
                for folder_id, folder_info in folder_list:
                    if folder_info.get('is_leaf', False):
                        # ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰ã®å ´åˆï¼šdataã‚’ç›´æ¥ãƒãƒ¼ã‚¸
                        if isinstance(folder_info.get('data'), dict):
                            merged_data.update(folder_info['data'])
                    else:
                        # éãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰ã®å ´åˆï¼šå­ãƒ•ã‚©ãƒ«ãƒ€ã‚’ãƒãƒ¼ã‚¸
                        all_is_leaf = False
                        if isinstance(folder_info.get('data'), dict):
                            merged_data.update(folder_info['data'])
                
                merged_dict[merged_folder_id] = {
                    'data': merged_data,
                    'is_leaf': all_is_leaf,
                    'name': folder_name
                }
        
        return merged_dict

    def _add_parent_ids(self, clustering_dict: dict, parent_id: str = None) -> dict:
        """
        å…¨ã¦ã®è¦ç´ ã«parent_idã‚’è¿½åŠ ã™ã‚‹å†å¸°é–¢æ•°
        
        Args:
            clustering_dict: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã®è¾æ›¸
            parent_id: è¦ªè¦ç´ ã®IDï¼ˆæœ€ä¸Šä½éšå±¤ã®å ´åˆã¯Noneï¼‰
            
        Returns:
            parent_idãŒè¿½åŠ ã•ã‚ŒãŸè¾æ›¸
        """
        result = {}
        
        for key, value in clustering_dict.items():
            # å€¤ãŒè¾æ›¸ã®å ´åˆã®ã¿å‡¦ç†
            if isinstance(value, dict):
                # ç¾åœ¨ã®è¦ç´ ã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ
                new_value = value.copy()
                
                # parent_idã‚’è¿½åŠ 
                new_value['parent_id'] = parent_id
                
                # dataãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚‹å ´åˆã€å†å¸°çš„ã«å‡¦ç†
                if 'data' in new_value and isinstance(new_value['data'], dict):
                    new_value['data'] = self._add_parent_ids(new_value['data'], key)
                
                result[key] = new_value
            else:
                # æ–‡å­—åˆ—ã‚„ãã®ä»–ã®å€¤ã®å ´åˆã¯ãã®ã¾ã¾
                result[key] = value
        
        return result

    def _get_folder_name(self, captions: list[str], extra_stop_words: list[str]) -> str:
        """
        TF-IDFã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚©ãƒ«ãƒ€åã‚’æ±ºå®šã™ã‚‹ï¼ˆä¸Šä½3èªã¾ã§ã‚’ã‚«ãƒ³ãƒã§é€£çµï¼‰
        """
        if not captions:
            return Utils.generate_uuid()

        important_words = ClusteringUtils.get_tfidf_from_documents_array(
            documents=captions,
            max_words=3,
            extra_stop_words=extra_stop_words
        )

        if not important_words:
            return Utils.generate_uuid()

        # ä¸Šä½èªã‚’å–ã‚Šå‡ºã—ã€é †åºã‚’ä¿ã£ãŸã¾ã¾é‡è¤‡ã‚’é™¤å»
        seen = set()
        words = []
        for w, _ in important_words:
            if w and w not in seen:
                seen.add(w)
                words.append(w)

        # æœ€å¤§3èªã¾ã§ã‚«ãƒ³ãƒã§é€£çµ
        name = ",".join(words[:3])
        return name if name else Utils.generate_uuid()

    def _get_folder_name_with_sibling_comparison(
        self, 
        target_captions: list[str], 
        sibling_captions_list: list[list[str]], 
        extra_stop_words: list[str]
    ) -> str:
        """
        åŒéšå±¤ãƒ•ã‚©ãƒ«ãƒ€ã¨ã®æ¯”è¼ƒã‚’ç”¨ã„ã¦ãƒ•ã‚©ãƒ«ãƒ€åã‚’æ±ºå®šã™ã‚‹ï¼ˆç¶™ç¶šçš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        
        Args:
            target_captions: å¯¾è±¡ãƒ•ã‚©ãƒ«ãƒ€ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãƒªã‚¹ãƒˆ
            sibling_captions_list: åŒéšå±¤ã®ä»–ã®ãƒ•ã‚©ãƒ«ãƒ€ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãƒªã‚¹ãƒˆï¼ˆãƒªã‚¹ãƒˆã®ãƒªã‚¹ãƒˆï¼‰
            extra_stop_words: è¿½åŠ ã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰
            
        Returns:
            ãƒ•ã‚©ãƒ«ãƒ€åï¼ˆä¸Šä½3å˜èªã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰
        """
        import re
        from collections import Counter
        
        if not target_captions:
            return Utils.generate_uuid()
        
        # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚»ãƒƒãƒˆ
        stopwords_set = set(CAPTION_STOPWORDS + extra_stop_words)
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ•ã‚©ãƒ«ãƒ€ã®å˜èªã‚’æŠ½å‡ºï¼ˆæ–‡ã®ä½ç½®ã«ã‚ˆã‚‹ãƒã‚¤ã‚¢ã‚¹ä»˜ãï¼‰
        target_words = []
        for caption in target_captions:
            sentences = caption.split('.')
            for sentence_idx, sentence in enumerate(sentences):
                if not sentence.strip():
                    continue
                
                # æ–‡ã®ä½ç½®ã«ã‚ˆã‚‹é‡ã¿
                if sentence_idx == 0:
                    position_weight = 1.0
                elif sentence_idx == 1:
                    position_weight = 0.85
                elif sentence_idx == 2:
                    position_weight = 0.7
                else:
                    position_weight = 0.6
                
                words = re.findall(r'\b[a-z]+\b', sentence.lower())
                filtered_words = [w for w in words if w not in stopwords_set]
                
                for word in filtered_words:
                    target_words.append((word, position_weight))
        
        # é‡ã¿ä»˜ãã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        target_counter = {}
        for word, weight in target_words:
            target_counter[word] = target_counter.get(word, 0.0) + weight
        
        # å…„å¼Ÿãƒ•ã‚©ãƒ«ãƒ€ã®å˜èªã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ï¼ˆé‡ã¿ä»˜ãï¼‰
        sibling_counters = []
        for sibling_captions in sibling_captions_list:
            sibling_words = []
            for caption in sibling_captions:
                sentences = caption.split('.')
                for sentence_idx, sentence in enumerate(sentences):
                    if not sentence.strip():
                        continue
                    
                    if sentence_idx == 0:
                        position_weight = 1.0
                    elif sentence_idx == 1:
                        position_weight = 0.85
                    elif sentence_idx == 2:
                        position_weight = 0.7
                    else:
                        position_weight = 0.6
                    
                    words = re.findall(r'\b[a-z]+\b', sentence.lower())
                    filtered_words = [w for w in words if w not in stopwords_set]
                    
                    for word in filtered_words:
                        sibling_words.append((word, position_weight))
            
            sibling_counter = {}
            for word, weight in sibling_words:
                sibling_counter[word] = sibling_counter.get(word, 0.0) + weight
            
            sibling_counters.append(sibling_counter)
        
        # TF-IDFé¢¨ã‚¹ã‚³ã‚¢è¨ˆç®—
        word_scores = {}
        for word, count_in_target in target_counter.items():
            tf = count_in_target
            
            # ä»–ã®ãƒ•ã‚©ãƒ«ãƒ€ã§ã®å‡ºç¾å›æ•°ã®åˆè¨ˆ
            count_in_others = sum(
                counter.get(word, 0.0) for counter in sibling_counters
            )
            
            # ã‚¹ã‚³ã‚¢: tf * (tf / (count_in_others + 1))
            idf_like_score = tf / (count_in_others + 1.0)
            final_score = tf * idf_like_score
            
            word_scores[word] = final_score
        
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)
        
        # ä¸Šä½3å˜èªã‚’å–å¾—
        top_words = [word for word, score in sorted_words[:3]]
        
        if not top_words:
            return Utils.generate_uuid()
        
        return ",".join(top_words)

    def clustering_dummy(
        self, 
        sentence_name_db_data: dict[str, list],
        image_db_data: dict[str, list],
        clustering_id_dict: dict,
        sentence_id_dict: dict,
        image_id_dict: dict,
        cluster_num: int, 
        overall_folder_name: str = None,
        output_folder: bool = False, 
        output_json: bool = False
    ):
        """
        ãƒ€ãƒŸãƒ¼ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°é–¢æ•°ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        å¼•æ•°ã¨ã—ã¦æ¸¡ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ›ã™ã‚‹
        """
        print(f"\n{'='*80}")
        print(f"clustering_dummy å‘¼ã³å‡ºã— - å¼•æ•°ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›")
        print(f"{'='*80}\n")
        
        print(f"ğŸ“Š sentence_name_db_data:")
        print(f"  - idsæ•°: {len(sentence_name_db_data.get('ids', []))}")
        print(f"  - embeddingsæ•°: {len(sentence_name_db_data.get('embeddings', []))}")
        print(f"  - documentsæ•°: {len(sentence_name_db_data.get('documents', []))}")
        print(f"  - metadatasæ•°: {len(sentence_name_db_data.get('metadatas', []))}")
        if len(sentence_name_db_data.get('ids', [])) > 0:
            print(f"  - æœ€åˆã®id: {sentence_name_db_data['ids'][0]}")
            if len(sentence_name_db_data.get('metadatas', [])) > 0:
                metadata = sentence_name_db_data['metadatas'][0]
                print(f"  - æœ€åˆã®metadata.path: {metadata.path if hasattr(metadata, 'path') else metadata.get('path', 'N/A')}")
        
        print(f"\nğŸ“Š image_db_data:")
        print(f"  - idsæ•°: {len(image_db_data.get('ids', []))}")
        print(f"  - embeddingsæ•°: {len(image_db_data.get('embeddings', []))}")
        
        print(f"\nğŸ“Š clustering_id_dict:")
        print(f"  - è¦ç´ æ•°: {len(clustering_id_dict)}")
        print(f"  - æœ€åˆã®5ä»¶:")
        for i, (cid, info) in enumerate(list(clustering_id_dict.items())[:5]):
            print(f"    [{i+1}] {cid}: {info}")
        
        print(f"\nğŸ“Š sentence_id_dict:")
        print(f"  - è¦ç´ æ•°: {len(sentence_id_dict)}")
        print(f"  - æœ€åˆã®5ä»¶:")
        for i, (sid, info) in enumerate(list(sentence_id_dict.items())[:5]):
            print(f"    [{i+1}] {sid}: {info}")
        
        print(f"\nğŸ“Š image_id_dict:")
        print(f"  - è¦ç´ æ•°: {len(image_id_dict)}")
        print(f"  - æœ€åˆã®5ä»¶:")
        for i, (iid, info) in enumerate(list(image_id_dict.items())[:5]):
            print(f"    [{i+1}] {iid}: {info}")
        
        print(f"\nğŸ“Š ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
        print(f"  - cluster_num: {cluster_num}")
        print(f"  - overall_folder_name: {overall_folder_name}")
        print(f"  - output_folder: {output_folder}")
        print(f"  - output_json: {output_json}")
        
        print(f"\n{'='*80}")
        print(f"clustering_dummy å‡ºåŠ›çµ‚äº†")
        print(f"{'='*80}\n")
        
        # === ã‚¹ãƒ†ãƒƒãƒ—1: å…¨ç”»åƒã¨clustering_idã®dictã‚’ä½œæˆ ===
        print(f"\nğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ—1: å…¨ç”»åƒã¨clustering_idã®dictã‚’ä½œæˆ")
        print(f"{'='*80}\n")
        
        # clustering_idã‚’ã‚­ãƒ¼ã€ç”»åƒãƒ‘ã‚¹ã‚’å€¤ã¨ã™ã‚‹è¾æ›¸ã‚’ä½œæˆ
        clustering_id_to_path = {}
        
        # sentence_idã‹ã‚‰clustering_idã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å–å¾—
        for i, (sentence_id, metadata) in enumerate(zip(sentence_name_db_data.get('ids', []), sentence_name_db_data.get('metadatas', []))):
            # sentence_id_dictã‹ã‚‰clustering_idã‚’å–å¾—
            if sentence_id in sentence_id_dict:
                clustering_id = sentence_id_dict[sentence_id]['clustering_id']
                path = metadata.path if hasattr(metadata, 'path') else metadata.get('path', 'N/A')
                clustering_id_to_path[clustering_id] = path
                print(f"  [{i+1}] clustering_id: {clustering_id}")
                print(f"       path: {path}")
        
        print(f"\nğŸ“Š ä½œæˆå®Œäº†: {len(clustering_id_to_path)}å€‹ã®ãƒãƒƒãƒ”ãƒ³ã‚°")
        print(f"\nè¾æ›¸ã®æœ€åˆã®5ä»¶:")
        for i, (cid, path) in enumerate(list(clustering_id_to_path.items())[:5]):
            print(f"  {i+1}. '{cid}': '{path}'")
        
        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã®æ¥é ­è¾ã‚’æŠ½å‡ºã—ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–
        print(f"\nğŸ·ï¸  ç”»åƒã®ç¨®é¡ï¼ˆæ¥é ­è¾ï¼‰ã‚’æŠ½å‡º:")
        print(f"{'-'*80}")
        
        prefixes = set()
        for path in clustering_id_to_path.values():
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—
            import os
            filename = os.path.basename(path)
            # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢(_)ã§åˆ†å‰²ã—ã¦æœ€åˆã®éƒ¨åˆ†ã‚’æ¥é ­è¾ã¨ã—ã¦å–å¾—
            if '_' in filename:
                prefix = filename.split('_')[0]
                prefixes.add(prefix)
        
        # ã‚½ãƒ¼ãƒˆã—ã¦é…åˆ—ã¨ã—ã¦å‡ºåŠ›
        prefix_list = sorted(list(prefixes))
        print(f"\næ¥é ­è¾ã®é…åˆ—:")
        print(f"{prefix_list}")
        print(f"\nğŸ“Š åˆè¨ˆ: {len(prefix_list)}ç¨®é¡ã®ç”»åƒ")
        
        # === ã‚¹ãƒ†ãƒƒãƒ—2: ç‰©ä½“åã”ã¨ã«ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ ===
        print(f"\nğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ—2: ç‰©ä½“åã”ã¨ã«ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ")
        print(f"{'='*80}\n")
        
        leaf_folders = {}
        
        for prefix in prefix_list:
            folder_id = Utils.generate_uuid()
            folder_data = {}
            
            # ã“ã®æ¥é ­è¾ã‚’æŒã¤å…¨ã¦ã®ç”»åƒã‚’åé›†
            for clustering_id, path in clustering_id_to_path.items():
                import os
                filename = os.path.basename(path)
                if filename.startswith(prefix + '_'):
                    folder_data[clustering_id] = path
            
            if folder_data:  # ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
                leaf_folders[folder_id] = {
                    'data': folder_data,
                    'is_leaf': True,
                    'name': prefix
                }
                print(f"  ğŸ“ [{prefix}] ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ")
                print(f"     - folder_id: {folder_id}")
                print(f"     - ç”»åƒæ•°: {len(folder_data)}")
                print(f"     - is_leaf: True")
        
        print(f"\nğŸ“Š åˆè¨ˆ: {len(leaf_folders)}å€‹ã®ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ")
        
        # ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€ã®è©³ç´°ã‚’å‡ºåŠ›
        print(f"\nğŸ“¦ ä½œæˆã•ã‚ŒãŸãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€ã®è©³ç´°:")
        print(f"{'-'*80}")
        for i, (folder_id, folder_info) in enumerate(leaf_folders.items(), 1):
            print(f"\n  {i}. ãƒ•ã‚©ãƒ«ãƒ€å: '{folder_info['name']}'")
            print(f"     folder_id: {folder_id}")
            print(f"     is_leaf: {folder_info['is_leaf']}")
            print(f"     ç”»åƒæ•°: {len(folder_info['data'])}")
            print(f"     data: {{")
            for j, (cid, path) in enumerate(list(folder_info['data'].items())[:3], 1):
                print(f"       '{cid}': '{path}'")
                if j >= 3 and len(folder_info['data']) > 3:
                    print(f"       ... ({len(folder_info['data']) - 3}ä»¶çœç•¥)")
                    break
            print(f"     }}")
        
        # === ã‚¹ãƒ†ãƒƒãƒ—3: å„ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚©ãƒ«ãƒ€ã§ãƒ©ãƒƒãƒ”ãƒ³ã‚° ===
        print(f"\nğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ—3: å„ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚©ãƒ«ãƒ€ã§ãƒ©ãƒƒãƒ”ãƒ³ã‚°")
        print(f"{'='*80}\n")
        
        category_folders = {}
        
        for leaf_folder_id, leaf_folder_info in leaf_folders.items():
            # å„ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€ã«å¯¾ã—ã¦ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ
            category_folder_id = Utils.generate_uuid()
            category_name = leaf_folder_info['name']  # ãƒªãƒ¼ãƒ•ã¨åŒã˜åå‰ã‚’ä½¿ç”¨
            
            category_folders[category_folder_id] = {
                'data': {
                    leaf_folder_id: leaf_folder_info
                },
                'is_leaf': False,
                'name': category_name
            }
            
            print(f"  ğŸ“‚ ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ: '{category_name}'")
            print(f"     - category_folder_id: {category_folder_id}")
            print(f"     - is_leaf: False")
            print(f"     - å­ãƒ•ã‚©ãƒ«ãƒ€æ•°: 1")
            print(f"     - å­ãƒ•ã‚©ãƒ«ãƒ€: {leaf_folder_id} (ãƒªãƒ¼ãƒ•)")
        
        print(f"\nğŸ“Š åˆè¨ˆ: {len(category_folders)}å€‹ã®ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ")
        print(f"ğŸ“Š ç·ãƒ•ã‚©ãƒ«ãƒ€æ•°: {len(leaf_folders)} (ãƒªãƒ¼ãƒ•) + {len(category_folders)} (ã‚«ãƒ†ã‚´ãƒª) = {len(leaf_folders) + len(category_folders)}å€‹")
        
        # æ§‹é€ ã®è©³ç´°ã‚’å‡ºåŠ›
        print(f"\nğŸ“¦ æœ€çµ‚çš„ãªãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ :")
        print(f"{'-'*80}")
        for i, (cat_folder_id, cat_folder_info) in enumerate(category_folders.items(), 1):
            print(f"\n  {i}. ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚©ãƒ«ãƒ€: '{cat_folder_info['name']}'")
            print(f"     - folder_id: {cat_folder_id}")
            print(f"     - is_leaf: {cat_folder_info['is_leaf']}")
            print(f"     - å­ãƒ•ã‚©ãƒ«ãƒ€:")
            for leaf_id, leaf_info in cat_folder_info['data'].items():
                print(f"       â””â”€ ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€: '{leaf_info['name']}'")
                print(f"          - folder_id: {leaf_id}")
                print(f"          - is_leaf: {leaf_info['is_leaf']}")
                print(f"          - ç”»åƒæ•°: {len(leaf_info['data'])}")
        
        print(f"\n{'='*80}\n")
        
        # === ã‚¹ãƒ†ãƒƒãƒ—4: ãƒˆãƒƒãƒ—ãƒ•ã‚©ãƒ«ãƒ€ã§ãƒ©ãƒƒãƒ—ã—ã¦parent_idã‚’è¿½åŠ  ===
        print(f"\nğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ—4: ãƒˆãƒƒãƒ—ãƒ•ã‚©ãƒ«ãƒ€ã§ãƒ©ãƒƒãƒ—")
        print(f"{'='*80}\n")
        
        top_folder_id = Utils.generate_uuid()
        display_name = overall_folder_name if overall_folder_name else "ãƒ€ãƒŸãƒ¼éšå±¤åˆ†é¡"
        
        wrapped_result = {
            top_folder_id: {
                "data": category_folders,
                "parent_id": None,
                "is_leaf": False,
                "name": display_name
            }
        }
        
        print(f"ğŸ“¦ ãƒˆãƒƒãƒ—ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ:")
        print(f"   - ID: {top_folder_id}")
        print(f"   - Name: {display_name}")
        print(f"   - å­ãƒ•ã‚©ãƒ«ãƒ€æ•°: {len(category_folders)}")
        
        # parent_idã‚’è¿½åŠ 
        wrapped_result = self._add_parent_ids(wrapped_result)
        print(f"âœ… parent_idè¿½åŠ å®Œäº†")
        
        # === ã‚¹ãƒ†ãƒƒãƒ—5: all_nodesç”Ÿæˆ ===
        print(f"\nğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ—5: all_nodesç”Ÿæˆ")
        print(f"{'='*80}\n")
        
        all_nodes, folder_nodes, file_nodes = self.create_all_nodes(wrapped_result)
        
        print(f"âœ… all_nodesç”Ÿæˆå®Œäº†:")
        print(f"   - ãƒ•ã‚©ãƒ«ãƒ€ãƒãƒ¼ãƒ‰æ•°: {len(folder_nodes)}")
        print(f"   - ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ¼ãƒ‰æ•°: {len(file_nodes)}")
        print(f"   - åˆè¨ˆãƒãƒ¼ãƒ‰æ•°: {len(all_nodes)}")
        
        print(f"\nğŸ“Š æƒ³å®šãƒãƒ¼ãƒ‰æ•°:")
        print(f"   - ãƒˆãƒƒãƒ—ãƒ•ã‚©ãƒ«ãƒ€: 1")
        print(f"   - ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚©ãƒ«ãƒ€: {len(category_folders)}")
        print(f"   - ãƒªãƒ¼ãƒ•ãƒ•ã‚©ãƒ«ãƒ€: {len(leaf_folders)}")
        print(f"   - ãƒ•ã‚¡ã‚¤ãƒ«: {len(clustering_id_to_path)}")
        print(f"   - åˆè¨ˆæƒ³å®š: {1 + len(category_folders) + len(leaf_folders) + len(clustering_id_to_path)}")
        
        result_dict = category_folders
        
        print(f"\nğŸ“„ wrapped_result ã®å®Œå…¨ãªå†…å®¹:")
        print(f"{'-'*80}")
        import json
        print(json.dumps(wrapped_result, indent=2, ensure_ascii=False, default=str))
        
        print(f"\nğŸ“„ all_nodes ã®å®Œå…¨ãªå†…å®¹ (æœ€åˆã®10ä»¶):")
        print(f"{'-'*80}")
        print(json.dumps(all_nodes[:10], indent=2, ensure_ascii=False, default=str))
        if len(all_nodes) > 10:
            print(f"\n... æ®‹ã‚Š {len(all_nodes) - 10} ä»¶çœç•¥")
        
        print(f"\n{'='*80}\n")
        
        return wrapped_result, all_nodes

    def clustering(
        self, 
        sentence_name_db_data: dict[str, list],
        image_db_data: dict[str, list],
        clustering_id_dict: dict,
        sentence_id_dict: dict,
        image_id_dict: dict,
        cluster_num: int, 
        overall_folder_name: str = None,
        output_folder: bool = False, 
        output_json: bool = False
    ):
        """
        æ–°ã—ã„3æ®µéšã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
        1. captionå…¨ä½“(usage + category)ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        2. usage + category ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°  
        3. name ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        å„æ®µéšã§å‡é›†åº¦ãƒ™ãƒ¼ã‚¹ã®ãƒãƒ¼ã‚¸ã‚’å®Ÿè¡Œ
        """
        
        print(f"=== æ–°3æ®µéšã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ é–‹å§‹ ===")
        print(f"Total documents: {len(sentence_name_db_data['ids'])}")
        
        # JSONå‡ºåŠ›ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¾ãŸã¯ãƒ•ã‚©ãƒ«ãƒ€å‡ºåŠ›ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒTrueã®å ´åˆã€output_base_pathã‚’ã‚¯ãƒªã‚¢ã—ã¦æ–°ãŸã«ä½œæˆ
        if output_json or output_folder:
            if self.output_base_path.exists():
                shutil.rmtree(self.output_base_path)
                os.makedirs(self.output_base_path, exist_ok=True)
        
        # ç”»åƒåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®è¾æ›¸ã‚’ä½œæˆï¼ˆsentence_id -> image_embeddingï¼‰
        image_embeddings_dict = {}
        for sentence_id in sentence_id_dict.keys():
            clustering_id = sentence_id_dict[sentence_id]['clustering_id']
            # clustering_idã‹ã‚‰image_idã‚’å–å¾—
            for cid, ids_dict in clustering_id_dict.items():
                if cid == clustering_id and 'image_id' in ids_dict:
                    image_id = ids_dict['image_id']
                    # image_db_dataã‹ã‚‰embeddingã‚’å–å¾—
                    for i, iid in enumerate(image_db_data['ids']):
                        if iid == image_id:
                            image_embeddings_dict[sentence_id] = image_db_data['embeddings'][i]
                            break
                    break
        
        # ========================================
        # ç¬¬1æ®µéš: captionå…¨ä½“ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° (usage + category)
        # ========================================
        print("\nã€ç¬¬1æ®µéšã€‘captionå…¨ä½“ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°")
        
        # usage + categoryã®åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—ï¼ˆ2æ–‡ç›®ã¨3æ–‡ç›®ï¼‰
        usage_data = self._sentence_usage_db.get_data_by_sentence_ids(sentence_id_dict.keys())
        category_data = self._sentence_category_db.get_data_by_sentence_ids(sentence_id_dict.keys())
        
        # usage + categoryã®åŸ‹ã‚è¾¼ã¿ã‚’çµåˆ
        combined_embeddings = []
        for i in range(len(usage_data['embeddings'])):
            combined = np.concatenate([usage_data['embeddings'][i], category_data['embeddings'][i]])
            combined_embeddings.append(combined)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’æ±ºå®š
        overall_cluster_num, _ = self.get_optimal_cluster_num(
            embeddings=combined_embeddings, 
            min_cluster_num=2, 
            max_cluster_num=min(15, len(sentence_id_dict.keys())//3)
        )
        
        print(f"  captionå…¨ä½“ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {overall_cluster_num}")
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
        if overall_cluster_num <= 1:
            overall_clusters = {0: list(sentence_id_dict.keys())}
        else:
            embeddings_array = np.array([emb.tolist() if hasattr(emb, 'tolist') else emb for emb in combined_embeddings])
            
            clustering = AgglomerativeClustering(
                n_clusters=overall_cluster_num,
                metric='cosine',
                linkage='average'
            )
            labels = clustering.fit_predict(embeddings_array)
            
            # ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
            if len(set(labels)) > 1:
                silhouette_avg = silhouette_score(embeddings_array, labels, metric='cosine')
                print(f"  éšå±¤å‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ: ã‚¯ãƒ©ã‚¹ã‚¿æ•°={overall_cluster_num}, ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢={silhouette_avg:.4f}")
            
            overall_clusters = {}
            for cluster_idx in range(overall_cluster_num):
                cluster_indices = np.where(labels == cluster_idx)[0]
                overall_clusters[cluster_idx] = [usage_data['ids'][i] for i in cluster_indices]
        
        # å‡é›†åº¦ãƒ™ãƒ¼ã‚¹ã®ãƒãƒ¼ã‚¸
        print(f"  ãƒãƒ¼ã‚¸å‰ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {len(overall_clusters)}")
        overall_clusters = self._merge_similar_clusters(overall_clusters, combined_embeddings, image_embeddings_dict)
        print(f"  ãƒãƒ¼ã‚¸å¾Œã‚¯ãƒ©ã‚¹ã‚¿æ•°: {len(overall_clusters)}")
        
        # è¦ç´ æ•°1ã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’çµ±åˆ
        overall_clusters = self._merge_singleton_clusters(overall_clusters, image_embeddings_dict)
        print(f"  ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³çµ±åˆå¾Œã‚¯ãƒ©ã‚¹ã‚¿æ•°: {len(overall_clusters)}")
        
        # ========================================
        # å„captionå…¨ä½“ã‚¯ãƒ©ã‚¹ã‚¿ã«å¯¾ã—ã¦ç¬¬2æ®µéšãƒ»ç¬¬3æ®µéšã‚’å®Ÿè¡Œ
        # ========================================
        overall_result_dict = {}
        
        # å…¨ã¦ã® captionå…¨ä½“ã‚¯ãƒ©ã‚¹ã‚¿ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’äº‹å‰ã«åé›†ï¼ˆå…„å¼Ÿãƒ•ã‚©ãƒ«ãƒ€ã¨ã®å·®åˆ¥åŒ–ç”¨ï¼‰
        all_overall_captions_by_cluster = {}
        for overall_idx, sentence_ids_in_overall in overall_clusters.items():
            overall_captions = []
            for sentence_id in sentence_ids_in_overall:
                for i, sid in enumerate(usage_data['ids']):
                    if sid == sentence_id:
                        overall_captions.append(f"{usage_data['documents'][i].document} {category_data['documents'][i].document}")
                        break
            all_overall_captions_by_cluster[overall_idx] = overall_captions
        
        for overall_idx, sentence_ids_in_overall in overall_clusters.items():
            overall_folder_id = Utils.generate_uuid()
            
            # captionå…¨ä½“ãƒ•ã‚©ãƒ«ãƒ€åã‚’å…„å¼Ÿãƒ•ã‚©ãƒ«ãƒ€ã¨ã®å·®åˆ¥åŒ–ã‚’è€ƒæ…®ã—ã¦æ±ºå®š
            target_captions = all_overall_captions_by_cluster[overall_idx]
            sibling_captions_list = [captions for idx, captions in all_overall_captions_by_cluster.items() if idx != overall_idx]
            
            if len(sibling_captions_list) > 0:
                overall_folder_name_tfidf = self._get_folder_name_with_sibling_comparison(
                    target_captions, 
                    sibling_captions_list, 
                    ['object','main','its','used'] + MAJOR_COLORS + MAJOR_SHAPES
                )
            else:
                overall_folder_name_tfidf = self._get_folder_name(target_captions, ['object','main','its','used'] + MAJOR_COLORS + MAJOR_SHAPES)
            
            print(f"\nã€ç¬¬2æ®µéšã€‘usage+categoryã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° (å…¨ä½“ã‚¯ãƒ©ã‚¹ã‚¿ {overall_idx}: {overall_folder_name_tfidf})")
            
            # ========================================
            # ç¬¬2æ®µéš: usage + category ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
            # ========================================
            usage_category_data = self._sentence_usage_db.get_data_by_sentence_ids(sentence_ids_in_overall)
            usage_category_cat_data = self._sentence_category_db.get_data_by_sentence_ids(sentence_ids_in_overall)
            
            # usage + categoryã®åŸ‹ã‚è¾¼ã¿ã‚’çµåˆ
            usage_category_embeddings = []
            for i in range(len(usage_category_data['embeddings'])):
                combined = np.concatenate([usage_category_data['embeddings'][i], usage_category_cat_data['embeddings'][i]])
                usage_category_embeddings.append(combined)
            
            usage_category_cluster_num, _ = self.get_optimal_cluster_num(
                embeddings=usage_category_embeddings, 
                min_cluster_num=2, 
                max_cluster_num=min(10, len(sentence_ids_in_overall)//2)
            )
            
            print(f"  usage+categoryã‚¯ãƒ©ã‚¹ã‚¿æ•°: {usage_category_cluster_num}")
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
            if usage_category_cluster_num <= 1:
                usage_category_clusters = {0: sentence_ids_in_overall}
            else:
                embeddings_array = np.array([emb.tolist() if hasattr(emb, 'tolist') else emb for emb in usage_category_embeddings])
                
                clustering = AgglomerativeClustering(
                    n_clusters=usage_category_cluster_num,
                    metric='cosine',
                    linkage='average'
                )
                labels = clustering.fit_predict(embeddings_array)
                
                # ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                if len(set(labels)) > 1:
                    silhouette_avg = silhouette_score(embeddings_array, labels, metric='cosine')
                    print(f"  éšå±¤å‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ: ã‚¯ãƒ©ã‚¹ã‚¿æ•°={usage_category_cluster_num}, ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢={silhouette_avg:.4f}")
                
                usage_category_clusters = {}
                for cluster_idx in range(usage_category_cluster_num):
                    cluster_indices = np.where(labels == cluster_idx)[0]
                    usage_category_clusters[cluster_idx] = [usage_category_data['ids'][i] for i in cluster_indices]
            
            # å‡é›†åº¦ãƒ™ãƒ¼ã‚¹ã®ãƒãƒ¼ã‚¸
            print(f"  ãƒãƒ¼ã‚¸å‰ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {len(usage_category_clusters)}")
            usage_category_clusters = self._merge_similar_clusters(usage_category_clusters, usage_category_embeddings, image_embeddings_dict)
            print(f"  ãƒãƒ¼ã‚¸å¾Œã‚¯ãƒ©ã‚¹ã‚¿æ•°: {len(usage_category_clusters)}")
            
            # è¦ç´ æ•°1ã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’çµ±åˆ
            usage_category_clusters = self._merge_singleton_clusters(usage_category_clusters, image_embeddings_dict)
            print(f"  ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³çµ±åˆå¾Œã‚¯ãƒ©ã‚¹ã‚¿æ•°: {len(usage_category_clusters)}")
            
            # ========================================
            # å„usage+categoryã‚¯ãƒ©ã‚¹ã‚¿ã«å¯¾ã—ã¦ç¬¬3æ®µéšã‚’å®Ÿè¡Œ
            # ========================================
            usage_category_result_dict = {}
            
            # åŒéšå±¤ãƒ•ã‚©ãƒ«ãƒ€æ¯”è¼ƒã®ãŸã‚ã€å…¨ã‚¯ãƒ©ã‚¹ã‚¿ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å…ˆã«åé›†
            all_usage_category_clusters_captions = []
            for usage_category_idx, sentence_ids_in_usage_category in usage_category_clusters.items():
                cluster_captions = []
                for sentence_id in sentence_ids_in_usage_category:
                    for i, sid in enumerate(usage_category_data['ids']):
                        if sid == sentence_id:
                            cluster_captions.append(f"{usage_category_data['documents'][i].document} {usage_category_cat_data['documents'][i].document}")
                            break
                all_usage_category_clusters_captions.append(cluster_captions)
            
            # å„usage+categoryã‚¯ãƒ©ã‚¹ã‚¿ã®ãƒ•ã‚©ãƒ«ãƒ€åã‚’ç”Ÿæˆãƒ»å‡¦ç†
            for cluster_idx, (usage_category_idx, sentence_ids_in_usage_category) in enumerate(usage_category_clusters.items()):
                usage_category_folder_id = Utils.generate_uuid()
                
                # usage+categoryãƒ•ã‚©ãƒ«ãƒ€åã‚’æ±ºå®šï¼ˆåŒéšå±¤ã®ä»–ã‚¯ãƒ©ã‚¹ã‚¿ã¨æ¯”è¼ƒï¼‰
                usage_category_captions = all_usage_category_clusters_captions[cluster_idx]
                sibling_captions = [all_usage_category_clusters_captions[i] for i in range(len(all_usage_category_clusters_captions)) if i != cluster_idx]
                
                if len(sibling_captions) > 0:
                    # åŒéšå±¤ãƒ•ã‚©ãƒ«ãƒ€æ¯”è¼ƒã‚’ä½¿ç”¨
                    usage_category_folder_name = self._get_folder_name_with_sibling_comparison(
                        usage_category_captions,
                        sibling_captions,
                        ['object','main','its','used'] + MAJOR_COLORS + MAJOR_SHAPES
                    )
                    print(f"    ğŸ“ usage+categoryãƒ•ã‚©ãƒ«ãƒ€åç”Ÿæˆï¼ˆåŒéšå±¤æ¯”è¼ƒï¼‰: '{usage_category_folder_name}' (ID: {usage_category_folder_id})")
                else:
                    # 1ã¤ã—ã‹ãªã„å ´åˆã¯é€šå¸¸ã®TF-IDF
                    usage_category_folder_name = self._get_folder_name(usage_category_captions, ['object','main','its','used'] + MAJOR_COLORS + MAJOR_SHAPES)
                    print(f"    ğŸ“ usage+categoryãƒ•ã‚©ãƒ«ãƒ€åç”Ÿæˆï¼ˆTF-IDFï¼‰: '{usage_category_folder_name}' (ID: {usage_category_folder_id})")
                
                print(f"\n  ã€ç¬¬3æ®µéšã€‘nameã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° (usage+categoryã‚¯ãƒ©ã‚¹ã‚¿ {usage_category_idx}: {usage_category_folder_name})")
                
                # ========================================
                # ç¬¬3æ®µéš: name ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
                # ========================================
                name_data = self._sentence_name_db.get_data_by_sentence_ids(sentence_ids_in_usage_category)
                
                name_cluster_num, _ = self.get_optimal_cluster_num(
                    embeddings=name_data['embeddings'], 
                    min_cluster_num=2, 
                    max_cluster_num=min(10, len(sentence_ids_in_usage_category)//2)
                )
                
                print(f"    nameã‚¯ãƒ©ã‚¹ã‚¿æ•°: {name_cluster_num}")
                
                # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
                if name_cluster_num <= 1:
                    name_clusters = {0: sentence_ids_in_usage_category}
                else:
                    embeddings_array = np.array([emb.tolist() if hasattr(emb, 'tolist') else emb for emb in name_data['embeddings']])
                    
                    clustering = AgglomerativeClustering(
                        n_clusters=name_cluster_num,
                        metric='cosine',
                        linkage='average'
                    )
                    labels = clustering.fit_predict(embeddings_array)
                    
                    # ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                    if len(set(labels)) > 1:
                        silhouette_avg = silhouette_score(embeddings_array, labels, metric='cosine')
                        print(f"    éšå±¤å‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœ: ã‚¯ãƒ©ã‚¹ã‚¿æ•°={name_cluster_num}, ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢={silhouette_avg:.4f}")
                    
                    name_clusters = {}
                    for cluster_idx in range(name_cluster_num):
                        cluster_indices = np.where(labels == cluster_idx)[0]
                        name_clusters[cluster_idx] = [name_data['ids'][i] for i in cluster_indices]
                
                # å‡é›†åº¦ãƒ™ãƒ¼ã‚¹ã®ãƒãƒ¼ã‚¸
                print(f"    ãƒãƒ¼ã‚¸å‰ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {len(name_clusters)}")
                name_clusters = self._merge_similar_clusters(name_clusters, name_data['embeddings'], image_embeddings_dict)
                print(f"    ãƒãƒ¼ã‚¸å¾Œã‚¯ãƒ©ã‚¹ã‚¿æ•°: {len(name_clusters)}")
                
                # è¦ç´ æ•°1ã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’çµ±åˆ
                name_clusters = self._merge_singleton_clusters(name_clusters, image_embeddings_dict)
                print(f"    ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³çµ±åˆå¾Œã‚¯ãƒ©ã‚¹ã‚¿æ•°: {len(name_clusters)}")
                
                # ========================================
                # ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰ä½œæˆ
                # ========================================
                name_result_dict = {}
                
                # åŒéšå±¤ãƒ•ã‚©ãƒ«ãƒ€æ¯”è¼ƒã®ãŸã‚ã€å…¨ã‚¯ãƒ©ã‚¹ã‚¿ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å…ˆã«åé›†
                all_name_clusters_captions = []
                for name_idx, sentence_ids_in_name in name_clusters.items():
                    cluster_captions = []
                    for sentence_id in sentence_ids_in_name:
                        for i, sid in enumerate(sentence_name_db_data['ids']):
                            if sid == sentence_id:
                                cluster_captions.append(sentence_name_db_data['documents'][i].document)
                                break
                    all_name_clusters_captions.append(cluster_captions)
                
                # å„nameã‚¯ãƒ©ã‚¹ã‚¿ã®ãƒ•ã‚©ãƒ«ãƒ€åã‚’ç”Ÿæˆ
                for cluster_idx, (name_idx, sentence_ids_in_name) in enumerate(name_clusters.items()):
                    name_folder_id = Utils.generate_uuid()
                    
                    # å¯¾å¿œã™ã‚‹clustering_idã¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—
                    leaf_data = {}
                    leaf_captions = []
                    
                    for sentence_id in sentence_ids_in_name:
                        if sentence_id in sentence_id_dict:
                            clustering_id = sentence_id_dict[sentence_id]['clustering_id']
                            
                            # metadataã‚’å–å¾—
                            for i, sid in enumerate(sentence_name_db_data['ids']):
                                if sid == sentence_id:
                                    leaf_data[clustering_id] = sentence_name_db_data['metadatas'][i].path
                                    leaf_captions.append(sentence_name_db_data['documents'][i].document)
                                    break
                    
                    # nameãƒ•ã‚©ãƒ«ãƒ€åã‚’æ±ºå®šï¼ˆåŒéšå±¤ã®ä»–ã‚¯ãƒ©ã‚¹ã‚¿ã¨æ¯”è¼ƒï¼‰
                    sibling_captions = [all_name_clusters_captions[i] for i in range(len(all_name_clusters_captions)) if i != cluster_idx]
                    
                    if len(sibling_captions) > 0:
                        # åŒéšå±¤ãƒ•ã‚©ãƒ«ãƒ€æ¯”è¼ƒã‚’ä½¿ç”¨ï¼ˆCAPTION_STOPWORDSã®ã¿ã§ã€è‰²ãƒ»å½¢çŠ¶ã¯ä¿æŒï¼‰
                        name_folder_name = self._get_folder_name_with_sibling_comparison(
                            leaf_captions, 
                            sibling_captions, 
                            []  # è¿½åŠ ã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ãªã—ï¼ˆè‰²ãƒ»å½¢çŠ¶ã‚’æ®‹ã™ï¼‰
                        )
                        print(f"      ğŸ“ nameãƒ•ã‚©ãƒ«ãƒ€åç”Ÿæˆï¼ˆåŒéšå±¤æ¯”è¼ƒï¼‰: '{name_folder_name}' (ID: {name_folder_id})")
                    else:
                        # 1ã¤ã—ã‹ãªã„å ´åˆã¯é€šå¸¸ã®TF-IDFï¼ˆè‰²ãƒ»å½¢çŠ¶ã‚’æ®‹ã™ï¼‰
                        name_folder_name = self._get_folder_name(leaf_captions, [])
                        print(f"      ğŸ“ nameãƒ•ã‚©ãƒ«ãƒ€åç”Ÿæˆï¼ˆTF-IDFï¼‰: '{name_folder_name}' (ID: {name_folder_id})")
                    
                    name_result_dict[name_folder_id] = {
                        'data': leaf_data,
                        'is_leaf': True,
                        'name': name_folder_name
                    }
                
                # åŒã˜åå‰ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ã¾ã¨ã‚ã‚‹
                print(f"    ğŸ”„ nameéšå±¤ãƒ•ã‚©ãƒ«ãƒ€ãƒãƒ¼ã‚¸å‰: {len(name_result_dict)}å€‹")
                name_result_dict = self._merge_folders_by_name(name_result_dict)
                print(f"    âœ… nameéšå±¤ãƒ•ã‚©ãƒ«ãƒ€ãƒãƒ¼ã‚¸å¾Œ: {len(name_result_dict)}å€‹")
                
                # usage+categoryãƒ•ã‚©ãƒ«ãƒ€ã«è¿½åŠ 
                usage_category_result_dict[usage_category_folder_id] = {
                    'data': name_result_dict,
                    'is_leaf': False,
                    'name': usage_category_folder_name
                }
            
            # åŒã˜åå‰ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ã¾ã¨ã‚ã‚‹
            print(f"  ğŸ”„ usage+categoryéšå±¤ãƒ•ã‚©ãƒ«ãƒ€ãƒãƒ¼ã‚¸å‰: {len(usage_category_result_dict)}å€‹")
            usage_category_result_dict = self._merge_folders_by_name(usage_category_result_dict)
            print(f"  âœ… usage+categoryéšå±¤ãƒ•ã‚©ãƒ«ãƒ€ãƒãƒ¼ã‚¸å¾Œ: {len(usage_category_result_dict)}å€‹")
            
            # captionå…¨ä½“ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¦usage+categoryãƒ•ã‚©ãƒ«ãƒ€ã‚’ãã®ä¸‹ã«é…ç½®
            overall_result_dict[overall_folder_id] = {
                'data': usage_category_result_dict,
                'is_leaf': False,
                'name': overall_folder_name_tfidf
            }
        
        # åŒã˜åå‰ã®ãƒ•ã‚©ãƒ«ãƒ€ã‚’ã¾ã¨ã‚ã‚‹
        print(f"\nğŸ”„ ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ãƒ•ã‚©ãƒ«ãƒ€ãƒãƒ¼ã‚¸å‰: {len(overall_result_dict)}å€‹")
        overall_result_dict = self._merge_folders_by_name(overall_result_dict)
        print(f"âœ… ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ãƒ•ã‚©ãƒ«ãƒ€ãƒãƒ¼ã‚¸å¾Œ: {len(overall_result_dict)}å€‹")
        
        # ãƒãƒ¼ã‚¸å¾Œã®ãƒ•ã‚©ãƒ«ãƒ€åã‚’ç¢ºèª
        print(f"\nğŸ“‹ æœ€çµ‚çš„ãªãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ãƒ•ã‚©ãƒ«ãƒ€ä¸€è¦§:")
        for folder_id, folder_data in overall_result_dict.items():
            folder_name = folder_data.get('name', folder_id)
            is_leaf = folder_data.get('is_leaf', False)
            child_count = len(folder_data.get('data', {}))
            print(f"   - {folder_name} (ID: {folder_id}, is_leaf: {is_leaf}, å­è¦ç´ æ•°: {child_count})")
        
        result_clustering_uuid_dict = overall_result_dict
                
        #ãƒ•ã‚©ãƒ«ãƒ€å‡ºåŠ›ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒTrueã®æ™‚ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã‚’ãƒ•ã‚©ãƒ«ãƒ€ã¨ã—ã¦å‡ºåŠ›
        if output_folder:
            def copy_tree(node: dict, output_path: Path, images_folder_path: Path):
                os.makedirs(output_path, exist_ok=True)
                
                if node['is_leaf']:
                    for filename in node['data'].values():
                        src = images_folder_path / Path(filename)
                        dst = output_path / Path(filename)
                        shutil.copy(src, dst)
                else:
                    for folder_id, child_node in node['data'].items():
                        child_output_path = output_path / Path(folder_id)
                        copy_tree(child_node, child_output_path, images_folder_path)
                        
            for _folder_id, value in result_clustering_uuid_dict.items():
                output_path = self.output_base_path / Path(_folder_id)
                copy_tree(value, output_path, self.images_folder_path)
                
        # å…¨ä½“ã‚’ã¾ã¨ã‚ãŸãƒ•ã‚©ãƒ«ãƒ€è¦ç´ ã§ãƒ©ãƒƒãƒ—
        top_folder_id = Utils.generate_uuid()
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦top_folder_idã‚’ä½¿ç”¨
        display_name = overall_folder_name if overall_folder_name else top_folder_id
        
        print(f"\nğŸ“¦ ãƒˆãƒƒãƒ—ãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ:")
        print(f"   - ID: {top_folder_id}")
        print(f"   - Name: {display_name}")
        print(f"   - å­ãƒ•ã‚©ãƒ«ãƒ€æ•°: {len(result_clustering_uuid_dict)}")
        
        # å…¨ä½“ãƒ•ã‚©ãƒ«ãƒ€ã§ãƒ©ãƒƒãƒ—ã—ã¦ã‹ã‚‰parent_idã‚’è¿½åŠ 
        wrapped_result = {
            top_folder_id: {
                "data": result_clustering_uuid_dict,
                "parent_id": None,
                "is_leaf": False,
                "name": display_name
            }
        }
        
        # å…¨ä½“ãƒ•ã‚©ãƒ«ãƒ€ã§ãƒ©ãƒƒãƒ—ã—ãŸå¾Œã«parent_idã‚’è¿½åŠ 
        wrapped_result = self._add_parent_ids(wrapped_result)
        
        print(f"\nğŸ” all_nodesç”Ÿæˆé–‹å§‹...")
        #mongodbã«ç™»éŒ²ã™ã‚‹ãŸã‚ã®nodeæƒ…å ±ã‚’ä½œæˆã™ã‚‹
        all_nodes,_, _ = self.create_all_nodes(wrapped_result)
        print(f"âœ… all_nodesç”Ÿæˆå®Œäº†: {len(all_nodes)}å€‹ã®ãƒãƒ¼ãƒ‰")
        
        
        if output_json:
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
            os.makedirs(self._output_base_path, exist_ok=True)
            
            output_json_path = self._output_base_path / "result.json"
            with open(output_json_path, "w", encoding="utf-8") as f:
                json.dump(wrapped_result, f, ensure_ascii=False, indent=2)  
            
            output_json_path= self._output_base_path / "all_nodes.json"
            with open(output_json_path, "w", encoding="utf-8") as f:
                json.dump(all_nodes, f, ensure_ascii=False, indent=2)
                
        print(f"\n=== ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Œäº† ===")
        return wrapped_result,all_nodes
    
    def create_folder_nodes(self,data, parent_id=None, result=None):
        """
        ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ãƒãƒ¼ãƒ‰ã‚’ä½œæˆã™ã‚‹
        
        Args:
            data: JSONãƒ‡ãƒ¼ã‚¿
            parent_id: è¦ªãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ã®IDï¼ˆæœ€åˆã®ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ã¯nullï¼‰
            result: çµæœã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
        
        Returns:
            list: ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ãƒãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
        """
        if result is None:
            result = []
        
        for folder_id, folder_info in data.items():
            # ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ãƒãƒ¼ãƒ‰ã‚’ä½œæˆ
            folder_name = folder_info.get("name", folder_id)
            folder_node = {
                "type": "folder",
                "id": folder_id,
                "name": folder_name,
                "parent_id": parent_id,
                "is_leaf": folder_info.get("is_leaf", False)
            }
            result.append(folder_node)
            
            # ãƒ­ã‚°å‡ºåŠ›
            indent = "  " * (len([p for p in result if p.get("id") == parent_id]) + 1)
            leaf_mark = "ğŸƒ" if folder_info.get("is_leaf", False) else "ğŸ“‚"
            print(f"{indent}{leaf_mark} ãƒ•ã‚©ãƒ«ãƒ€ãƒãƒ¼ãƒ‰ä½œæˆ: name='{folder_name}', id={folder_id}, parent_id={parent_id}")
            
            # å­ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€å†å¸°çš„ã«å‡¦ç†
            if "data" in folder_info and not folder_info.get("is_leaf", False):
                self.create_folder_nodes(folder_info["data"], folder_id, result)
        
        return result

    def create_file_nodes(self, data, parent_id=None, result=None):
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ¼ãƒ‰ã‚’ä½œæˆã™ã‚‹ï¼ˆis_leafãŒTrueã®ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ï¼‰
        
        Args:
            data: JSONãƒ‡ãƒ¼ã‚¿
            parent_id: è¦ªãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ã®IDï¼ˆæœ€åˆã®ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ã¯nullï¼‰
            result: çµæœã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
        
        Returns:
            list: ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
        """
        if result is None:
            result = []
        
        for folder_id, folder_info in data.items():
            # is_leafãŒTrueã®ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
            if folder_info.get("is_leaf", False) and "data" in folder_info:
                for file_id, file_name in folder_info["data"].items():
                    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ¼ãƒ‰ã‚’ä½œæˆ
                    file_node = {
                        "type": "file",
                        "id": file_id,
                        "name": file_name,
                        "parent_id": folder_id,
                        "is_leaf": None
                    }
                    result.append(file_node)
            
            # å­ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€å†å¸°çš„ã«å‡¦ç†
            if "data" in folder_info and not folder_info.get("is_leaf", False):
                self.create_file_nodes(folder_info["data"], folder_id, result)
        
        return result

    def create_all_nodes(self,data):
        """
        ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ãƒãƒ¼ãƒ‰ã¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ¼ãƒ‰ã‚’ä½œæˆã™ã‚‹
        
        Args:
            data: JSONãƒ‡ãƒ¼ã‚¿
        
        Returns:
            tuple: (ãƒ•ã‚©ãƒ«ãƒ€ãƒ¼ãƒãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ, ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ, å…¨ãƒãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ)
        """
        folder_nodes = self.create_folder_nodes(data)
        file_nodes = self.create_file_nodes(data)
        all_nodes = folder_nodes + file_nodes
        
        return  all_nodes,folder_nodes, file_nodes

if __name__ == "__main__":
    cl_module = InitClusteringManager(
        sentence_name_db=ChromaDBManager('sentence_name_embeddings'),
        sentence_usage_db=ChromaDBManager('sentence_usage_embeddings'),
        sentence_category_db=ChromaDBManager('sentence_category_embeddings'),
        image_db=ChromaDBManager('image_embeddings'),
        images_folder_path='./imgs',
        output_base_path='./results'
    )
    # print(type(all_sentence_data['metadatas'][0]))
    cluster_num, _ = cl_module.get_optimal_cluster_num(embeddings=cl_module.sentence_name_db.get_all()['embeddings'])

    a = cl_module.sentence_name_db.get_all()['embeddings']
    cluster_result = cl_module.clustering(
        sentence_name_db_data=cl_module.sentence_name_db.get_all(), 
        image_db_data=cl_module.image_db.get_all(),
        clustering_id_dict={}, 
        sentence_id_dict={}, 
        image_id_dict={}, 
        cluster_num=cluster_num,
        output_folder=True, 
        output_json=True
    )