# フォルダ固有単語抽出アルゴリズム

## 概要

同階層にある複数のフォルダを比較し、各フォルダに特徴的な単語（そのフォルダには頻出するが、他のフォルダには少ない単語）を抽出するアルゴリズムです。

## 目的

- **フォルダの特徴把握**: 各フォルダがどのような内容を含んでいるか、単語レベルで理解する
- **フォルダ間の差異分析**: 同階層のフォルダがどのように異なるかを定量的に把握する
- **分類基準の発見**: 画像分類において、どのような言葉が分類の鍵になっているかを発見する

## アルゴリズムの流れ

### 1. データ収集フェーズ

```
入力: 同階層にある複数のis_leafフォルダ
出力: 各フォルダのキャプション一覧
```

**処理:**
1. 指定されたフォルダと同じ`parent_id`を持つ全フォルダを取得
2. `is_leaf = true`のフォルダのみをフィルタリング
3. 各フォルダ内の全画像の`clustering_id`を取得
4. `clustering_id`から対応するキャプションをデータベースから取得

### 2. 単語抽出フェーズ

```
入力: 各フォルダのキャプション一覧
出力: 各フォルダの重み付き単語カウンター
```

**処理:**
1. 各キャプションを「.」で文に分割
2. 文の位置によって重みを設定:
   - 1文目: 1.0 (100%)
   - 2文目: 0.85 (85%)
   - 3文目: 0.7 (70%)
   - 4文目以降: 0.6 (60%)
3. 各文を小文字化
4. 正規表現`\b[a-z]+\b`で英単語を抽出
5. ストップワード（`CAPTION_STOPWORDS`）を除外
6. 各フォルダごとに重み付きカウンターを作成し、単語の重み付き出現回数を集計

**重みの設計思想:**
- キャプションの冒頭に現れる単語ほど重要性が高い
- 極端なバイアスを避けるため、重みの差は緩やか（1.0 → 0.6）
- 4文目以降も無視せず、最低60%の重みを保持

**例:**
```python
# フォルダA: ["A cat with tail. The dog has fur."]
# 文1: "A cat with tail" → ['cat', 'tail'] (重み: 1.0)
# 文2: "The dog has fur" → ['dog', 'fur'] (重み: 0.85)

folder_word_counters = {
    'folder_A': {
        'cat': 1.0,    # 1文目に出現
        'tail': 1.0,   # 1文目に出現
        'dog': 0.85,   # 2文目に出現
        'fur': 0.85    # 2文目に出現
    },
    'folder_B': {'wheel': 2.0, 'engine': 1.0},
    ...
}
```

### 3. 特徴的単語スコアリングフェーズ（TF-IDF風 + 位置バイアス）

```
入力: 各フォルダの重み付き単語カウンター
出力: 各フォルダの特徴的単語ランキング（上位N個）
```

**スコア計算式:**

```
score(word, folder) = TF_weighted × IDF_like

where:
  TF_weighted = weighted_count_in_target_folder (位置バイアス考慮済み)
  IDF_like = TF_weighted / (weighted_count_in_other_folders + 1)
  
最終スコア = TF_weighted × IDF_like = TF_weighted² / (weighted_count_in_other_folders + 1)
```

**パラメータ説明:**
- `TF_weighted` (Weighted Term Frequency): 対象フォルダでの重み付き単語出現回数
  - 文の位置による重みが加味されている
  - 例: 1文目に2回、3文目に1回出現 → 2×1.0 + 1×0.7 = 2.7
- `weighted_count_in_other_folders`: 他の全フォルダでの重み付き出現回数の合計
- `+1`: ゼロ除算を防ぐためのスムージング定数

**スコアの意味:**
- **高スコア**: そのフォルダの冒頭文に頻出し、他のフォルダには出現しない単語
- **低スコア**: 全フォルダに共通して出現する単語、または後半の文にしか出現しない単語

### 4. ランキングフェーズ

```
入力: 各単語のスコア
出力: 各フォルダの特徴的単語Top N
```

**処理:**
1. 各フォルダについて、全単語をスコア降順にソート
2. 上位N個（デフォルト: 10個）を抽出
3. 各単語について以下の情報を記録:
   - `word`: 単語
   - `score`: 最終スコア
   - `count_in_folder`: そのフォルダでの出現回数
   - `count_in_others`: 他のフォルダでの出現回数合計
   - `ratio`: TF / (count_in_others + 1)

## 具体例（位置バイアス考慮版）

### 入力データ

**フォルダA（動物）:**
- キャプション1: "A cat with a long tail. The cat is cute."
- キャプション2: "A dog with brown fur. Dogs are loyal."
- キャプション3: "White fur and tail. A cat sleeps."

**フォルダB（乗り物）:**
- キャプション1: "A car with four wheels. Cars are fast."
- キャプション2: "A bike with two wheels. Bikes are eco-friendly."

### ステップ1: 文分割と重み付き単語抽出

**フォルダA - キャプション1:**
```
文1: "A cat with a long tail" → ['cat', 'long', 'tail'] (重み: 1.0)
文2: "The cat is cute" → ['cat', 'cute'] (重み: 0.85)
```

**フォルダA - キャプション2:**
```
文1: "A dog with brown fur" → ['dog', 'brown', 'fur'] (重み: 1.0)
文2: "Dogs are loyal" → ['dogs', 'loyal'] (重み: 0.85)
```

### ステップ2: 重み付き単語カウント

```
Weighted_Counter_A = {
  'cat': 1.0 + 0.85 + 1.0 = 2.85,  # 文1に2回、文2に1回
  'tail': 1.0 + 1.0 = 2.0,          # 文1に2回
  'fur': 1.0 + 1.0 = 2.0,           # 文1に2回
  'dog': 1.0,                       # 文1に1回
  'dogs': 0.85,                     # 文2に1回
  'long': 1.0,                      # 文1に1回
  'cute': 0.85,                     # 文2に1回
  'white': 1.0,                     # 文1に1回
  'sleeps': 0.85,                   # 文2に1回
  ...
}

Weighted_Counter_B = {
  'car': 1.0,                       # 文1に1回
  'wheels': 1.0 + 1.0 = 2.0,        # 文1に2回
  'cars': 0.85,                     # 文2に1回
  'bike': 1.0,                      # 文1に1回
  'bikes': 0.85,                    # 文2に1回
  ...
}
```

### ステップ3: スコア計算（フォルダAの単語 "cat"）

```
TF_weighted = 2.85 (フォルダAでの重み付き出現回数)
weighted_count_in_others = 0 (フォルダBには出現しない)

ratio = 2.85 / (0 + 1) = 2.85
score = 2.85 × 2.85 = 8.12
```

### ステップ3: スコア計算（フォルダAの単語 "cute" - 2文目のみ）

```
TF_weighted = 0.85 (フォルダAの文2での出現、重み0.85)
weighted_count_in_others = 0 (フォルダBには出現しない)

ratio = 0.85 / (0 + 1) = 0.85
score = 0.85 × 0.85 = 0.72
```

**比較:** 同じ1回出現でも、1文目の単語は `score = 1.0 × 1.0 = 1.0`、2文目の単語は `score = 0.85 × 0.85 = 0.72` となり、冒頭の単語が優先される。

### ステップ4: フォルダAの特徴的単語ランキング

```
1. cat     | score: 8.12 | 重み付き回数: 2.85 | 他: 0 | ratio: 2.85
2. tail    | score: 4.00 | 重み付き回数: 2.00 | 他: 0 | ratio: 2.00
3. fur     | score: 4.00 | 重み付き回数: 2.00 | 他: 0 | ratio: 2.00
4. dog     | score: 1.00 | 重み付き回数: 1.00 | 他: 0 | ratio: 1.00
5. long    | score: 1.00 | 重み付き回数: 1.00 | 他: 0 | ratio: 1.00
6. dogs    | score: 0.72 | 重み付き回数: 0.85 | 他: 0 | ratio: 0.85
7. cute    | score: 0.72 | 重み付き回数: 0.85 | 他: 0 | ratio: 0.85
```

**効果:** 1文目に頻出する "cat" が最高スコア、2文目のみの "cute" はスコアが低くなる。

## アルゴリズムの特性

### 長所

1. **シンプルで直感的**: TF-IDFの簡易版で理解しやすい
2. **計算効率が良い**: O(N×M) (N=フォルダ数, M=ユニーク単語数)
3. **解釈可能**: スコアの意味が明確（比率とカウントで説明可能）
4. **チューニング不要**: パラメータがほぼ不要
5. **位置バイアスによる精度向上**: キャプション冒頭の重要な単語を適切に重視
6. **極端なバイアス回避**: 後半の文も60%の重みを保持し、情報を失わない

### 短所・制約

1. **文脈無視**: 単語の意味や文脈を考慮しない
2. **複合語非対応**: "red car"のような複合的な概念を捉えられない
3. **スムージング定数**: `+1`の値により結果が変わる
4. **出現回数依存**: 極端に少ない/多い単語は適切に評価されない可能性
5. **文分割の精度**: "."以外の文区切り（!、?など）は考慮していない
6. **重みの固定値**: 1.0 → 0.85 → 0.7 → 0.6 の重みが全ケースに最適とは限らない

## 改善案

### 1. N-gram対応

```python
# 2-gramの例
words = ['red', 'car', 'fast', 'engine']
bigrams = ['red car', 'car fast', 'fast engine']
```

### 2. 動的な位置重み調整

```python
# キャプション長に応じて重みを調整
def calculate_position_weight(sentence_idx, total_sentences):
    if total_sentences == 1:
        return 1.0  # 1文のみの場合は重み1.0
    elif sentence_idx == 0:
        return 1.0  # 常に1文目は最大重み
    else:
        # 文が多いほど後半の重みを下げる
        decay_rate = 0.15 / total_sentences
        return max(0.6, 1.0 - (sentence_idx * decay_rate))
```

### 3. 文区切り記号の拡張

```python
# .以外の文区切りにも対応
import re
sentences = re.split(r'[.!?]+', caption)
```

### 4. 意味的類似度の考慮

```python
# Word2VecやBERTで意味的に類似した単語をグループ化
similar_words = ['car', 'vehicle', 'automobile']  # 同じグループとして扱う
```

## 出力フォーマット

### JSONファイル構造

```json
{
  "summary": {
    "total_captions": 150,
    "unique_words": 350,
    "sibling_leaf_folder_count": 5
  },
  "folder_unique_words": {
    "folder_id_001": {
      "folder_name": "animals",
      "unique_words": [
        {
          "word": "tail",
          "score": 125.5,
          "count_in_folder": 45,
          "count_in_others": 2,
          "ratio": 22.5
        }
      ]
    }
  }
}
```

### コンソール出力例

```
🎯 各フォルダの特徴的な単語 (Top 10):

   📁 animals (ID: folder_001):
      1. tail       | スコア: 125.50 | このフォルダ:  45回 | 他:   2回 | 比率: 22.50
      2. fur        | スコア:  98.30 | このフォルダ:  38回 | 他:   1回 | 比率: 19.00
```

## 応用例

### 1. 自動フォルダ名生成

特徴的な単語の上位3個を組み合わせてフォルダ名を生成:
```
animals → "tail, fur, paw"
vehicles → "wheel, engine, drive"
```

### 2. フォルダ推薦

新しい画像のキャプションから単語を抽出し、各フォルダの特徴的単語との一致度を計算してフォルダを推薦。

### 3. 分類基準の可視化

どのような単語がフォルダ分類の決め手になっているかを可視化し、分類ルールの理解を促進。

## 実装上の注意点

1. **メモリ使用量**: フォルダ数×単語数のマトリックスをメモリに保持
2. **処理時間**: フォルダ数が多い場合は計算時間が増加
3. **データベース負荷**: キャプション取得時のクエリ数に注意
4. **文字エンコーディング**: 日本語などの非ASCII文字の扱いに注意
5. **浮動小数点精度**: 重み付きカウントは浮動小数点数として扱われる

## パラメータ設定

```python
# 特徴語抽出パラメータ
TOP_N_UNIQUE_WORDS = 10  # 各フォルダから抽出する特徴的単語の数
SMOOTHING_CONSTANT = 1   # スコア計算時のスムージング定数

# 位置バイアスパラメータ（文の位置による重み）
POSITION_WEIGHTS = {
    0: 1.0,   # 1文目: 100%
    1: 0.85,  # 2文目: 85%
    2: 0.7,   # 3文目: 70%
    'default': 0.6  # 4文目以降: 60%
}

# オプションパラメータ
MIN_WORD_LENGTH = 2      # 最小単語長（オプション）
MAX_WORD_LENGTH = 20     # 最大単語長（オプション）
```

## 位置バイアスのチューニング

### 重みの選択指針

- **強いバイアス** (1.0 → 0.5 → 0.3): 1文目を極端に重視、要約的な短文キャプション向け
- **中程度のバイアス** (1.0 → 0.85 → 0.7) ⭐ **現在の設定**: バランス型、多文キャプション向け
- **弱いバイアス** (1.0 → 0.95 → 0.9): ほぼ均等、長文詳細キャプション向け

### データ特性に応じた調整例

```python
# 短文キャプション（平均1-2文）の場合
POSITION_WEIGHTS = {0: 1.0, 1: 0.75, 2: 0.5, 'default': 0.3}

# 長文キャプション（平均5文以上）の場合
POSITION_WEIGHTS = {0: 1.0, 1: 0.95, 2: 0.9, 'default': 0.85}
```

## 参考文献

- TF-IDF (Term Frequency-Inverse Document Frequency)
- Information Retrieval基礎理論
- テキストマイニング手法

---

**作成日**: 2025年11月17日  
**バージョン**: 1.0  
**実装**: `/backend/routers/action.py` - `execute_continuous_clustering()` 関数内
